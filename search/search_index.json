{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p>Autolabel is a Python library to label, clean and enrich datasets with Large Language Models (LLMs).</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Autolabel data for NLP tasks such as classification, question-answering and named entity-recognition, entity matching and more.</li> <li>Seamlessly use commercial and open source LLMs from providers such as OpenAI, Anthropic, HuggingFace, Google and more.</li> <li>Leverage research-proven LLM techniques to boost label quality, such as few-shot learning and chain-of-thought prompting.</li> <li>Confidence estimation and explanations out of the box for every single output label</li> <li>Caching and state management to minimize costs and experimentation time</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":"<p>You can get started with Autolabel by simpling bringing the dataset you want to label, picking your favorite LLM and writing a few lines of code. </p> <ul> <li>Installation and your first labeling task: Steps to install Autolabel and run sentiment analysis for movie reviews using OpenAI's <code>gpt-3.5-turbo</code>. </li> <li>Classification tutorial: A deeper dive into how Autolabel can be used to detect toxic comments at 95%+ accuracy. </li> <li>Here are more examples with sample notebooks that show how Autolabel can be used for different NLP tasks.</li> </ul>"},{"location":"#resources","title":"Resources","text":"<ul> <li>Discord: Join our Discord community for conversations on LLMs, Autolabel and so much more!</li> <li>Github: Create an issue to report any bugs or give us a star on Github. </li> <li>Contribute: Share your feedback or add new features, and help us improve Autolabel!</li> </ul>"},{"location":"concepts/concepts/","title":"Modules","text":"<p>On this page, we will talk about the different pages that exist in Autolabel. We will first discuss the overview of a module and then go into the different subheadings, expanding and giving some examples for each.</p>"},{"location":"concepts/concepts/#prompts","title":"Prompts","text":"<p>Writing prompts is a crucial aspect of training language models for specific tasks. In this tutorial, we will explore the five essential parts of a prompt: the prefix prompt, task prompt, output prompt, seed examples, and current example. Understanding and constructing these components effectively can help guide the model's behavior and generate accurate and contextually appropriate responses. Let's delve into each part in detail.</p>"},{"location":"concepts/concepts/#prefix-prompt","title":"Prefix Prompt","text":"<p>The prefix prompt is the initial line of the prompt, which sets the domain and provides task-independent information to the model. It helps the model understand the specific area or expertise it should embody while generating responses. For example, if the prefix prompt indicates a medical domain, the model will focus on generating responses that align with medical knowledge and terminology. Example: [Medical] In this prompt, the model should provide expert advice on diagnosing and treating common ailments.</p>"},{"location":"concepts/concepts/#task-prompt","title":"Task Prompt","text":"<p>The task prompt explains the objective or task the model needs to accomplish. It describes the specific instructions or guidelines for completing the task. This section is crucial for clearly conveying the desired output from the model. Example: You are a medical expert. Given a patient's symptoms and medical history, provide a diagnosis and recommend appropriate treatment options.</p>"},{"location":"concepts/concepts/#output-prompt","title":"Output Prompt","text":"<p>The output prompt informs the model about the expected answer format or structure. It defines the specific format in which the model should provide the answer. This step ensures consistency and enables easier processing of the model's responses. Example: Provide the diagnosis and treatment recommendations in JSON format, with the following keys: \"diagnosis\" and \"treatment.\" The value for each key should be a string representing the diagnosis and treatment, respectively.</p>"},{"location":"concepts/concepts/#seed-examples","title":"Seed Examples","text":"<p>Seed examples play a vital role in training the model by providing real-world examples from the task distribution. These examples help the model grasp the nature of the task, understand the expected outputs, and align its behavior accordingly. It is crucial to provide meaningful and diverse seed examples to facilitate accurate responses. Example: Seed Examples:  </p> <p>Patient: Fever, sore throat, and fatigue. Medical History: None. Diagnosis: \"Common cold\" Treatment: \"Rest, plenty of fluids, and over-the-counter cold medication.\" Patient: Persistent cough, shortness of breath, and wheezing. Medical History: Asthma. Diagnosis: \"Asthma exacerbation\" Treatment: \"Inhaled bronchodilators and corticosteroids as prescribed.\"</p>"},{"location":"concepts/concepts/#current-example","title":"Current Example","text":"<p>The current example is the specific instance for which you seek the model's response. It provides the exact answer or label you want the model to assign to this particular example. Example: Current Example: Patient: Severe headache, visual disturbances, and nausea. Medical History: None. Desired Diagnosis: \"Migraine\" Desired Treatment: \"Prescribed pain-relief medication and lifestyle modifications.\"  </p>"},{"location":"concepts/concepts/#configs","title":"Configs","text":"<p>There are 3 modules required by every labeling run - 1. A task 2. An LLM 3. A dataset</p> <p>All 3 of these modules can be instantiated with configs. A config can be passed in as a dictionary or as the path to a json file. The config consists of different keys and the following section will list out each key along with the property of the module that it affects.</p>"},{"location":"concepts/concepts/#config","title":"Config","text":"<p>The Config class is used to parse, validate, and store information about the labeling task being performed.</p> <p>         Bases: <code>BaseConfig</code></p> <p>Class to parse and store configs passed to Autolabel agent.</p> Source code in <code>src/autolabel/configs/config.py</code> <pre><code>class AutolabelConfig(BaseConfig):\n\"\"\"Class to parse and store configs passed to Autolabel agent.\"\"\"\n# Top-level config keys\nTASK_NAME_KEY = \"task_name\"\nTASK_TYPE_KEY = \"task_type\"\nDATASET_CONFIG_KEY = \"dataset\"\nMODEL_CONFIG_KEY = \"model\"\nPROMPT_CONFIG_KEY = \"prompt\"\n# Dataset config keys (config[\"dataset\"][&lt;key&gt;])\nLABEL_COLUMN_KEY = \"label_column\"\nEXPLANATION_COLUMN_KEY = \"explanation_column\"\nTEXT_COLUMN_KEY = \"text_column\"\nDELIMITER_KEY = \"delimiter\"\n# Model config keys (config[\"model\"][&lt;key&gt;])\nPROVIDER_KEY = \"provider\"\nMODEL_NAME_KEY = \"name\"\nMODEL_PARAMS_KEY = \"params\"\nCOMPUTE_CONFIDENCE_KEY = \"compute_confidence\"\n# Prompt config keys (config[\"prompt\"][&lt;key&gt;])\nTASK_GUIDELINE_KEY = \"task_guidelines\"\nVALID_LABELS_KEY = \"labels\"\nFEW_SHOT_EXAMPLE_SET_KEY = \"few_shot_examples\"\nFEW_SHOT_SELECTION_ALGORITHM_KEY = \"few_shot_selection\"\nFEW_SHOT_NUM_KEY = \"few_shot_num\"\nEXAMPLE_TEMPLATE_KEY = \"example_template\"\nOUTPUT_GUIDELINE_KEY = \"output_guidelines\"\nOUTPUT_FORMAT_KEY = \"output_format\"\nCHAIN_OF_THOUGHT_KEY = \"chain_of_thought\"\ndef __init__(self, config: Union[str, Dict]) -&gt; None:\nsuper().__init__(config)\n@cached_property\ndef _dataset_config(self) -&gt; Dict:\n\"\"\"Returns information about the dataset being used for labeling (e.g. label_column, text_column, delimiter)\"\"\"\nreturn self.config.get(self.DATASET_CONFIG_KEY, {})\n@cached_property\ndef _model_config(self) -&gt; Dict:\n\"\"\"Returns information about the model being used for labeling (e.g. provider name, model name, parameters)\"\"\"\nreturn self.config[self.MODEL_CONFIG_KEY]\n@cached_property\ndef _prompt_config(self) -&gt; Dict:\n\"\"\"Returns information about the prompt we are passing to the model (e.g. task guidelines, examples, output formatting)\"\"\"\nreturn self.config[self.PROMPT_CONFIG_KEY]\n# project and task definition config\ndef task_name(self) -&gt; str:\nreturn self.config[self.TASK_NAME_KEY]\ndef task_type(self) -&gt; str:\n\"\"\"Returns the type of task we have configured the labeler to perform (e.g. Classification, Question Answering)\"\"\"\nreturn self.config[self.TASK_TYPE_KEY]\n# Dataset config\ndef label_column(self) -&gt; str:\n\"\"\"Returns the name of the column containing labels for the dataset. Used for comparing accuracy of autolabel results vs ground truth\"\"\"\nreturn self._dataset_config.get(self.LABEL_COLUMN_KEY, None)\ndef text_column(self) -&gt; str:\n\"\"\"Returns the name of the column containing text data we intend to label\"\"\"\nreturn self._dataset_config.get(self.TEXT_COLUMN_KEY, None)\ndef explanation_column(self) -&gt; str:\n\"\"\"Returns the name of the column containing an explanation as to why the data is labeled a certain way\"\"\"\nreturn self._dataset_config.get(self.EXPLANATION_COLUMN_KEY, None)\ndef delimiter(self) -&gt; str:\n\"\"\"Returns the token used to seperate cells in the dataset. Defaults to a comma ','\"\"\"\nreturn self._dataset_config.get(self.DELIMITER_KEY, \",\")\n# Model config\ndef provider(self) -&gt; str:\n\"\"\"Returns the name of the entity that provides the currently configured model (e.g. OpenAI, Anthropic, Refuel)\"\"\"\nreturn self._model_config[self.PROVIDER_KEY]\ndef model_name(self) -&gt; str:\n\"\"\"Returns the name of the model being used for labeling (e.g. gpt-4, claude-v1)\"\"\"\nreturn self._model_config[self.MODEL_NAME_KEY]\ndef model_params(self) -&gt; Dict:\n\"\"\"Returns a dict of configured settings for the model (e.g. hyperparameters)\"\"\"\nreturn self._model_config.get(self.MODEL_PARAMS_KEY, {})\ndef confidence(self) -&gt; bool:\n\"\"\"Returns true if the model is able to return a confidence score along with its predictions\"\"\"\nreturn self._model_config.get(self.COMPUTE_CONFIDENCE_KEY, False)\n# Prompt config\ndef task_guidelines(self) -&gt; str:\nreturn self._prompt_config.get(self.TASK_GUIDELINE_KEY, \"\")\ndef labels_list(self) -&gt; List[str]:\n\"\"\"Returns a list of valid labels\"\"\"\nreturn self._prompt_config.get(self.VALID_LABELS_KEY, [])\ndef few_shot_example_set(self) -&gt; Union[str, List]:\n\"\"\"Returns examples of how data should be labeled, used to guide context to the model about the task it is performing\"\"\"\nreturn self._prompt_config.get(self.FEW_SHOT_EXAMPLE_SET_KEY, [])\ndef few_shot_algorithm(self) -&gt; str:\n\"\"\"Returns which algorithm is being used to construct the set of examples being given to the model about the labeling task\"\"\"\nreturn self._prompt_config.get(self.FEW_SHOT_SELECTION_ALGORITHM_KEY, None)\ndef few_shot_num_examples(self) -&gt; int:\n\"\"\"Returns how many examples should be given to the model in its instruction prompt\"\"\"\nreturn self._prompt_config.get(self.FEW_SHOT_NUM_KEY, 0)\ndef example_template(self) -&gt; str:\n\"\"\"Returns a string containing a template for how examples will be formatted in the prompt\"\"\"\nexample_template = self._prompt_config.get(self.EXAMPLE_TEMPLATE_KEY, None)\nif not example_template:\nraise ValueError(\"An example template needs to be specified in the config.\")\nreturn example_template\ndef output_format(self) -&gt; str:\nreturn self._prompt_config.get(self.OUTPUT_FORMAT_KEY, None)\ndef output_guidelines(self) -&gt; str:\nreturn self._prompt_config.get(self.OUTPUT_GUIDELINE_KEY, None)\ndef chain_of_thought(self) -&gt; bool:\n\"\"\"Returns true if the model is able to perform chain of thought reasoning.\"\"\"\nreturn self._prompt_config.get(self.CHAIN_OF_THOUGHT_KEY, False)\n</code></pre>"},{"location":"concepts/concepts/#src.autolabel.configs.config.AutolabelConfig.chain_of_thought","title":"<code>chain_of_thought()</code>","text":"<p>Returns true if the model is able to perform chain of thought reasoning.</p> Source code in <code>src/autolabel/configs/config.py</code> <pre><code>def chain_of_thought(self) -&gt; bool:\n\"\"\"Returns true if the model is able to perform chain of thought reasoning.\"\"\"\nreturn self._prompt_config.get(self.CHAIN_OF_THOUGHT_KEY, False)\n</code></pre>"},{"location":"concepts/concepts/#src.autolabel.configs.config.AutolabelConfig.confidence","title":"<code>confidence()</code>","text":"<p>Returns true if the model is able to return a confidence score along with its predictions</p> Source code in <code>src/autolabel/configs/config.py</code> <pre><code>def confidence(self) -&gt; bool:\n\"\"\"Returns true if the model is able to return a confidence score along with its predictions\"\"\"\nreturn self._model_config.get(self.COMPUTE_CONFIDENCE_KEY, False)\n</code></pre>"},{"location":"concepts/concepts/#src.autolabel.configs.config.AutolabelConfig.delimiter","title":"<code>delimiter()</code>","text":"<p>Returns the token used to seperate cells in the dataset. Defaults to a comma ','</p> Source code in <code>src/autolabel/configs/config.py</code> <pre><code>def delimiter(self) -&gt; str:\n\"\"\"Returns the token used to seperate cells in the dataset. Defaults to a comma ','\"\"\"\nreturn self._dataset_config.get(self.DELIMITER_KEY, \",\")\n</code></pre>"},{"location":"concepts/concepts/#src.autolabel.configs.config.AutolabelConfig.example_template","title":"<code>example_template()</code>","text":"<p>Returns a string containing a template for how examples will be formatted in the prompt</p> Source code in <code>src/autolabel/configs/config.py</code> <pre><code>def example_template(self) -&gt; str:\n\"\"\"Returns a string containing a template for how examples will be formatted in the prompt\"\"\"\nexample_template = self._prompt_config.get(self.EXAMPLE_TEMPLATE_KEY, None)\nif not example_template:\nraise ValueError(\"An example template needs to be specified in the config.\")\nreturn example_template\n</code></pre>"},{"location":"concepts/concepts/#src.autolabel.configs.config.AutolabelConfig.explanation_column","title":"<code>explanation_column()</code>","text":"<p>Returns the name of the column containing an explanation as to why the data is labeled a certain way</p> Source code in <code>src/autolabel/configs/config.py</code> <pre><code>def explanation_column(self) -&gt; str:\n\"\"\"Returns the name of the column containing an explanation as to why the data is labeled a certain way\"\"\"\nreturn self._dataset_config.get(self.EXPLANATION_COLUMN_KEY, None)\n</code></pre>"},{"location":"concepts/concepts/#src.autolabel.configs.config.AutolabelConfig.few_shot_algorithm","title":"<code>few_shot_algorithm()</code>","text":"<p>Returns which algorithm is being used to construct the set of examples being given to the model about the labeling task</p> Source code in <code>src/autolabel/configs/config.py</code> <pre><code>def few_shot_algorithm(self) -&gt; str:\n\"\"\"Returns which algorithm is being used to construct the set of examples being given to the model about the labeling task\"\"\"\nreturn self._prompt_config.get(self.FEW_SHOT_SELECTION_ALGORITHM_KEY, None)\n</code></pre>"},{"location":"concepts/concepts/#src.autolabel.configs.config.AutolabelConfig.few_shot_example_set","title":"<code>few_shot_example_set()</code>","text":"<p>Returns examples of how data should be labeled, used to guide context to the model about the task it is performing</p> Source code in <code>src/autolabel/configs/config.py</code> <pre><code>def few_shot_example_set(self) -&gt; Union[str, List]:\n\"\"\"Returns examples of how data should be labeled, used to guide context to the model about the task it is performing\"\"\"\nreturn self._prompt_config.get(self.FEW_SHOT_EXAMPLE_SET_KEY, [])\n</code></pre>"},{"location":"concepts/concepts/#src.autolabel.configs.config.AutolabelConfig.few_shot_num_examples","title":"<code>few_shot_num_examples()</code>","text":"<p>Returns how many examples should be given to the model in its instruction prompt</p> Source code in <code>src/autolabel/configs/config.py</code> <pre><code>def few_shot_num_examples(self) -&gt; int:\n\"\"\"Returns how many examples should be given to the model in its instruction prompt\"\"\"\nreturn self._prompt_config.get(self.FEW_SHOT_NUM_KEY, 0)\n</code></pre>"},{"location":"concepts/concepts/#src.autolabel.configs.config.AutolabelConfig.label_column","title":"<code>label_column()</code>","text":"<p>Returns the name of the column containing labels for the dataset. Used for comparing accuracy of autolabel results vs ground truth</p> Source code in <code>src/autolabel/configs/config.py</code> <pre><code>def label_column(self) -&gt; str:\n\"\"\"Returns the name of the column containing labels for the dataset. Used for comparing accuracy of autolabel results vs ground truth\"\"\"\nreturn self._dataset_config.get(self.LABEL_COLUMN_KEY, None)\n</code></pre>"},{"location":"concepts/concepts/#src.autolabel.configs.config.AutolabelConfig.labels_list","title":"<code>labels_list()</code>","text":"<p>Returns a list of valid labels</p> Source code in <code>src/autolabel/configs/config.py</code> <pre><code>def labels_list(self) -&gt; List[str]:\n\"\"\"Returns a list of valid labels\"\"\"\nreturn self._prompt_config.get(self.VALID_LABELS_KEY, [])\n</code></pre>"},{"location":"concepts/concepts/#src.autolabel.configs.config.AutolabelConfig.model_name","title":"<code>model_name()</code>","text":"<p>Returns the name of the model being used for labeling (e.g. gpt-4, claude-v1)</p> Source code in <code>src/autolabel/configs/config.py</code> <pre><code>def model_name(self) -&gt; str:\n\"\"\"Returns the name of the model being used for labeling (e.g. gpt-4, claude-v1)\"\"\"\nreturn self._model_config[self.MODEL_NAME_KEY]\n</code></pre>"},{"location":"concepts/concepts/#src.autolabel.configs.config.AutolabelConfig.model_params","title":"<code>model_params()</code>","text":"<p>Returns a dict of configured settings for the model (e.g. hyperparameters)</p> Source code in <code>src/autolabel/configs/config.py</code> <pre><code>def model_params(self) -&gt; Dict:\n\"\"\"Returns a dict of configured settings for the model (e.g. hyperparameters)\"\"\"\nreturn self._model_config.get(self.MODEL_PARAMS_KEY, {})\n</code></pre>"},{"location":"concepts/concepts/#src.autolabel.configs.config.AutolabelConfig.provider","title":"<code>provider()</code>","text":"<p>Returns the name of the entity that provides the currently configured model (e.g. OpenAI, Anthropic, Refuel)</p> Source code in <code>src/autolabel/configs/config.py</code> <pre><code>def provider(self) -&gt; str:\n\"\"\"Returns the name of the entity that provides the currently configured model (e.g. OpenAI, Anthropic, Refuel)\"\"\"\nreturn self._model_config[self.PROVIDER_KEY]\n</code></pre>"},{"location":"concepts/concepts/#src.autolabel.configs.config.AutolabelConfig.task_type","title":"<code>task_type()</code>","text":"<p>Returns the type of task we have configured the labeler to perform (e.g. Classification, Question Answering)</p> Source code in <code>src/autolabel/configs/config.py</code> <pre><code>def task_type(self) -&gt; str:\n\"\"\"Returns the type of task we have configured the labeler to perform (e.g. Classification, Question Answering)\"\"\"\nreturn self.config[self.TASK_TYPE_KEY]\n</code></pre>"},{"location":"concepts/concepts/#src.autolabel.configs.config.AutolabelConfig.text_column","title":"<code>text_column()</code>","text":"<p>Returns the name of the column containing text data we intend to label</p> Source code in <code>src/autolabel/configs/config.py</code> <pre><code>def text_column(self) -&gt; str:\n\"\"\"Returns the name of the column containing text data we intend to label\"\"\"\nreturn self._dataset_config.get(self.TEXT_COLUMN_KEY, None)\n</code></pre>"},{"location":"concepts/concepts/#tasks","title":"Tasks","text":""},{"location":"concepts/concepts/#classification","title":"Classification","text":""},{"location":"concepts/concepts/#question-answering","title":"Question Answering","text":""},{"location":"concepts/concepts/#entity-matching","title":"Entity matching","text":""},{"location":"concepts/concepts/#named-entity-recognition","title":"Named Entity Recognition","text":""},{"location":"guide/accuracy/chain-of-thought/","title":"Chain of Thought","text":"Chain of Thought Prompting (Wei et al) <p>LLMs find it hard to perform well on complex reasoning tasks. We can unlock the reasoning abilities of LLMs using chain of thought prompting. This involves giving the LLM a few reasoning explanations along with the questions and answers and then asking the model to produce the reasoning before producing the answer. The hope is that the seed examples with explanations help the model understand the reasoning behind answer and prods it to use similar reasoning before arriving to the answer.</p> <p>Chain of thought makes LLMs more effective at reasoning tasks like mathematical word problems, commonsense reasoning questions and complex medical questions. It also provides a window into the thought process of the LLM, though some research points the link between the generated explanation and the final answer may be weak.</p>"},{"location":"guide/accuracy/chain-of-thought/#using-chain-of-thought-in-autolabel","title":"Using Chain Of Thought in Autolabel","text":"<p>Enabling chain-of-thought prompting for your task is straightforward with Autolabel. It works best when provided with a few seed examples with explanations. Thus enabling chain of thought requires a few things -  </p> <ol> <li>Writing up explanations or generating explanations for your seed examples automatically by using an LLM</li> <li>Specifying the <code>explanation_column</code> in the dataset part of the config.</li> <li>Altering the task guidelines to tell the model to generate an explanation before generating the final answer.</li> <li>Altering the <code>example_template</code> to reflect the presence of an explanation.</li> </ol> <p>We will go through using chain of thought on a dataset where it shows improvement, like the Squad question answering dataset.</p> <p>Let's see a datapoint before there is any explanation added to it.</p> context question answer Private schools generally prefer to be called independent schools, because of their freedom to operate outside of government and local government control. Some of these are also known as public schools. Preparatory schools in the UK prepare pupils aged up to 13 years old to enter public schools. The name 'public school' is based on the fact that the schools were open to pupils from anywhere, and not merely to those from a certain locality, and of any religion or occupation. According to The Good Schools Guide approximately 9 per cent of children being educated in the UK are doing so at fee-paying schools at GSCE level and 13 per cent at A-level.[citation needed] Many independent schools are single-sex (though this is becoming less common). Fees range from under \u00a33,000 to \u00a321,000 and above per year for day pupils, rising to \u00a327,000+ per year for boarders. For details in Scotland, see 'Meeting the Cost'. At A-level, what percentage of British students attend fee-paying schools? 13 <p>Now we can manually write the explanation for this or a couple of seed examples easily. But this will be tiresome for &gt; 10 examples. LLMs come to the rescue yet again! We can just define the config and ask the agent to generate explanations as well!</p> <pre><code>config = {\n\"task_name\": \"OpenbookQAWikipedia\",\n\"task_type\": \"question_answering\",\n\"dataset\": {\n\"label_column\": \"answer\",\n\"explanation_column\": \"explanation\",\n\"delimiter\": \",\"\n},\n\"model\": {\n\"provider\": \"openai\",\n\"name\": \"gpt-3.5-turbo\",\n},\n\"prompt\": {\n\"task_guidelines\": \"You are an expert at answering questions based on wikipedia articles. Your job is to answer the following questions using the context provided with the question. Use the context to answer the question - the answer is a continuous span of words from the context.\\n\",\n\"output_guidelines\": \"Your answer will consist of an explanation, followed by the correct answer. The last line of the response should always be is JSON format with one key: {\\\"label\\\": \\\"the correct answer\\\"}.\\n If the question cannot be answered using the context and the context alone without any outside knowledge, the question is unanswerable. If the question is unanswerable, return the answer as {\\\"label\\\": \\\"unanswerable\\\"}\\n\",\n\"few_shot_examples\": \"../examples/squad_v2/seed.csv\",\n\"few_shot_selection\": \"semantic_similarity\",\n\"few_shot_num\": 3,\n\"example_template\": \"Context: {context}\\nQuestion: {question}\\nAnswer: Let's think step by step.\\n{explanation}\\n{answer}\"\n}\n}\n</code></pre> <p>Notice the changes that we have made to the config compared to the config without Chain-of-Thought here. We have added two new fields to the config</p> <ul> <li><code>explanation_column</code></li> <li><code>example_template</code></li> </ul> <p><code>explanation_column</code> is the column where the explanation for the seed examples will reside. Next, notice that the <code>example_template</code> key contains the explanation column as well. This tells the config where the explanation should be put when using the seed examples. We use the <code>Let's think step by step</code> prompt to initiate the chain of thought in the model.</p> <p>Now, in order to generate explanations for the seed examples, in case they were not manually generated is, <pre><code>from autolabel import LabelingAgent\nagent = LabelingAgent(config)\nagent.generate_explanations(\"path_to_seed_examples.csv\")\n</code></pre></p> <p>Once these explanations are generated, the dataset looks like</p> context question answer explanation Private schools generally prefer to be called independent schools, because of their freedom to operate outside of government and local government control. Some of these are also known as public schools. Preparatory schools in the UK prepare pupils aged up to 13 years old to enter public schools. The name 'public school' is based on the fact that the schools were open to pupils from anywhere, and not merely to those from a certain locality, and of any religion or occupation. According to The Good Schools Guide approximately 9 per cent of children being educated in the UK are doing so at fee-paying schools at GSCE level and 13 per cent at A-level.[citation needed] Many independent schools are single-sex (though this is becoming less common). Fees range from under \u00a33,000 to \u00a321,000 and above per year for day pupils, rising to \u00a327,000+ per year for boarders. For details in Scotland, see 'Meeting the Cost'. At A-level, what percentage of British students attend fee-paying schools? 13 Independent schools in the UK are private schools that charge fees.  These schools are also known as public schools. According to The Good Schools Guide, about 9% of children in the UK attend fee-paying schools at the GSCE level. At the A-level, which is a higher level of education, a higher percentage of students, 13%, attend fee-paying independent schools. Since 13% of students attend fee-paying schools at the A-level, and the question asks what percentage attend at the A-level specifically, So, the answer is 13. <p>Now to generate labels for this dataset, all we have to do is, <pre><code>agent.plan('data/squad_v2_test.csv')\nagent.run('data/squad_v2_test.csv', max_items = 100)\n</code></pre></p>"},{"location":"guide/accuracy/confidence/","title":"Confidence","text":"ChatGPT summarizing a non-existent New York Times article even without access to the Internet <p>One of the biggest criticisms of using a LLMs so far has been hallucinations - LLMs can seem very confidence in their language even when they are completely incorrect. <code>autolabel</code> provides a confidence score for each LLM output that is correlated with the likelihood of that output being incorrect, i.e. if the confidence score is high, then it is more likely that the output is correct, and if confidence score is low, it is likely that the LLM has produced an incorrect output. </p>"},{"location":"guide/accuracy/confidence/#computing-confidence-scores","title":"Computing Confidence Scores","text":"<p>The <code>autolabel</code> library today relies on token level probabilities, also known as logprobs, to compute confidence scores. However, very few models today return token level probabilities alongside prediction. Out of all models supported by <code>autolabel</code> today, only the <code>text-davinci-003</code> model by <code>openai</code> can return logprobs. For all other models, Refuel has setup an in-house API to generate logprobs for a specific prediction given an input, regardless of the language model that was originally used to query for the prediction. For <code>text-davinci-003</code>, we use the logprobs returned by <code>openai</code>'s API instead of querying our in-house API.</p> <p>Generating confidence scores is simple - setting the key <code>compute_confidence</code> to <code>True</code> in the <code>model</code> dictionary of the config should initiate confidence score retrieval. Here is an example:</p> <pre><code>{\n\"task_name\": \"PersonLocationOrgMiscNER\",\n\"task_type\": \"named_entity_recognition\",\n\"dataset\": {\n\"label_column\": \"CategorizedLabels\",\n\"text_column\": \"example\",\n\"delimiter\": \"%\"\n},\n\"model\": {\n\"provider\": \"anthropic\",\n\"name\": \"claude-v1\",\n\"compute_confidence\": True\n},\n\"prompt\": {\n\"task_guidelines\": \"You are an expert at extracting entities from text.\",\n\"labels\": [\n\"Location\",\n\"Organization\",\n\"Person\",\n\"Miscellaneous\"\n],\n\"example_template\": \"Example: {example}\\nOutput: {CategorizedLabels}\",\n\"few_shot_examples\": \"../examples/conll2003/seed.csv\",\n\"few_shot_selection\": \"semantic_similarity\",\n\"few_shot_num\": 5\n}\n}\n</code></pre> <p>In the above example, by setting <code>compute_confidence</code> to True, <code>autolabel</code> will start calling Refuel's api to generate token level probabilities and compute confidence scores for each prediction. In order for this to run successfully, ensure that the following setup has been completed:</p> <p>Set the following environment variable: <pre><code>export REFUEL_API_KEY=&lt;your-refuel-key&gt;\n</code></pre> replacing <code>&lt;your-refuel-key&gt;</code> with your API key, which you can get from here</p>"},{"location":"guide/accuracy/confidence/#interpreting-scores","title":"Interpreting Scores","text":"<p>To see how confidence scores can be used to make a tradeoff between task performance and completion rate, let's take a look at the following example:</p> <p> </p> Library output when confidence is enabled <p><code>autolabel</code> outputs a table consisting of metrics at various confidence thresholds when <code>compute_confidence</code> is set to <code>True</code>. Specifically, this is the table we get when we label 100 examples from the CONLL-2003 dataset with semantic similarity enabled. The first row in the table corresponds to the overall performance: we were able to successfully label 98% of examples at an F1 score of 0.885. However, we can use this table to decide on a confidence threshold to accept predictions at and increase our metrics. For example, note that according the highlighed row, if we accept labels with confidence scores above ~2.207, we can boost our F1 score to 0.95 while reducing completion rate to 79%. </p>"},{"location":"guide/accuracy/few-shot/","title":"Few-shot Prompting","text":"<p>It has been shown that the specific seed examples used while constructing the prompt have an impact on the performance of the model. Seed examples are the labeled dataset examples which the model is shown to help it understand the task better. Selecting the seed example per datapoint can help boost performance. We support the following example selection techniques:</p> <ol> <li>Fixed_few_shot - The same set of seed examples are used for every input data point.</li> <li>Semantic_similarity - Language embeddings are computed for all the examples in the seed set and a vector similarity search finds the few shot examples which are closest to the input datapoint. The hope is that closer datapoints from the seed set will give the model more context on how similar examples have been labeled, helping it improve performance.</li> <li>Max_marginal_relevance - Semantic similarity search is used to retrieve a set of candidate examples. Then, a diversity-driven selection strategy is used amongst these candidates to select a final subset of examples that have the most coverage of the initial pool of candidate examples.</li> </ol> <p>Example: </p> <p></p> <p>Consider the following labeling runs for a classification task on the banking dataset. There are a total of 1998 items to be labeled and we assume a starting labeled seedset of 200 examples. Here is the config to label this dataset in zero-shot fashion:</p> <pre><code>config_zero_shot = {\n\"task_name\": \"BankingComplaintsClassification\",\n\"task_type\": \"classification\",\n\"dataset\": {\n\"label_column\": \"label\",\n\"delimiter\": \",\"\n},\n\"model\": {\n\"provider\": \"openai\",\n\"name\": \"gpt-3.5-turbo\"\n},\n\"prompt\": {\n\"task_guidelines\": \"You are an expert at understanding bank customers support complaints and queries.\\nYour job is to correctly classify the provided input example into one of the following categories.\\nCategories:\\n{labels}\",\n\"output_guidelines\": \"You will answer with just the the correct output label and nothing else.\",\n\"labels\": [\n\"activate_my_card\",\n\"age_limit\",\n\"apple_pay_or_google_pay\",\n\"atm_support\",\n\"automatic_top_up\",\n\"balance_not_updated_after_bank_transfer\",\n\"balance_not_updated_after_cheque_or_cash_deposit\",\n\"beneficiary_not_allowed\",\n\"cancel_transfer\",\n\"card_about_to_expire\",\n\"card_acceptance\",\n\"card_arrival\",\n\"card_delivery_estimate\",\n\"card_linking\",\n\"card_not_working\",\n\"card_payment_fee_charged\",\n\"card_payment_not_recognised\",\n\"card_payment_wrong_exchange_rate\",\n\"card_swallowed\",\n\"cash_withdrawal_charge\",\n\"cash_withdrawal_not_recognised\",\n\"change_pin\",\n\"compromised_card\",\n\"contactless_not_working\",\n\"country_support\",\n\"declined_card_payment\",\n\"declined_cash_withdrawal\",\n\"declined_transfer\",\n\"direct_debit_payment_not_recognised\",\n\"disposable_card_limits\",\n\"edit_personal_details\",\n\"exchange_charge\",\n\"exchange_rate\",\n\"exchange_via_app\",\n\"extra_charge_on_statement\",\n\"failed_transfer\",\n\"fiat_currency_support\",\n\"get_disposable_virtual_card\",\n\"get_physical_card\",\n\"getting_spare_card\",\n\"getting_virtual_card\",\n\"lost_or_stolen_card\",\n\"lost_or_stolen_phone\",\n\"order_physical_card\",\n\"passcode_forgotten\",\n\"pending_card_payment\",\n\"pending_cash_withdrawal\",\n\"pending_top_up\",\n\"pending_transfer\",\n\"pin_blocked\",\n\"receiving_money\",\n\"Refund_not_showing_up\",\n\"request_refund\",\n\"reverted_card_payment?\",\n\"supported_cards_and_currencies\",\n\"terminate_account\",\n\"top_up_by_bank_transfer_charge\",\n\"top_up_by_card_charge\",\n\"top_up_by_cash_or_cheque\",\n\"top_up_failed\",\n\"top_up_limits\",\n\"top_up_reverted\",\n\"topping_up_by_card\",\n\"transaction_charged_twice\",\n\"transfer_fee_charged\",\n\"transfer_into_account\",\n\"transfer_not_received_by_recipient\",\n\"transfer_timing\",\n\"unable_to_verify_identity\",\n\"verify_my_identity\",\n\"verify_source_of_funds\",\n\"verify_top_up\",\n\"virtual_card_not_working\",\n\"visa_or_mastercard\",\n\"why_verify_identity\",\n\"wrong_amount_of_cash_received\",\n\"wrong_exchange_rate_for_cash_withdrawal\"\n],\n\"example_template\": \"Input: {example}\\nOutput: {label}\"\n}\n}\n</code></pre> <pre><code>from autolabel import LabelingAgent\nagent = LabelingAgent(config=config_zero_shot)\nlabels, df, metrics_list = agent.run('../examples/banking/test.csv')\n</code></pre> <p>This zero-shot task execution results in an accuracy of 70.19%. </p> <p>Iterating on this, we compare a fixed few-shot example selection strategy, which randomly chooses k examples from the labeled seedset and appends these same k examples to each prompt for the 1998 items to be labeled. In this case, we use k=10 seed examples per prompt. To use this selection strategy, we need to modify the config:</p> <pre><code>config_fixed_few_shot = {\n\"task_name\": \"BankingComplaintsClassification\",\n\"task_type\": \"classification\",\n\"dataset\": {\n\"label_column\": \"label\",\n\"delimiter\": \",\"\n},\n\"model\": {\n\"provider\": \"openai\",\n\"name\": \"gpt-3.5-turbo\"\n},\n\"prompt\": {\n...\n\"few_shot_examples\": {\"../examples/banking/seed.csv\"},\n\"few_shot_selection\": \"fixed\",\n\"few_shot_num\": 10,\n\"example_template\": \"Input: {example}\\nOutput: {label}\"\n}\n}\n</code></pre> <pre><code>agent = LabelingAgent(config=config_fixed_few_shot)\nlabels, df, metrics_list = agent.run('../examples/banking/test.csv')\n</code></pre> <p>This leads to an accuracy of 73.16%, an improvement of ~3% over the zero-shot baseline.</p> <p>Finally, we compare a semantic similarity example selection strategy, which computes a text embedding for each of the 200 labeled seedset examples. Then, for each of the 1998 items to be labeled, we compute a text embedding and find the k most similar examples from the labeled seedset and append those k examples to the prompt for the current example. This leads to custom examples used for each item to be labeled, with the idea being that more similar examples and their corresponding labels may assist the LLM in labeling. Here is the config change to use semantic similarity as the example selection strategy:</p> <pre><code>config_semantic_similarity = {\n\"task_name\": \"BankingComplaintsClassification\",\n\"task_type\": \"classification\",\n\"dataset\": {\n\"label_column\": \"label\",\n\"delimiter\": \",\"\n},\n\"model\": {\n\"provider\": \"openai\",\n\"name\": \"gpt-3.5-turbo\"\n},\n\"prompt\": {\n...\n\"few_shot_examples\": {\"../examples/banking/seed.csv\"},\n\"few_shot_selection\": \"semantic_similarity\",\n\"few_shot_num\": 10,\n\"example_template\": \"Input: {example}\\nOutput: {label}\"\n}\n}\n</code></pre> <pre><code>agent = LabelingAgent(config=config_semantic_similarity)\nlabels, df, metrics_list = agent.run('../examples/banking/test.csv')\n</code></pre> <p>With semantic similarity example selectiom, we obtain a 79.02% accuracy, a significant increase of ~6% over the fixed-shot strategy.</p> <p>It is almost always advisable to use an example selection strategy over a zero-shot approach in your autolabeling workflows, but the choice of which example selection strategy to use is dependent upon the specific labeling task and dataset. In some cases, there may not be sufficient labeled data to use as a seedset for semantic similarity and so fixed few-shot may be ideal as it requires a small fixed number of labeled examples. In other cases, a semantic similarity example selection strategy may be necessary for labeling tasks that are more complex and require more similar labeled references for the LLM.</p>"},{"location":"guide/accuracy/prompting-better/","title":"Prompting Better","text":"<p>Like most LLM tasks, a critical part of improving LLM performance in autolabeling tasks is selecting a good prompt. Often, this entails finding a good balance between a descriptive set of instructions, while still remaining concise and clear. </p> <p>Consider the following example of refining a prompt used for a classification task on the civil-comments dataset. Each labeling run below included 500 examples and used the same LLM: gpt-3.5-turbo and used a fixed-shot example selection strategy with 4 seed examples.</p> <p></p> <p>First attempt: <pre><code>config = {\n\"task_name\": \"ToxicCommentClassification\",\n\"task_type\": \"classification\",\n\"dataset\": {\n\"label_column\": \"label\",\n\"delimiter\": \",\"\n},\n\"model\": {\n\"provider\": \"openai\",\n\"name\": \"gpt-3.5-turbo\",\n\"compute_confidence\": True\n},\n\"prompt\": {\n\"task_guidelines\": \"You are an expert at identifying toxic comments and understanding if a comment is sexually explicit, obscene, toxic, insults a person, demographic or race. \\nYour job is to correctly label the provided input example into one of the following categories:\\n{labels}\",\n\"labels\": [\n\"toxic\",\n\"not toxic\"\n],\n\"few_shot_examples\": \"../examples/civil_comments/seed.csv\",\n\"few_shot_selection\": \"fixed\",\n\"few_shot_num\": 4,\n\"example_template\": \"Input: {example}\\nOutput: {label}\"\n}\n}\n</code></pre></p> <pre><code>from autolabel import LabelingAgent\nagent = LabelingAgent(config=config)\nlabels, df, metrics_list = agent.run('../examples/civil_comments/test.csv', max_items = 100)\n</code></pre> <p>Accuracy: 68%</p> <p>This first basic prompt seems clear and concise, but only attains a baseline accuracy of 68%. We can analyze some of the errors the LLM is making to get a better idea of how to improve our prompt. </p> <pre><code>df[df['label'] != df['ToxicCommentClassification_llm_label']]\n</code></pre> <p>In doing so, we notice that a vast majority of the errors (97.2%) are misclassifications of civil comments as toxic by the LLM. For instance, one such example comment is:</p> <pre><code>'This is malfeasance by the Administrator and the Board. They are wasting our money!'\n</code></pre> <p>The presence of generally negative words such as \"malfeasance\" and \"wasting\" may be misleading the LLM. Our prompt may need to include details that guide the LLM to correctly identify cases where the vocabulary used could be mistaken as toxic, but the surrounding context suggests that the comment is actually civil.</p> <p>Adding nuance to the prompt:</p> <p>We can replace the prompt in the above config with the following updated guidelines and re-run the labeling task.</p> <pre><code>\"task_guidelines\": \"You are an expert at identifying toxic comments. You aim to act in a fair and balanced manner, where comments that provide fair criticism of something or someone are labelled 'not toxic'. Similarly, criticisms of policy and politicians are marked 'not toxic', unless the comment includes obscenities, racial slurs or sexually explicit material. Any comments that are sexually explicit, obscene, or insults a person, demographic or race are not allowed and labeled 'toxic'.\\nYour job is to correctly label the provided input example into one of the following categories:\\n{labels}\",\n</code></pre> <pre><code>agent = LabelingAgent(config=config)\nlabels, df, metrics_list = agent.run('../examples/civil_comments/test.csv', max_items = 100)\n</code></pre> <p>Accuracy: 74%</p> <p>In this second iteration, we added more detail to the prompt such as addressing the nuances between \"fair criticisms\" vs. toxic comments. These additional details lead to better performance, reaching 74% accuracy. From a similar analysis of the LLM errors, we see that the previous misclassification example, along with several other similar ones, has now been correctly labeled.</p> <p>Further improvements:</p> <p>After subsequently experimenting with a few different variations to this prompt, we do not see significant improvements in performance for this task. As a result, after sufficient iteration of the prompt, it is better to look for performance gains through other modifications to the task configuration. For example, comparing different LLM's can often lead to significant improvements. With the same final prompt above, the text-davinci-003 model achieved 88% accuracy, a 14% increase compared to gpt-turbo-3.5.</p>"},{"location":"guide/llms/benchmarks/","title":"Benchmarks","text":""},{"location":"guide/llms/benchmarks/#benchmarking-llms-for-data-labeling","title":"Benchmarking LLMs for data labeling","text":"<p>Key takeaways from our technical report:</p> <ul> <li>State of the art LLMs can label text datasets at the same or better quality compared to skilled human annotators, but ~20x faster and ~7x cheaper.</li> <li>For achieving the highest quality labels, GPT-4 is the best choice among out of the box LLMs (88.4% agreement with ground truth, compared to 86% for skilled human annotators). </li> <li>For achieving the best tradeoff between label quality and cost, GPT-3.5-turbo, PaLM-2 and open source models like FLAN-T5-XXL are compelling.</li> <li>Confidence based thresholding can be a very effective way to mitigate impact of hallucinations and ensure high label quality.</li> </ul>"},{"location":"guide/llms/llms/","title":"Large Language Models (LLMs)","text":"<p>Autolabel supports multiple LLMs for labeling data. Some LLMs are available by calling an API with the appropriate API keys (OpenAI, Anthropic, etc.) while others can be run locally (such as the ones available on Huggingface). The LLM used to label can be controlled using the <code>provider</code> and <code>name</code> keys in the dictionary specified under <code>model</code> in the input config. </p> <p>Each LLM belongs to an LLM provider -- which refers to the organization or open-source framework through which we are able to access the LLM. A full list of LLM providers and LLMs that are currently supported is provided towards the end of this page.</p> <p>Autolabel makes it easy to try out different LLMs for your task and this page will walk you through how to get started with each LLM provider and model. Separately, we've also benchmarked multiple LLMs across different datasets - you can read the full technical report here [link to blog post] or check out the latest benchmark results here. </p>"},{"location":"guide/llms/llms/#openai","title":"OpenAI","text":"<p>To use models from OpenAI, you can set <code>provider</code> to <code>openai</code> when creating a labeling configuration. The specific model that will be queried can be specified using the <code>name</code> key. Autolabel currently supports the following models from OpenAI:</p> <ul> <li><code>text-davinci-003</code></li> <li><code>gpt-3.5-turbo</code> and <code>gpt-3.5-turbo-0613</code> (4,096 max tokens)</li> <li><code>gpt-3.5-turbo-16k</code> and <code>gpt-3.5-turbo-16k--613</code> (16,384 max tokens)</li> <li><code>gpt-4</code> and <code>gpt-4-0613</code>  (8,192 max tokens)</li> <li><code>gpt-4-32k</code> and <code>gpt-4-32k-0613</code>  (32,768 max tokens)</li> </ul> <p><code>gpt-4</code> set of models are the most capable (and most expensive) from OpenAI, while <code>gpt-3.5-turbo</code> set of models are cheap (but still quite capable). Detailed pricing for these models is available here.</p>"},{"location":"guide/llms/llms/#setup","title":"Setup","text":"<p>To use OpenAI models with Autolabel, make sure to first install the relevant packages by running: <pre><code>pip install refuel-autolabel[openai]\n</code></pre> and also setting the following environment variable: <pre><code>export OPENAI_API_KEY=&lt;your-openai-key&gt;\n</code></pre> replacing <code>&lt;your-openai-key&gt;</code> with your API key, which you can get from here.</p>"},{"location":"guide/llms/llms/#example-usage","title":"Example usage","text":"<p>Here is an example of setting config to a dictionary that will use openai's <code>gpt-3.5-turbo</code> model for labeling. Specifically, note that in the dictionary proivded by the <code>model</code> tag, <code>provider</code> is set to <code>openai</code> and <code>name</code> is set to be <code>gpt-3.5-turbo</code>. <code>name</code> can be switched to use any of the three models mentioned above.</p> <pre><code>config = {\n\"task_name\": \"OpenbookQAWikipedia\",\n\"task_type\": \"question_answering\",\n\"dataset\": {\n\"label_column\": \"answer\",\n\"delimiter\": \",\"\n},\n\"model\": {\n\"provider\": \"openai\",\n\"name\": \"gpt-3.5-turbo\",\n\"params\": {}\n},\n\"prompt\": {\n\"task_guidelines\": \"You are an expert at answering questions.\"\n\"example_template\": \"Question: {question}\\nAnswer: {answer}\"\n}\n}\n</code></pre>"},{"location":"guide/llms/llms/#additional-parameters","title":"Additional parameters","text":"<p>A few parameters can be passed in alongside <code>openai</code> models to tweak their behavior:</p> <ul> <li><code>max_tokens</code> (int): The maximum tokens to sample from the model</li> <li><code>temperature</code> (float): A float between 0 and 2 which indicates the diversity you want in the output. 0 uses greedy sampling (picks the most likely outcome).</li> </ul> <p>These parameters can be passed in via the <code>params</code> dictionary under <code>model</code>. Here is an example: <pre><code>\"model\": {\n\"provider\": \"openai\",\n\"name\": \"gpt-3.5-turbo\",\n\"params\": {\n\"max_tokens\": 512,\n\"temperature\": 0.1\n}\n}\n</code></pre></p>"},{"location":"guide/llms/llms/#anthropic","title":"Anthropic","text":"<p>To use models from Anthropic, you can set the <code>provider</code> to <code>anthropic</code> when creating a labeling configuration. The specific model that will be queried can be specified using the <code>name</code> key. Autolabel currently supports the following models from Anthropic:</p> <ul> <li><code>claude-instant-v1</code></li> <li><code>claude-v1</code></li> </ul> <p><code>claude-v1</code> is a state-of-the-art high-performance model, while <code>claude-instant-v1</code> is a lighter, less expensive, and much faster option. <code>claude-instant-v1</code> is ~6.7 times cheaper than <code>claude-v1</code>, at $1.63/1 million tokens. On the other hand <code>claude-v1</code> costs $11.02/1 million tokens.</p>"},{"location":"guide/llms/llms/#setup_1","title":"Setup","text":"<p>To use Anthropic models with Autolabel, make sure to first install the relevant packages by running: <pre><code>pip install refuel-autolabel[anthropic]\n</code></pre> and also setting the following environment variable: <pre><code>export ANTHROPIC_API_KEY=&lt;your-anthropic-key&gt;\n</code></pre> replacing <code>&lt;your-anthropic-key&gt;</code> with your API key, which you can get from here.</p>"},{"location":"guide/llms/llms/#example-usage_1","title":"Example usage","text":"<p>Here is an example of setting config to a dictionary that will use anthropic's <code>claude-instant-v1</code> model for labeling. Specifically, note that in the dictionary proivded by the <code>model</code> tag, <code>provider</code> is set to <code>anthropic</code> and <code>name</code> is set to be <code>claude-instant-v1</code>. <code>name</code> can be switched to use any of the two models mentioned above.</p> <pre><code>config = {\n\"task_name\": \"OpenbookQAWikipedia\",\n\"task_type\": \"question_answering\",\n\"dataset\": {\n\"label_column\": \"answer\",\n\"delimiter\": \",\"\n},\n\"model\": {\n\"provider\": \"anthropic\",\n\"name\": \"claude-instant-v1\",\n\"params\": {}\n},\n\"prompt\": {\n\"task_guidelines\": \"You are an expert at answering questions.\"\n\"example_template\": \"Question: {question}\\nAnswer: {answer}\"\n}\n}\n</code></pre>"},{"location":"guide/llms/llms/#additional-parameters_1","title":"Additional parameters","text":"<p>A few parameters that can be passed in for <code>anthropic</code> models to control the model behavior:</p> <ul> <li><code>max_tokens_to_sample</code> (int): The maximum tokens to sample from the model</li> <li><code>temperature</code> (float): A float between 0 and 2 which indicates the diversity you want in the output. 0 uses greedy sampling (picks the most likely outcome).</li> </ul> <p>These parameters can be passed in via the <code>params</code> dictionary under <code>model</code>. Here is an example: <pre><code>\"model\": {\n\"provider\": \"anthropic\",\n\"name\": \"claude-instant-v1\",\n\"params\": {\n\"max_tokens_to_sample\": 512,\n\"temperature\": 0.1\n}\n}\n</code></pre></p>"},{"location":"guide/llms/llms/#huggingface","title":"Huggingface","text":"<p>To use models from Huggingface, you can set <code>provider</code> to <code>huggingface_pipeline</code> when creating a labeling configuration. The specific model that will be queried can be specified using the <code>name</code> key. Autolabel currently supports all Sequence2Sequence Language Models on Huggingface. All models available on Huggingface can be found here. Ensure that the model you choose can be loaded using <code>AutoModelForSeq2SeqLM</code>. Here are a few examples:</p> <ul> <li><code>google/flan-t5-small</code> (all flan-t5-* models)</li> <li><code>google/pegasus-x-base</code></li> <li><code>microsoft/prophetnet-large-uncased</code></li> </ul> <p>This will run the model locally on a GPU (if available). You can also specify  quantization strategy to load larger models in lower precision (and thus decreasing memory requirements).</p>"},{"location":"guide/llms/llms/#setup_2","title":"Setup","text":"<p>To use Huggingface models with Autolabel, make sure to first install the relevant packages by running: <pre><code>pip install refuel-autolabel[huggingface]\n</code></pre></p>"},{"location":"guide/llms/llms/#example-usage_2","title":"Example usage","text":"<p>Here is an example of setting config to a dictionary that will use <code>google/flan-t5-small</code> model for labeling via Huggingface. Specifically, note that in the dictionary proivded by the <code>model</code> tag, <code>provider</code> is set to <code>huggingface_pipeline</code> and <code>name</code> is set to be <code>google/flan-t5-small</code>. <code>name</code> can be switched to use any model that satisfies the constraints above.</p> <pre><code>config = {\n\"task_name\": \"OpenbookQAWikipedia\",\n\"task_type\": \"question_answering\",\n\"dataset\": {\n\"label_column\": \"answer\",\n\"delimiter\": \",\"\n},\n\"model\": {\n\"provider\": \"huggingface_pipeline\",\n\"name\": \"google/flan-t5-small\",\n\"params\": {}\n},\n\"prompt\": {\n\"task_guidelines\": \"You are an expert at answering questions.\"\n\"example_template\": \"Question: {question}\\nAnswer: {answer}\"\n}\n}\n</code></pre>"},{"location":"guide/llms/llms/#additional-parameters_2","title":"Additional parameters","text":"<p>A few parameters that can be passed in for <code>huggingface_pipeline</code> models to control the model behavior:</p> <ul> <li><code>max_new_tokens</code> (int) - The maximum tokens to sample from the model</li> <li><code>temperature</code> (float) - A float b/w 0 and 1 which indicates the diversity you want in the output. 0 uses greedy sampling.</li> <li><code>quantize</code> (int) - The model quantization to use. 32 bit by default, but we also support 16 bit and 8 bit support for models which have been hosted on huggingface.</li> </ul> <p>These parameters can be passed in via the <code>params</code> dictionary under <code>model</code>. Here is an example: <pre><code>\"model\": {\n\"provider\": \"huggingface_pipeline\",\n\"name\": \"google/flan-t5-small\",\n\"params\": {\n\"max_new_tokens\": 512,\n\"temperature\": 0.1,\n\"quantize\": 8\n}\n},\n</code></pre></p>"},{"location":"guide/llms/llms/#refuel","title":"Refuel","text":"<p>To use models hosted by Refuel, you can set <code>provider</code> to <code>refuel</code> when creating a labeling configuration. The specific model that will be queried can be specified using the <code>name</code> key. Autolabel currently supports only one model: </p> <ul> <li><code>flan-t5-xxl</code></li> </ul> <p>This is a 13 billion parameter model, which is also available on Huggingface here. However, running such a huge model locally is a challenge, which is why we are currently hosting the model on our servers.</p>"},{"location":"guide/llms/llms/#setup_3","title":"Setup","text":"<p>To use Refuel models with Autolabel, make sure set the following environment variable: <pre><code>export REFUEL_API_KEY=&lt;your-refuel-key&gt;\n</code></pre> replacing <code>&lt;your-refuel-key&gt;</code> with your API key.</p>"},{"location":"guide/llms/llms/#getting-a-refuel-api-key","title":"Getting a Refuel API key","text":"<p>If you're interested in trying one of the LLMs hosted by Refuel, sign up for your Refuel API key by filling out the form here. We'll review your application and get back to you soon!</p>"},{"location":"guide/llms/llms/#example-usage_3","title":"Example usage","text":"<p>Here is an example of setting config to a dictionary that will use Refuel's <code>flan-t5-xxl</code> model. Specifically, note that in the dictionary proivded by the <code>model</code> tag, <code>provider</code> is set to <code>refuel</code> and <code>name</code> is set to be <code>flan-t5-xxl</code>.</p> <pre><code>config = {\n\"task_name\": \"OpenbookQAWikipedia\",\n\"task_type\": \"question_answering\",\n\"dataset\": {\n\"label_column\": \"answer\",\n\"delimiter\": \",\"\n},\n\"model\": {\n\"provider\": \"refuel\",\n\"name\": \"flan-t5-xxl\",\n\"params\": {}\n},\n\"prompt\": {\n\"task_guidelines\": \"You are an expert at answering questions.\"\n\"example_template\": \"Question: {question}\\nAnswer: {answer}\"\n}\n}\n</code></pre>"},{"location":"guide/llms/llms/#additional-parameters_3","title":"Additional parameters","text":"<p>A few parameters that can be passed in for <code>refuel</code> models to control the model behavior. For example:</p> <ul> <li><code>max_new_tokens</code> (int) - The maximum tokens to sample from the model</li> <li><code>temperature</code> (float) - A float b/w 0 and 1 which indicates the diversity you want in the output. 0 uses greedy sampling.</li> </ul> <p>These parameters can be passed in via the <code>params</code> dictionary under <code>model</code>. Here is an example: <pre><code>\"model\": {\n\"provider\": \"refuel\",\n\"name\": \"flan-t5-xxl\",\n\"params\": {\n\"max_new_tokens\": 512,\n\"temperature\": 0.1,\n}\n}\n</code></pre> <code>refuel</code> hosted LLMs support all the parameters that can be passed as a part of GenerationConfig while calling generate functions of Huggingface LLMs. </p>"},{"location":"guide/llms/llms/#google-palm","title":"Google PaLM","text":"<p>To use models from Google, you can set the <code>provider</code> to <code>google</code> when creating a labeling configuration. The specific model that will be queried can be specified using the <code>name</code> key. Autolabel currently supports the following models from Google:</p> <ul> <li><code>text-bison@001</code></li> <li><code>chat-bison@001</code></li> </ul> <p><code>text-bison@001</code> is often more suitable for labeling tasks due to its ability to follow natural language instructions. <code>chat-bison@001</code> is fine-tuned for multi-turn conversations. <code>text-bison@001</code> costs $0.001/1K characters and <code>chat-bison@001</code> costs half that at $0.0005/1K characters. Detailed pricing for these models is available here</p>"},{"location":"guide/llms/llms/#setup_4","title":"Setup","text":"<p>To use Google models with Autolabel, make sure to first install the relevant packages by running: <pre><code>pip install refuel-autolabel[google]\n</code></pre> and also setting up Google authentication locally.</p>"},{"location":"guide/llms/llms/#example-usage_4","title":"Example usage","text":"<p>Here is an example of setting config to a dictionary that will use google's <code>text-bison@001</code> model for labeling. Specifically, note that in the dictionary provided by the <code>model</code> tag, <code>provider</code> is set to <code>google</code> and <code>name</code> is set to be <code>text-bison@001</code>. <code>name</code> can be switched to use any of the two models mentioned above.</p> <pre><code>config = {\n\"task_name\": \"OpenbookQAWikipedia\",\n\"task_type\": \"question_answering\",\n\"dataset\": {\n\"label_column\": \"answer\",\n\"delimiter\": \",\"\n},\n\"model\": {\n\"provider\": \"google\",\n\"name\": \"text-bison@001\",\n\"params\": {}\n},\n\"prompt\": {\n\"task_guidelines\": \"You are an expert at answering questions.\"\n\"example_template\": \"Question: {question}\\nAnswer: {answer}\"\n}\n}\n</code></pre>"},{"location":"guide/llms/llms/#additional-parameters_4","title":"Additional parameters","text":"<p>A few parameters can be passed in alongside <code>google</code> models to tweak their behavior:</p> <ul> <li><code>max_output_tokens</code> (int): Maximum number of tokens that can be generated in the response.</li> <li><code>temperature</code> (float): A float between 0 and 1 which indicates the diversity you want in the output. 0 uses greedy sampling (picks the most likely outcome).</li> </ul> <p>These parameters can be passed in via the <code>params</code> dictionary under <code>model</code>. Here is an example: <pre><code>\"model\": {\n\"provider\": \"google\",\n\"name\": \"text-bison@001\",\n\"params\": {\n\"max_output_tokens\": 512,\n\"temperature\": 0.1\n}\n}\n</code></pre></p>"},{"location":"guide/llms/llms/#model-behavior","title":"Model behavior","text":"<p><code>chat-bison@001</code> always responds in a \"chatty\" manner (example below), often returning more than just the requested label. This can cause problems on certain labeling tasks.</p>"},{"location":"guide/llms/llms/#content-moderation","title":"Content moderation","text":"<p>Both Google LLMs seem to have much stricter content moderation rules than the other supported models. This can cause certain labeling jobs to completely fail as shown in our technical report [add link to technical report]. Consider a different model if your dataset has content that is likely to trigger Google's built-in content moderation.</p>"},{"location":"guide/llms/llms/#provider-list","title":"Provider List","text":"<p>The table lists out all the provider, model combinations that Autolabel supports today:</p> Provider Name openai text-davinci-003 openai gpt-3.5-turbo openai gpt-4 anthropic claude-v1 anthropic claude-instant-v1 huggingface_pipeline seq2seq models refuel flan-t5-xxl google text-bison@001 google chat-bison@001"},{"location":"guide/overview/getting-started/","title":"Getting Started with Autolabel","text":"<p>This page will walk you through your very first labeling task using Refuel Autolabel. Specifically, it'll go over:</p> <ul> <li>Installation</li> <li>Overview of a dataset to label</li> <li>Labeling the dataset using Autolabel</li> </ul>"},{"location":"guide/overview/getting-started/#installation","title":"Installation","text":"<p>Autolabel is available on PyPI and can be installed by running: <pre><code>pip install 'refuel-autolabel[openai]'\n</code></pre></p> <p>Separate from the Autolabel library, you'll also need to install an integration with your favorite LLM provider. In the example below, we'll be using OpenAI, so you'll need to install the OpenAI SDK and set your API key as an environment variable: <pre><code>export OPENAI_API_KEY=\"&lt;your-openai-key&gt;\"\n</code></pre></p> <p>To use a different LLM provider, follow the documentation here. </p>"},{"location":"guide/overview/getting-started/#goal-sentiment-analysis-on-a-movie-review-dataset","title":"Goal: Sentiment Analysis on a Movie Review Dataset","text":"<p>Let's say we wanted to run sentiment analysis on a dataset of movie reviews. We want to train our own ML model, but first, we need to label some data for training.</p> <p>Now, we could label a few hundred examples by hand which would take us a few hours. Instead, let's use Autolabel to get a clean, labeled dataset in a few minutes. </p> <p>A dataset1 containing 200 unlabeled movie reviews is available here, and a couple of examples (with labels) are shown below:</p> text label I was very excited about seeing this film, anticipating a visual excursus on the relation of artistic beauty and nature, containing the kinds of wisdom the likes of \"Rivers and Tides.\" However, that's not what I received. Instead, I get a fairly uninspired film about how human industry is bad for nature. Which is clearly a quite unorthodox claim.The photographer seems conflicted about the aesthetic qualities of his images and the supposed \"ethical\" duty he has to the workers occasionally peopling the images, along the periphery. And frankly, the images were not generally that impressive. And according to this \"artist,\" scale is the basis for what makes something beautiful.In all respects, a stupid film. For people who'd like to feel better about their environmental consciousness ... but not for any one who would like to think about the complexities of the issues surrounding it. negative I loved this movie. I knew it would be chocked full of camp and silliness like the original series. I found it very heart warming to see Adam West, Burt Ward, Frank Gorshin, and Julie Newmar all back together once again. Anyone who loved the Batman series from the 60's should have enjoyed Return to the Batcave. You could tell the actors had a lot of fun making this film, especially Adam West. And I'll bet he would have gladly jumped back into his Batman costume had the script required him to do so. I told a number of friends about this movie who chose not to view it... now they wished they had. I have all of the original 120 episodes on VHS. Now this movie will join my collection. Thank You for the reunion Adam and Burt. positive <p>Our goal is to label the full 200 examples using Autolabel. </p>"},{"location":"guide/overview/getting-started/#labeling-with-autolabel","title":"Labeling with AutoLabel","text":"<p>Autolabel provides a simple 3-step process for labeling data:</p> <ul> <li>Specify the configuration of your labeling task as a JSON</li> <li>Preview the labeling task against your dataset</li> <li>Label your data!</li> </ul>"},{"location":"guide/overview/getting-started/#specify-the-labeling-task-via-configuration","title":"Specify the labeling task via configuration","text":"<p>First, create a JSON file that specifies:</p> <ul> <li>Task: <code>task_name</code> is <code>MovieSentimentReview</code> and the <code>task_type</code> is <code>classification</code></li> <li>LLM: Choice of LLM provider and model - here we are using <code>gpt-3.5-turbo</code> from OpenAI</li> <li>Instructions: These are the labeling guidelines provided to the LLM for labeling</li> </ul> <pre><code>config = {\n\"task_name\": \"MovieSentimentReview\",\n\"task_type\": \"classification\",\n\"dataset\": {\n\"label_column\": \"label\"\n},\n\"model\": {\n\"provider\": \"openai\",\n\"name\": \"gpt-3.5-turbo\"\n},\n\"prompt\": {\n\"task_guidelines\": \"You are an expert at analyzing the sentiment of movie reviews. Your job is to classify the provided movie review into one of the following labels: {labels}\",\n\"labels\": [\n\"positive\",\n\"negative\",\n\"neutral\",\n],\n\"few_shot_examples\": [\n{\n\"example\": \"I got a fairly uninspired stupid film about how human industry is bad for nature.\",\n\"label\": \"negative\"\n},\n{\n\"example\": \"I loved this movie. I found it very heart warming to see Adam West, Burt Ward, Frank Gorshin, and Julie Newmar together again.\",\n\"label\": \"positive\"\n},\n{\n\"example\": \"This movie will be played next week at the Chinese theater.\",\n\"label\": \"neutral\"\n}\n],\n\"example_template\": \"Exanple: {example}\\Label: {label}\"\n}\n}\n</code></pre>"},{"location":"guide/overview/getting-started/#preview-the-labeling-against-your-dataset","title":"Preview the labeling against your dataset","text":"<p>First import <code>autolabel</code>, create a <code>LabelingAgent</code> object and then run the <code>plan</code> command against the dataset (available here and can be downloaded through the <code>autolabel.get_data</code> function):</p> <pre><code>from autolabel import LabelingAgent, get_data\nget_data('movie_reviews')\nagent = LabelingAgent(config)\nagent.plan('test.csv')\n</code></pre> <p>This produces: <pre><code>Computing embeddings... \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100/100 0:00:00 0:00:00\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Total Estimated Cost     \u2502 $0.538  \u2502\n\u2502 Number of Examples       \u2502 200     \u2502\n\u2502 Average cost per example \u2502 0.00269 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Prompt Example \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nYou are an expert at analyzing the sentiment of moview reviews. Your job is to classify the provided movie review as positive or negative.\n\nYou will return the answer with just one element: \"the correct label\"\n\nNow I want you to label the following example:\nInput: I was very excited about seeing this film, anticipating a visual excursus on the relation of artistic beauty and nature, containing the kinds of wisdom the likes of \"Rivers and Tides.\" However, that's not what I received. Instead, I get a fairly uninspired film about how human industry is bad for nature. Which is clearly a quite unorthodox claim.&lt;br /&gt;&lt;br /&gt;The photographer seems conflicted about the aesthetic qualities of his images and the supposed \"ethical\" duty he has to the workers occasionally peopling the images, along the periphery. And frankly, the images were not generally that impressive. And according to this \"artist,\" scale is the basis for what makes something beautiful.&lt;br /&gt;&lt;br /&gt;In all respects, a stupid film. For people who'd like to feel better about their environmental consciousness ... but not for any one who would like to think about the complexities of the issues surrounding it.\nOutput:\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n</code></pre></p> <p>This shows you:</p> <ul> <li>Number of examples to be labeled in the dataset: <code>200</code></li> <li>Estimated cost of running this labeling task: <code>&lt;$1</code></li> <li>Exact prompt being sent to the LLM</li> </ul> <p>Having previewed the labeling, we are ready to start labeling. </p>"},{"location":"guide/overview/getting-started/#label-your-dataset","title":"Label your dataset","text":"<p>Now, you can use the <code>run</code> command to label:</p> <pre><code>labels, output_df, metrics = agent.run('docs/assets/movie_reviews.csv')\n\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 200/200 0:04:01 0:00:00\n</code></pre> <p>This takes just a few minutes to run, and returns the labeled data as a Pandas DataFrame (<code>output_df</code> here). We can explore this by running: <pre><code>output_df.head()\n&gt;\ntext  ... MovieSentimentReview_llm_label\n0  I was very excited about seeing this film, ant...  ...                       negative\n1  Serum is about a crazy doctor that finds a ser...  ...                       negative\n2  This movie was so very badly written. The char...  ...                       negative\n3  Hmmmm, want a little romance with your mystery...  ...                       negative\n4  I loved this movie. I knew it would be chocked...  ...                       positive\n[5 rows x 4 columns]\n</code></pre></p> <p>At this point, we have a labeled dataset ready, and we can begin training our ML models. </p>"},{"location":"guide/overview/getting-started/#summary","title":"Summary","text":"<p>In this simple walkthrough, we have installed <code>autolabel</code>, gone over an example dataset to label (sentiment analysis for moview reviews) and used <code>autolabel</code> to label this dataset in just a few minutes. </p> <p>We hope that this gives you a glimpse of what you can do with Refuel. There are many other labeling tasks available within Autolabel, and if you have any questions, join our community here or open an issue on Github. </p> <ol> <li> <p>Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. (2011). Learning Word Vectors for Sentiment Analysis. The 49th Annual Meeting of the Association for Computational Linguistics (ACL 2011).\u00a0\u21a9</p> </li> </ol>"},{"location":"guide/overview/tutorial-classification/","title":"Tutorial - Toxic comment classification","text":"<p>This is a detailed tutorial that walks you through many features of the Autolabel library while solving a problem faced by many companies - labeling toxic comments for content moderation.</p> <p>We will be using OpenAI's <code>gpt-3.5-turbo</code> for the data labeling, and Refuel's LLM for confidence estimation.</p>"},{"location":"guide/overview/tutorial-classification/#autolabel-installation","title":"Autolabel installation","text":"<p>Since we'll be using OpenAI along with Autolabel, we can install all necessary libraries by simply running: <pre><code>pip install 'refuel-autolabel[openai]'\n</code></pre></p> <p>Now, we can set our OpenAI key as an environment variable to get started. You can always use an LLM of your choice - see more optioons and installation instructions here. </p>"},{"location":"guide/overview/tutorial-classification/#download-and-review-dataset","title":"Download and review dataset","text":"<p>We'll be using a dataset called Civil Comments, which is available through Autolabel. You can download it locally, by simply running: <pre><code>from autolabel import get_data\nget_data('civil_comments')\n</code></pre></p> <p>The output is: <pre><code>Downloading seed example dataset to \"seed.csv\"...\n100% [..............................................................................] 65757 / 65757\nDownloading test dataset to \"test.csv\"...\n100% [............................................................................] 610663 / 610663\n</code></pre></p> <p>This results in two files being downloaded locally:</p> <ul> <li><code>seed.csv</code>: small dataset with labels that we'll rely on as helpful examples.</li> <li><code>test.csv</code>: larger dataset that we are trying to label.</li> </ul> <p>A few examples are shown below:</p> label examples <code>toxic</code> \"The ignorance and bigotry comes from your post!\" <code>not toxic</code> \"This is malfeasance by the Administrator and the Board. They are wasting our money!\""},{"location":"guide/overview/tutorial-classification/#start-the-labeling-process","title":"Start the labeling process","text":"<p>Labeling with Autolabel is a 3-step process:</p> <ul> <li>First, we specify a labeling configuration (see <code>config</code> object below) and create a <code>LabelingAgent</code></li> <li>Next, we do a dry-run on our dataset using the LLM specified in <code>config</code> by running <code>agent.plan</code></li> <li>Finally, we run the labeling with <code>agent.run</code></li> </ul>"},{"location":"guide/overview/tutorial-classification/#experiment-1-try-simple-labeling-guidelines","title":"Experiment #1: Try simple labeling guidelines","text":"<p>Define the configuration file below: <pre><code>config = {\n\"task_name\": \"ToxicCommentClassification\",\n\"task_type\": \"classification\", # classification task\n\"dataset\": {\n\"label_column\": \"label\",\n},\n\"model\": {\n\"provider\": \"openai\",\n\"name\": \"gpt-3.5-turbo\" # the model we want to use\n},\n\"prompt\": {\n# very simple instructions for the LLM\n\"task_guidelines\": \"Does the provided comment contain 'toxic' language? Say toxic or not toxic.\",\n\"labels\": [ # list of labels to choose from\n\"toxic\",\n\"not toxic\"\n],\n\"example_template\": \"Input: {example}\\nOutput: {label}\"\n}\n}\n</code></pre></p> <p>Now, we do the dry-run with <code>agent.plan</code>: <pre><code>from autolabel import LabelingAgent\nagent = LabelingAgent(config)\nagent.plan('test.csv')\n</code></pre></p> <p>Output: <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Total Estimated Cost     \u2502 $4.4442 \u2502\n\u2502 Number of Examples       \u2502 2000    \u2502\n\u2502 Average cost per example \u2502 $0.0022 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Prompt Example \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nDoes the provided comment contain 'toxic' language? Say toxic or not toxic.\nYou will return the answer with just one element: \"the correct label\"\nNow I want you to label the following example:\nInput: [ Integrity means that you pay your debts.]. Does this apply to President Trump too?\nOutput: \n</code></pre></p> <p>Finally, we run the data labeling: <pre><code>labels, df, metrics = agent.run('test.csv', max_items=100)\n</code></pre></p> <pre><code>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 support \u2503 threshold \u2503 accuracy \u2503 completion_rate \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 100     \u2502 -inf      \u2502 0.54     \u2502 1.0             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>54% accuracy is not very good! Let's see if we can improve this further!</p>"},{"location":"guide/overview/tutorial-classification/#experiment-2-few-shot-prompting-to-provide-helpful-examples","title":"Experiment #2: Few-shot prompting to provide helpful examples","text":"<p>Similar to how human labelers find it helpful to use relevant examples when making a decision, LLM performance for labeling also goes up when choosing helpful examples in the prompt. For this next experiment, we will pick a few helpful examples from <code>seed.csv</code>. More information on few-shot prompting can be found here.</p> <p>We take the previous config, and just update the following fields: <pre><code>{\n\"task_name\": \"ToxicCommentClassification\",\n\"task_type\": \"classification\",\n\"dataset\": {\n\"label_column\": \"label\",\n\"delimiter\": \",\"\n},\n\"model\": {\n\"provider\": \"openai\",\n\"name\": \"gpt-3.5-turbo\",\n},\n\"prompt\": {\n\"task_guidelines\":  \"Does the provided comment contain 'toxic' language? Say toxic or not toxic.\",\n\"labels\": [\n\"toxic\",\n\"not toxic\"\n],\n\"few_shot_examples\": [\n{\n\"example\": \"It's ridiculous that these guys are being called 'protesters'. Being armed is a threat of violence, which makes them terrorists.\",\n\"label\": \"toxic\"\n},\n{\n\"example\": \"This is so cool. It's like, 'would you want your mother to read this??' Really great idea, well done!\",\n\"label\": \"not toxic\"\n},\n{\n\"example\": \"This bitch is nuts. Who would read a book by a woman\",\n\"label\": \"toxic\"\n},\n{\n\"example\": \"It was a great show. Not a combo I'd of expected to be good together but it was.\",\n\"label\": \"not toxic\"\n}\n],\n\"few_shot_selection\": \"fixed\",\n\"few_shot_num\": 4,\n\"example_template\": \"Input: {example}\\nOutput: {label}\"\n}\n}\n</code></pre></p> <p>That's it! We are now ready to create a <code>LabelingAgent</code> and run the same <code>agent.plan</code> and <code>agent.run</code> commands.</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Total Estimated Cost     \u2502 $4.9442 \u2502\n\u2502 Number of Examples       \u2502 2000    \u2502\n\u2502 Average cost per example \u2502 $0.0025 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Prompt Example \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nDoes the provided comment contain 'toxic' language? Say toxic or not toxic.\nYou will return the answer with just one element: \"the correct label\"\nSome examples with their output answers are provided below:\nInput: It's ridiculous that these guys are being called 'protesters'. Being armed is a threat of violence, which makes them terrorists.\nOutput: toxic\nInput: This is so cool. It's like, 'would you want your mother to read this??' Really great idea, well done!\nOutput: not toxic\nInput: This bitch is nuts. Who would read a book by a woman\nOutput: toxic\nInput: It was a great show. Not a combo I'd of expected to be good together but it was.\nOutput: not toxic\nNow I want you to label the following example:\nInput: [ Integrity means that you pay your debts.] Does this apply to President Trump too?\nOutput:\n</code></pre> <p>With additional examples, the cost has gone up slightly. Now, we run the labeling with:</p> <pre><code>labels, df, metrics = agent.run('test.csv', max_items=100)`:\n</code></pre> <pre><code>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 support \u2503 threshold \u2503 accuracy \u2503 completion_rate \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 100     \u2502 -inf      \u2502 0.68     \u2502 1.0             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Nice! We improved performance from 54% to 68% by providing a few examples to the LLM.</p>"},{"location":"guide/overview/tutorial-classification/#experiment-3-improving-task-guidelines-after-reviewing-errors-prompt-engineering","title":"Experiment #3: Improving task guidelines after reviewing errors (prompt engineering)","text":"<p>Typically, you can improve the accuracy by reviewing mistakes and updating the task guidelines (see another example here). You can review some of the mistakes from the previous run by looking at the output Pandas DataFrame produced called <code>df</code>: <pre><code>df[df['label'] != df['ToxicCommentClassification_llm_label']].head(10)\n</code></pre></p> <p>Let's say we update our task guidelines to be more explicit about how should the LLM make the decision about whether a comment is toxic or not:</p> <pre><code>{\n\"task_name\": \"ToxicCommentClassification\",\n\"task_type\": \"classification\",\n\"dataset\": {\n\"label_column\": \"label\",\n\"delimiter\": \",\"\n},\n\"model\": {\n\"provider\": \"openai\",\n\"name\": \"gpt-3.5-turbo\",\n},\n\"prompt\": {\n\"task_guidelines\": \"You are an expert at identifying toxic comments. You aim to act in a fair and balanced manner, where comments that provide fair criticism of something or someone are labelled 'not toxic'. Similarly, criticisms of policy and politicians are marked 'not toxic', unless the comment includes obscenities, racial slurs or sexually explicit material. Any comments that are sexually explicit, obscene, or insults a person, demographic or race are not allowed and labeled 'toxic'. \\nYour job is to correctly label the provided input example into one of the following categories:\\n{labels}\",\n\"labels\": [\n\"toxic\",\n\"not toxic\"\n],\n\"few_shot_examples\": [\n{\n\"example\": \"It's ridiculous that these guys are being called 'protesters'. Being armed is a threat of violence, which makes them terrorists.\",\n\"label\": \"toxic\"\n},\n{\n\"example\": \"This is so cool. It's like, 'would you want your mother to read this??' Really great idea, well done!\",\n\"label\": \"not toxic\"\n},\n{\n\"example\": \"This bitch is nuts. Who would read a book by a woman\",\n\"label\": \"toxic\"\n},\n{\n\"example\": \"It was a great show. Not a combo I'd of expected to be good together but it was.\",\n\"label\": \"not toxic\"\n}\n],\n\"few_shot_selection\": \"fixed\",\n\"few_shot_num\": 4,\n\"example_template\": \"Input: {example}\\nOutput: {label}\"\n}\n}\n</code></pre> <p>Now, when we run <code>agent.run</code>, we get the following results:</p> <pre><code>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 support \u2503 threshold \u2503 accuracy \u2503 completion_rate \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 100     \u2502 -inf      \u2502 0.78     \u2502 1.0             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>We now hit an accuracy of 78%, which is very promising! If we spend more time improving the guidelines or choosing different examples, we can push accuracy even further.</p>"},{"location":"guide/overview/tutorial-classification/#experiment-4-experimenting-with-llms","title":"Experiment #4: Experimenting with LLMs","text":"<p>We've iterated a fair bit on prompts, and few-shot examples. Let's evaluate a few different LLMs provided by the library out of the box. For example, we observe that we can boost performance even further by using <code>text-davinci-003</code>: </p> <pre><code>{\n\"task_name\": \"ToxicCommentClassification\",\n\"task_type\": \"classification\",\n\"dataset\": {\n\"label_column\": \"label\",\n\"delimiter\": \",\"\n},\n\"model\": {\n\"provider\": \"openai\",\n\"name\": \"text-davinci-003\",\n},\n\"prompt\": {\n\"task_guidelines\": \"You are an expert at identifying toxic comments. You aim to act in a fair and balanced manner, where comments that provide fair criticism of something or someone are labelled 'not toxic'. Similarly, criticisms of policy and politicians are marked 'not toxic', unless the comment includes obscenities, racial slurs or sexually explicit material. Any comments that are sexually explicit, obscene, or insults a person, demographic or race are not allowed and labeled 'toxic'. \\nYour job is to correctly label the provided input example into one of the following categories:\\n{labels}\",\n\"labels\": [\n\"toxic\",\n\"not toxic\"\n],\n\"few_shot_examples\": [\n{\n\"example\": \"It's ridiculous that these guys are being called 'protesters'. Being armed is a threat of violence, which makes them terrorists.\",\n\"label\": \"toxic\"\n},\n{\n\"example\": \"This is so cool. It's like, 'would you want your mother to read this??' Really great idea, well done!\",\n\"label\": \"not toxic\"\n},\n{\n\"example\": \"This bitch is nuts. Who would read a book by a woman\",\n\"label\": \"toxic\"\n},\n{\n\"example\": \"It was a great show. Not a combo I'd of expected to be good together but it was.\",\n\"label\": \"not toxic\"\n}\n],\n\"few_shot_selection\": \"fixed\",\n\"few_shot_num\": 4,\n\"example_template\": \"Input: {example}\\nOutput: {label}\"\n}\n}\n</code></pre> <p>While the per token API price for this model is higher, we're able to boost the accuracy to 88%!</p> <pre><code>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 support \u2503 threshold \u2503 accuracy \u2503 completion_rate \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 100     \u2502 -inf      \u2502 0.88     \u2502 1.0             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"guide/overview/tutorial-classification/#experiment-5-using-confidence-scores","title":"Experiment #5: Using confidence scores","text":"<p>Refuel provides LLMs that can compute confidence scores for every label, if the LLM you've chosen doesn't provide token-level log probabilities. This is helpful, because you can calibrate a confidence threshold for your labeling task, and then route less confident labels to humans, while you still get the benefits of auto-labeling for the confident examples. Let's see how this works. </p> <p>First, set your Refuel API key as an environment variable (and if you don't have this key yet, sign up here). <pre><code>os.environ['REFUEL_API_KEY'] = '&lt;your-api-key&gt;'\n</code></pre></p> <p>Now, update your configuration: <pre><code>config[\"model\"][\"compute_confidence\"] = True\n</code></pre></p> <p>Finally, let's run <code>agent.run</code> as before - this produces the table below: <pre><code>Metric: auroc: 0.8858\nActual Cost: 0.0376\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 support \u2503 threshold \u2503 accuracy \u2503 completion_rate \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 100     \u2502 -inf      \u2502 0.78     \u2502 1.0             \u2502\n\u2502 1       \u2502 0.9988    \u2502 1.0      \u2502 0.01            \u2502\n\u2502 12      \u2502 0.9957    \u2502 1.0      \u2502 0.12            \u2502\n\u2502 13      \u2502 0.9949    \u2502 0.9231   \u2502 0.13            \u2502\n\u2502 54      \u2502 0.9128    \u2502 0.9815   \u2502 0.54            \u2502\n\u2502 55      \u2502 0.9107    \u2502 0.9636   \u2502 0.55            \u2502\n\u2502 63      \u2502 0.6682    \u2502 0.9683   \u2502 0.63            \u2502\n\u2502 66      \u2502 0.6674    \u2502 0.9242   \u2502 0.66            \u2502\n\u2502 67      \u2502 0.6673    \u2502 0.9254   \u2502 0.67            \u2502\n\u2502 69      \u2502 0.6671    \u2502 0.8986   \u2502 0.69            \u2502\n\u2502 71      \u2502 0.6667    \u2502 0.9014   \u2502 0.71            \u2502\n\u2502 72      \u2502 0.6667    \u2502 0.8889   \u2502 0.72            \u2502\n\u2502 78      \u2502 0.4819    \u2502 0.8974   \u2502 0.78            \u2502\n\u2502 79      \u2502 0.4774    \u2502 0.8861   \u2502 0.79            \u2502\n\u2502 87      \u2502 0.4423    \u2502 0.8966   \u2502 0.87            \u2502\n\u2502 100     \u2502 0.0402    \u2502 0.78     \u2502 1.0             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre></p> <p>The rows in this table show labeling performance at different confidence thresholds, and set an autolabeling confidence threshold at the desired accuracy. For instance, from the table above we can set the confidence threshold at 0.6682 which allows us to label at 96% accuracy with a completion rate of 63%.</p>"},{"location":"guide/overview/tutorial-classification/#final-thoughts","title":"Final thoughts","text":"<p>Hopefully, this tutorial was helpful in understanding how Autolabel can help you label datasets quickly and at high quality. A Jupyter notebook for this tutorial can be found here.</p> <p>You can find more example notebooks here, including for tasks such as question answering, named entity recognition, etc. </p> <p>Drop us a message in our Discord if you want to chat with us, or go to Github to report any issues! </p>"},{"location":"guide/reliability/llm-output-caching/","title":"LLM Output Caching","text":"<p>To help reduce time and cost when iterating the prompt for better labeling accuracy, we cache the calls made to the LLM.</p>"},{"location":"guide/reliability/llm-output-caching/#cache-entry","title":"Cache Entry","text":"<p>A cache entry has the following attributes:</p> <ul> <li><code>Model Name</code></li> <li><code>Prompt</code></li> <li><code>Model Params</code></li> </ul> <p>This means that anytime there are changes to either the language model or the prompt, the model will be called for producing label. Also, changes to the model parameters like the <code>max_tokens</code> or <code>temperature</code> could affect the label output and therefore modifying such parameters result in new calls to the model instead of using cached calls.</p>"},{"location":"guide/reliability/llm-output-caching/#caching-storage","title":"Caching Storage","text":"<p>The cached entries are stored in a SQLite database. We will be adding support for In Memory cache and Redis cache in future.</p>"},{"location":"guide/reliability/llm-output-caching/#disable-caching","title":"Disable Caching","text":"<p>The cache is enabled by default and if you wish to disable it, you can set <code>cache=False</code> when initializing the LabelingAgent.</p> <pre><code>from autolabel import LabelingAgent\nagent = LabelingAgent(config='examples/configs/civil_comments.json', cache=False)\n</code></pre>"},{"location":"guide/reliability/state-management/","title":"State Management","text":"<p>Labeling a large dataset can take some time and if you're running the task on a Jupyter notebook and your machine decides to sleep during the time, it could be really frustrating. (we've been there! ).</p> <p>Therefore, we periodically save the progress of the labeling task in a SQLite database, so if the task is interrupted, you can resume it from where you left off.</p>"},{"location":"guide/reliability/state-management/#task-run-state","title":"Task Run State","text":"<p>When a labeling task is triggered, a task run entry gets initialized inside the database. We maintain the dataset index till where the labels have been computed. After every small chunk (size 5) of data gets labeled, the dataset index gets updated and the labels are persisted.</p> <p>In case the labeling process get interrupted/terminated and you trigger the task with the same parameters again, the library first checks for a previous instance of the same task.</p> <p>If there was an incomplete task present, you would be prompted with details of the previous run and asked to resume the task. If you choose to resume the previous task, it gets loaded into the memory and resumed from previous state otherwise the previous entry gets deleted.</p>"},{"location":"guide/reliability/state-management/#deep-dive","title":"Deep Dive","text":"<p>You'd likely never have to interact with the database directly but in case you wish to look at the state of the database, you can do that using any CLI or GUI that supports SQL. The database is saved in the same directory from where you run the LabelingAgent notebook and is named <code>.autolabel.db</code>.</p> <p>We have the following tables:</p> <ul> <li><code>datasets</code>: Stores the dataset file information</li> <li><code>tasks</code>: Stores the labeling task attributes</li> <li><code>task_runs</code>: Stores the current state of a labeling task run</li> <li><code>annotations</code>: Stores the LLM annotation corresponding to the task run</li> <li><code>generation_cache</code>: Cache for the LLM calls</li> </ul>"},{"location":"guide/resources/configs/","title":"Configs","text":"<p>Each labeling run with the autolabel library requires a config to be specified. The config has 5 top-level keys and several nested keys, many of which are optional.</p>"},{"location":"guide/resources/configs/#task-name","title":"Task Name","text":"<p>The task name is just a user-provided name for the labeling task and is only used to construct display names for various labeling artifacts (i.e. column names in the output labeled csv/dataframe)</p> Example<pre><code>\"task_name\": \"CompanyEntityMatch\"\n</code></pre>"},{"location":"guide/resources/configs/#task-type","title":"Task Type","text":"<p>The task type determines how the Autolabel library should construct the request to the LLM as well as how the LLM response should be parsed and which metrics should be computed. Currently, the library supports the following task types:</p> <ul> <li>entity_matching</li> <li>classification</li> <li>named_entity_recognition</li> <li>question_answering</li> </ul> Example<pre><code>\"task_type\": \"entity_matching\"\n</code></pre>"},{"location":"guide/resources/configs/#dataset","title":"Dataset","text":"<p>The dataset config contains information about the dataset to be labeled. Specifically, there are 4 dataset config keys:</p> <ol> <li>label_column (optional): The label column specifies the column containing the labels for each item to use for metric computation if labels are available for the dataset</li> <li>explanation_column (optional): The explanation column specifies the column containing explanations for each item to use for chain-of-thought prompting if it is enabled in the config.</li> <li>delimiter (optional): This key specifies the delimiter used for parsing the dataset CSV. By default, it is assumed to be a comma: \",\"</li> <li>text_column (required for named entity recognition): The text column is only necessary for named entity recognition tasks and specifies the column containing the text that we intend to label and is used for determining text spans.</li> </ol> Example 1: Classification task<pre><code>\"dataset\": {\n\"label_column\": \"label\",\n\"delimiter\": \",\"\n}\n</code></pre> Example 2: Chain of thought<pre><code>   \"dataset\": {\n\"label_column\": \"answer\",\n\"explanation_column\": \"explanation\",\n\"delimiter\": \",\"\n}\n</code></pre> Example 3: Named entity recognition task<pre><code>   \"dataset\": {\n\"label_column\": \"CategorizedLabels\",\n\"text_column\": \"example\",\n\"delimiter\": \",\"\n}\n</code></pre>"},{"location":"guide/resources/configs/#model","title":"Model","text":"<p>The model config contains information about the LLM provider and specific model we intend to use for labeling. There are 4 model config keys:</p> <ol> <li>provider: This key specifies the LLM provider.</li> <li>name: The model name specifies which of the provider's models to use for generating labels.</li> <li>params (optional): Params is a dictionary that allows the user to configure model-specific paramaters. Here is an example model params dict:</li> <li>max_tokens: Max tokens specifies the maximum total input and output tokens for each LLM call.</li> <li>temperature: The temperature controls how deterministic the LLM responses should be.</li> <li> <p>model_kwargs: The model kwargs contains the logprobs key which, when present, configures the LLM request to have the LLM return log probabilities</p> </li> <li> <p>compute_confidence (optional): This boolean determines whether to compute and output confidence scores.</p> </li> </ol> Example 1: Compute confidence<pre><code>\"model\": {\n\"provider\": \"openai\",\n\"name\": \"gpt-3.5-turbo\",\n\"compute_confidence\": True\n}\n</code></pre> Example 2: Defining model params<pre><code>\"model\": {\n\"provider\": \"openai\",\n\"name\": \"gpt-3.5-turbo\",\n\"params\": {\n\"max_tokens\": 512,\n\"temperature\": 0.1\n}\n}\n</code></pre>"},{"location":"guide/resources/configs/#prompt","title":"Prompt","text":"<p>The prompt config contains information about how the prompt should be constructed in the request to the LLM. There are 9 prompt config keys.</p> <ol> <li>task_guidelines: The task guidelines should contain a description of the specific labeling task, including any nuanced details about how to correctly label each item.</li> <li>labels (required for some tasks): The labels defines the full list of labels for the model.</li> <li>few_shot_examples (optional): The few shot examples is either a list or path to the CSV of possible seed examples to append to the prompt.</li> <li> <p>few_shot_selection (optional): The few shot selection is the specific strategy to use for selecting examples to use in the prompt. Currently, there are 3 example selection strategies implemented:</p> <ul> <li>fixed</li> <li>semantic_similarity</li> <li>max_marginal_relevance</li> </ul> </li> <li> <p>few_shot_num (optional): The few shot number determines how many seed examples to select and include in the prompt</p> </li> <li>example_template: The example template determines how each example should be formatted in the prompt. You can reference columns from the dataset by wrapping the column name with curly braces</li> <li>output_guidelines (optional): The output guidelines specify how the output should be returned by the LLM (i.e. just return the label vs. format as CSV). It is not recommended to add output guidelines for most use cases as default guidelines are already set.</li> <li>output_format (optional): The format of the output is either \"csv\" or \"json\", but it is not recommended to override the default selection.</li> <li>chain_of_thought (optional): This boolean determines whether to use chain of thought in the prompt or not.</li> </ol> Example 1: Classification task<pre><code>\"prompt\": {\n\"task_guidelines\": \"You are an expert at identifying toxic comments. You aim to act in a fair and balanced manner, where comments that provide fair criticism of something or someone are labelled 'not toxic'. Similarly, criticisms of policy and politicians are marked 'not toxic', unless the comment includes obscenities, racial slurs or sexually explicit material. Any comments that are sexually explicit, obscene, or insults a person, demographic or race are not allowed and labeled 'toxic'. \\nYour job is to correctly label the provided input example into one of the following categories:\\n{labels}\",\n\"labels\": [\n\"toxic\",\n\"not toxic\"\n],\n\"example_template\": \"Input: {example}\\nOutput: {label}\"\n}\n</code></pre> Example 2: Use seed examples<pre><code>   \"prompt\": {\n\"task_guidelines\": \"You are provided with descriptions of companies from their websites, and wikipedia pages. Your job is to categorize whether the descriptions are about the same company (duplicate) or different companies (not duplicate). Your answer must be from one of the following options:\\n{labels}\",\n\"labels\": [\n\"not duplicate\",\n\"duplicate\"\n],\n\"example_template\": \"Company 1 description: {entity1}\\nCompany 2 description: {entity2}\\nDuplicate or not: {label}\",\n\"few_shot_examples\": [\n{\n\"entity1\": \"lac wisconsin branding 95 1 &amp; 96 1 the rock frequency 96.1 mhz translator s 95.1 w236ag fond du lac first air date 1965 as wcwc fm at 95.9 format mainstream rock erp 4 000 watts haat 123 meters 404 ft class a facility id 54510 transmitter coordinates 43 49 10.00 n 88 43 20.00 w 43.8194444 n 88.7222222 w 43.8194444 ; 88.7222222 coordinates 43 49 10.00 n 88 43 20.00 w 43.8194444 n 88.7222222 w 43.8194444 ; 88.7222222 former callsigns wcwc fm 1965 1980 wyur 1980 1994 former frequencies 95.9 mhz 1965 affiliations cbs radio network westwood one premiere radio networks owner radio plus inc. sister stations wfdl wfdl fm wmdc webcast listen live website 961tcx . com studios in fond du lac wtcx 96.1 fm 95 1 &amp; 96 1 the rock is a radio station broadcasting a mainstream rock music format . 1 licensed to ripon wisconsin usa the station is currently owned by radio plus inc. and features programing from cbs radio network dial global and premiere radio networks . 2 wtcx was originally on 95.9 mhz . be\",\n\"entity2\": \"closings contact next racing rocks local news breaking wiaa releases football playoffs matchups and brackets october 15 2016 local news here are the full brackets for the state of wisconsin division 1 2 seed fond du lac hosts 7 seed milwaukee washington friday october 21 at 7pm division 5 3 seed wla hosts 6 seed ... read more 10 15 16 fdl man injured in hit and run car vs. bike crash october 15 2016 local news a fond du lac man received non life threatening injuries in a car versus bicycle hit and run crash in dodge county . the dodge county sheriff s office says shortly after 8pm friday a car ... read more 10 15 16 ripon woman remains in critical condition following one vehicle crash october 15 2016 local news a ripon woman injured in a one vehicle crash after apparently falling asleep at the wheel remains in critical condition . the fond du lac county sheriff s office says 29 year old raquel amador ... read more wiaa releases football groupings october 15 2016 local news 2016 wiaa fo\",\n\"label\": \"duplicate\"\n},\n{\n\"entity1\": \"stacy spikes hamet watt headquarters new york city united states website http www.moviepass.com moviepass is a subscription based service for going to movie theaters available in the united states . the service gives members across the country the ability to see up to one 2d movie every 24 hours for a fixed monthly fee . members may choose which theaters they wish to attend and there are no blackout dates . moviepass works in nearly all movie theaters that accept the mastercard credit card making it one of the largest subscription based theater networks in america . prices vary by local market and start at 30 per month . moviepass was launched in february 2011 and is headquartered in new york city . 1 contents 1 service 2 purchasing a ticket 3 history 4 media coverage 5 references service edit the moviepass service works via a smartphone app iphone android and a specially designed reloadable debit card which is mailed to new subscribers when they sign up . purchasing a ticket edit in o\",\n\"entity2\": \"repair buy warranty get service buy warranty home warranty pricing &amp; plans planning on moving home matters blog what s covered service professionals customer reviews benefits faqs appliance discount contract policies decor cost savers lawn &amp; garden lifestyle quick tips real estate repair &amp; maintenance tech close home warranty learn more what s covered service professionals faqs pricing and plans get a quote see plans planning on moving real estate plans buying a home selling a home home matters blog decor cost savers lawn &amp; garden lifestyle quick tips real estate repair &amp; maintenance tech our partner sites real estate professionals contractors 888 429 8247 email us log in back to top get a personalized quote explore plans in your area get covered in 3 easy steps . please correct highlighted fields request service log in create account oven on the fritz appliance breakdowns happen . get covered . get a personalized quote explore plans in your area get covered in 3 easy steps . please co\",\n\"label\": \"not duplicate\"\n},\n{\n\"entity1\": \"of over 110 gyms worldwide including 86 franchise locations in ma pa ny nj ct wa or ca tx fl ky va puerto rico and australia and is rapidly expanding across the u.s. and around the globe . contents 1 history 2 description 3 references 4 external links history edit crunch was founded in a basement level aerobics studio in new york city s east village in 1989 by doug levine . 1 with the collaboration of fitness instructors the group fitness programming was started at crunch . offerings such as hip hop aerobics co ed action wrestling and cyked yoga cycling were introduced . 2 in clubs members have access to innovative group fitness classes state of the art equipment personal and group training full service locker rooms and much more . select locations offer an exclusive crunch retail line that can also be purchased from the crunch online store . 3 in january 2014 crunch released its online workout extension called crunch live . this subscription based online video library has over 95 work\",\n\"entity2\": \"gallery esp en best rate guarantee check availability call us room only 1 800 990 8250 hotel air 1 800 219 2727 canada 1 855 478 2811 airport transportation travel agents close best rate guaranteebook your all inclusive stay hotel hotel air arrive departure adults 1 2 3 4 5 6 7 8 children 0 1 2 3 4 5 6 7 8 select property pacifica golf &amp; spa resort the towers at pacifica sunset beach golf &amp; spa resort ros resort &amp; spa los cabos montecristo estates mazatl n emerald bay resort &amp; spa emerald estates luxury villas departure country argentina australia austria bahamas belgium brazil canada chile colombia costa rica denmark ecuador finland france germany greece honduras iceland israel italy japan luxembourg mexico netherlands new zealand nicaragua norway panama paraguay peru portugal puerto rico republic of ireland republic of korea south africa spain sweden switzerland turks and caicos islands united kingdom united states uruguay venezuela departure city akron canton ohio reg . albany ny al\",\n\"label\": \"not duplicate\"\n}\n],\n\"few_shot_selection\": \"fixed\",\n\"few_shot_num\": 3\n}\n</code></pre>"},{"location":"guide/resources/configs/#full-example-configs","title":"Full Example Configs","text":"Example 1: Company Entity Match<pre><code>{\n\"task_name\": \"CompanyEntityMatch\",\n\"task_type\": \"entity_matching\",\n\"dataset\": {\n\"label_column\": \"label\",\n\"delimiter\": \",\"\n},\n\"model\": {\n\"provider\": \"openai\",\n\"name\": \"gpt-3.5-turbo\"\n},\n\"prompt\": {\n\"task_guidelines\": \"You are provided with descriptions of companies from their websites, and wikipedia pages. Your job is to categorize whether the descriptions are about the same company (duplicate) or different companies (not duplicate). Your answer must be from one of the following options:\\n{labels}\",\n\"labels\": [\n\"not duplicate\",\n\"duplicate\"\n],\n\"example_template\": \"Company 1 description: {entity1}\\nCompany 2 description: {entity2}\\nDuplicate or not: {label}\",\n\"few_shot_examples\": [\n{\n\"entity1\": \"lac wisconsin branding 95 1 &amp; 96 1 the rock frequency 96.1 mhz translator s 95.1 w236ag fond du lac first air date 1965 as wcwc fm at 95.9 format mainstream rock erp 4 000 watts haat 123 meters 404 ft class a facility id 54510 transmitter coordinates 43 49 10.00 n 88 43 20.00 w 43.8194444 n 88.7222222 w 43.8194444 ; 88.7222222 coordinates 43 49 10.00 n 88 43 20.00 w 43.8194444 n 88.7222222 w 43.8194444 ; 88.7222222 former callsigns wcwc fm 1965 1980 wyur 1980 1994 former frequencies 95.9 mhz 1965 affiliations cbs radio network westwood one premiere radio networks owner radio plus inc. sister stations wfdl wfdl fm wmdc webcast listen live website 961tcx . com studios in fond du lac wtcx 96.1 fm 95 1 &amp; 96 1 the rock is a radio station broadcasting a mainstream rock music format . 1 licensed to ripon wisconsin usa the station is currently owned by radio plus inc. and features programing from cbs radio network dial global and premiere radio networks . 2 wtcx was originally on 95.9 mhz . be\",\n\"entity2\": \"closings contact next racing rocks local news breaking wiaa releases football playoffs matchups and brackets october 15 2016 local news here are the full brackets for the state of wisconsin division 1 2 seed fond du lac hosts 7 seed milwaukee washington friday october 21 at 7pm division 5 3 seed wla hosts 6 seed ... read more 10 15 16 fdl man injured in hit and run car vs. bike crash october 15 2016 local news a fond du lac man received non life threatening injuries in a car versus bicycle hit and run crash in dodge county . the dodge county sheriff s office says shortly after 8pm friday a car ... read more 10 15 16 ripon woman remains in critical condition following one vehicle crash october 15 2016 local news a ripon woman injured in a one vehicle crash after apparently falling asleep at the wheel remains in critical condition . the fond du lac county sheriff s office says 29 year old raquel amador ... read more wiaa releases football groupings october 15 2016 local news 2016 wiaa fo\",\n\"label\": \"duplicate\"\n},\n{\n\"entity1\": \"stacy spikes hamet watt headquarters new york city united states website http www.moviepass.com moviepass is a subscription based service for going to movie theaters available in the united states . the service gives members across the country the ability to see up to one 2d movie every 24 hours for a fixed monthly fee . members may choose which theaters they wish to attend and there are no blackout dates . moviepass works in nearly all movie theaters that accept the mastercard credit card making it one of the largest subscription based theater networks in america . prices vary by local market and start at 30 per month . moviepass was launched in february 2011 and is headquartered in new york city . 1 contents 1 service 2 purchasing a ticket 3 history 4 media coverage 5 references service edit the moviepass service works via a smartphone app iphone android and a specially designed reloadable debit card which is mailed to new subscribers when they sign up . purchasing a ticket edit in o\",\n\"entity2\": \"repair buy warranty get service buy warranty home warranty pricing &amp; plans planning on moving home matters blog what s covered service professionals customer reviews benefits faqs appliance discount contract policies decor cost savers lawn &amp; garden lifestyle quick tips real estate repair &amp; maintenance tech close home warranty learn more what s covered service professionals faqs pricing and plans get a quote see plans planning on moving real estate plans buying a home selling a home home matters blog decor cost savers lawn &amp; garden lifestyle quick tips real estate repair &amp; maintenance tech our partner sites real estate professionals contractors 888 429 8247 email us log in back to top get a personalized quote explore plans in your area get covered in 3 easy steps . please correct highlighted fields request service log in create account oven on the fritz appliance breakdowns happen . get covered . get a personalized quote explore plans in your area get covered in 3 easy steps . please co\",\n\"label\": \"not duplicate\"\n},\n{\n\"entity1\": \"of over 110 gyms worldwide including 86 franchise locations in ma pa ny nj ct wa or ca tx fl ky va puerto rico and australia and is rapidly expanding across the u.s. and around the globe . contents 1 history 2 description 3 references 4 external links history edit crunch was founded in a basement level aerobics studio in new york city s east village in 1989 by doug levine . 1 with the collaboration of fitness instructors the group fitness programming was started at crunch . offerings such as hip hop aerobics co ed action wrestling and cyked yoga cycling were introduced . 2 in clubs members have access to innovative group fitness classes state of the art equipment personal and group training full service locker rooms and much more . select locations offer an exclusive crunch retail line that can also be purchased from the crunch online store . 3 in january 2014 crunch released its online workout extension called crunch live . this subscription based online video library has over 95 work\",\n\"entity2\": \"gallery esp en best rate guarantee check availability call us room only 1 800 990 8250 hotel air 1 800 219 2727 canada 1 855 478 2811 airport transportation travel agents close best rate guaranteebook your all inclusive stay hotel hotel air arrive departure adults 1 2 3 4 5 6 7 8 children 0 1 2 3 4 5 6 7 8 select property pacifica golf &amp; spa resort the towers at pacifica sunset beach golf &amp; spa resort ros resort &amp; spa los cabos montecristo estates mazatl n emerald bay resort &amp; spa emerald estates luxury villas departure country argentina australia austria bahamas belgium brazil canada chile colombia costa rica denmark ecuador finland france germany greece honduras iceland israel italy japan luxembourg mexico netherlands new zealand nicaragua norway panama paraguay peru portugal puerto rico republic of ireland republic of korea south africa spain sweden switzerland turks and caicos islands united kingdom united states uruguay venezuela departure city akron canton ohio reg . albany ny al\",\n\"label\": \"not duplicate\"\n}\n],\n\"few_shot_selection\": \"fixed\",\n\"few_shot_num\": 3\n}\n}\n</code></pre> Example 2: Banking Complaints Classification<pre><code>{\n\"task_name\": \"BankingComplaintsClassification\",\n\"task_type\": \"classification\",\n\"dataset\": {\n\"label_column\": \"label\",\n\"delimiter\": \",\"\n},\n\"model\": {\n\"provider\": \"openai\",\n\"name\": \"gpt-3.5-turbo\"\n},\n\"prompt\": {\n\"task_guidelines\": \"You are an expert at understanding bank customers support complaints and queries.\\nYour job is to correctly classify the provided input example into one of the following categories.\\nCategories:\\n{labels}\",\n\"output_guidelines\": \"You will answer with just the the correct output label and nothing else.\",\n\"labels\": [\n\"activate_my_card\",\n\"age_limit\",\n\"apple_pay_or_google_pay\",\n\"atm_support\",\n\"automatic_top_up\",\n\"balance_not_updated_after_bank_transfer\",\n\"balance_not_updated_after_cheque_or_cash_deposit\",\n\"beneficiary_not_allowed\",\n\"cancel_transfer\",\n\"card_about_to_expire\",\n\"card_acceptance\",\n\"card_arrival\",\n\"card_delivery_estimate\",\n\"card_linking\",\n\"card_not_working\",\n\"card_payment_fee_charged\",\n\"card_payment_not_recognised\",\n\"card_payment_wrong_exchange_rate\",\n\"card_swallowed\",\n\"cash_withdrawal_charge\",\n\"cash_withdrawal_not_recognised\",\n\"change_pin\",\n\"compromised_card\",\n\"contactless_not_working\",\n\"country_support\",\n\"declined_card_payment\",\n\"declined_cash_withdrawal\",\n\"declined_transfer\",\n\"direct_debit_payment_not_recognised\",\n\"disposable_card_limits\",\n\"edit_personal_details\",\n\"exchange_charge\",\n\"exchange_rate\",\n\"exchange_via_app\",\n\"extra_charge_on_statement\",\n\"failed_transfer\",\n\"fiat_currency_support\",\n\"get_disposable_virtual_card\",\n\"get_physical_card\",\n\"getting_spare_card\",\n\"getting_virtual_card\",\n\"lost_or_stolen_card\",\n\"lost_or_stolen_phone\",\n\"order_physical_card\",\n\"passcode_forgotten\",\n\"pending_card_payment\",\n\"pending_cash_withdrawal\",\n\"pending_top_up\",\n\"pending_transfer\",\n\"pin_blocked\",\n\"receiving_money\",\n\"Refund_not_showing_up\",\n\"request_refund\",\n\"reverted_card_payment?\",\n\"supported_cards_and_currencies\",\n\"terminate_account\",\n\"top_up_by_bank_transfer_charge\",\n\"top_up_by_card_charge\",\n\"top_up_by_cash_or_cheque\",\n\"top_up_failed\",\n\"top_up_limits\",\n\"top_up_reverted\",\n\"topping_up_by_card\",\n\"transaction_charged_twice\",\n\"transfer_fee_charged\",\n\"transfer_into_account\",\n\"transfer_not_received_by_recipient\",\n\"transfer_timing\",\n\"unable_to_verify_identity\",\n\"verify_my_identity\",\n\"verify_source_of_funds\",\n\"verify_top_up\",\n\"virtual_card_not_working\",\n\"visa_or_mastercard\",\n\"why_verify_identity\",\n\"wrong_amount_of_cash_received\",\n\"wrong_exchange_rate_for_cash_withdrawal\"\n],\n\"few_shot_examples\": \"seed.csv\",\n\"few_shot_selection\": \"semantic_similarity\",\n\"few_shot_num\": 10,\n\"example_template\": \"Input: {example}\\nOutput: {label}\"\n}\n}\n</code></pre>"},{"location":"guide/resources/refuel_datasets/","title":"Refuel-provided Datasets","text":"<p>Autolabel provides datasets out-of-the-box so you can easily get started with LLM-powered labeling. The full list of datasets is below:</p> Dataset Task Type banking Classification civil_comments Classification ledgar Classification walmart_amazon Entity Matching company Entity Matching squad_v2 Question Answering sciq Question Answering conll2003 Named Entity Matching"},{"location":"guide/resources/refuel_datasets/#downloading-any-dataset","title":"Downloading any dataset","text":"<p>To download a specific dataset, such as <code>squad_v2</code>, run: <pre><code>from autolabel import get_data\nget_data('civil_comments')\n&gt; Downloading seed example dataset to \"seed.csv\"...\n&gt; 100% [..............................................................................] 65757 / 65757\n&gt; Downloading test dataset to \"test.csv\"...\n&gt; 100% [............................................................................] 610663 / 610663\n</code></pre></p>"},{"location":"guide/tasks/classification_task/","title":"Classification Task","text":""},{"location":"guide/tasks/classification_task/#introduction","title":"Introduction","text":"<p>Text classification is a fundamental task in natural language processing (NLP) that involves categorizing textual data into predefined classes or categories. It is employed in various applications such as sentiment analysis, spam detection, topic classification, intent recognition, and document categorization and can be used in any setting where there are well defined categories which the LLM can understand and put an input into.</p>"},{"location":"guide/tasks/classification_task/#example","title":"Example","text":""},{"location":"guide/tasks/classification_task/#dataset","title":"Dataset","text":"<p>Lets walk through using Autolabel for text classification on the Banking77 dataset. The Banking77 dataset comprises of 13,083 customer service queries labeled with 77 intents. It focuses on fine-grained single-domain intent detection. Every datapoint consists of an example and its corresponding label as shown below. The label belongs to a set of 77 predefined intents that the customer had for the particular datapoint for eg. activate_my_card, card_delivery_estimate, get_physical_card.</p> <pre><code>{\n\"example\": \"What can I do if my card still hasn't arrived after 2 weeks?\",\n\"label\": \"card_arrival\"\n}\n</code></pre> <p>Thus the dataset consists of just two columns, example and label. Here, Autolabel would be given the example input for a new datapoint and told to predict the label column which in this case is label.</p>"},{"location":"guide/tasks/classification_task/#config","title":"Config","text":"<p>In order to run Autolabel, we need a config defining the 3 important things - task, llm and dataset. Let's assume gpt-3.5-turbo as the LLM for this section.</p> <p><pre><code>config = {\n\"task_name\": \"BankingClassification\",\n\"task_type\": \"classification\",\n\"dataset\": {\n\"label_column\": \"label\",\n\"delimiter\": \",\"\n},\n\"model\": {\n\"provider\": \"openai\",\n\"name\": \"gpt-3.5-turbo\"\n},\n\"prompt\": {\n\"task_guidelines\": \"\"\"You are an expert at understanding banking transaction complaints.\\nYour job is to correctly label the provided input example into one of the following {num_labels} categories:\\n{labels}\"\"\",\n\"output_guidelines\": \"You will just return one line consisting of the label for the given example.\",\n\"labels\": [\n\"activate_my_card\",\n\"age_limit\",\n\"apple_pay_or_google_pay\",\n...\n],\n\"example_template\": \"Example: {example}\\nOutput: {label}\"\n}\n}\n</code></pre> The <code>task_type</code> sets up the config for a specific task, classification in this case.</p> <p>Take a look at the prompt section of the config. This defines the settings related to defining the task and the machinery around it.  </p> <p>The <code>task_guidelines</code> key is the most important key, it defines the task for the LLM to understand and execute on. In this case, we first set up the task and tell the model the kind of data present in the dataset, by telling it that it is an expert at understanding banking transaction complaints. Next, we define the task more concretely using the num_labels and labels appropriately. <code>{num_labels}</code> will be internally translated by the library to be the number of elements in the <code>labels</code> list (defined below).  <code>{labels}</code> will be translated to be all the labels in the <code>labels</code> list separated by a newline. These are essential for setting up classification tasks by telling it the labels that it is constrained to, along with any meaning associated with a label.  </p> <p>The <code>labels</code> key defines the list of possible labels for the banking77 dataset which is a list of 77 possible labels.  </p> <p>The <code>example_template</code> is one of the most important keys to set for a task. This defines the format of every example that will be sent to the LLM. This creates a prompt using the columns from the input dataset, and sends this prompt to the LLM hoping for the llm to generate the column defined under the <code>label_column</code>, which is label in our case. For every input, the model will be given the example with all the columns from the datapoint filled in according to the specification in the <code>example_template</code>. The <code>label_column</code> will be empty, and the LLM will generate the label. The <code>example_template</code> will be used to format all seed examples.  </p>"},{"location":"guide/tasks/classification_task/#few-shot-config","title":"Few Shot Config","text":"<p>Let's assume we have access to a dataset of labeled seed examples. Here is a config which details how to use it.</p> <pre><code>config = {\n\"task_name\": \"BankingClassification\",\n\"task_type\": \"classification\",\n\"dataset\": {\n\"label_column\": \"label\",\n\"delimiter\": \",\"\n},\n\"model\": {\n\"provider\": \"openai\",\n\"name\": \"gpt-3.5-turbo\"\n},\n\"prompt\": {\n\"task_guidelines\": \"\"\"You are an expert at understanding banking transaction complaints.\\nYour job is to correctly label the provided input example into one of the following {num_labels} categories:\\n{labels}\"\"\",\n\"output_guidelines\": \"You will just return one line consisting of the label for the given example.\",\n\"labels\": [\n\"activate_my_card\",\n\"age_limit\",\n\"apple_pay_or_google_pay\",\n...\n],\n\"few_shot_examples\": \"../examples/banking/seed.csv\",\n\"few_shot_selection\": \"semantic_similarity\",\n\"few_shot_num\": 5,\n\"example_template\": \"Example: {example}\\nOutput: {label}\"\n}\n}\n</code></pre> <p>The <code>few_shot_examples</code> key defines the seed set of labeled examples that are present for the model to learn from. A subset of these examples will be picked while querying the LLM in order to help it understand the task better, and understand corner cases.  </p> <p>For the banking dataset, we found <code>semantic_similarity</code> search to work really well. This looks for examples similar to a query example from the seed set and sends those to the LLM when querying for a particular input. This is defined in the <code>few_shot_selection</code> key.  </p> <p><code>few_shot_num</code> defines the number of examples selected from the seed set and sent to the LLM. Experiment with this number based on the input token budget and performance degradation with longer inputs.</p>"},{"location":"guide/tasks/classification_task/#run-the-task","title":"Run the task","text":"<pre><code>from autolabel import LabelingAgent\nagent = LabelingAgent(config)\nagent.plan('data/banking77.csv')\nagent.run('data/banking77.csv', max_items = 100)\n</code></pre>"},{"location":"guide/tasks/classification_task/#evaluation-metrics","title":"Evaluation metrics","text":"<p>On running the above config, this is an example output expected for labeling 100 items. <pre><code>Cost in $=0.00, support=50, threshold=-inf, accuracy=0.6600, completion_rate=1.0000\nActual Cost: 0.0058579999999999995\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 support \u2503 threshold \u2503 accuracy \u2503 completion_rate \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 100     \u2502 -inf      \u2502 0.76     \u2502 1.0             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre></p> <p>Accuracy - We use accuracy as the main metric for evaluating classification tasks. This is done by checking the fraction of examples which are given the correct label in the training dataset.</p> <p>Completion Rate - There can be errors while running the LLM related to labeling for eg. the LLM may give a label which is not in the label list or provide an answer which is not parsable by the library. In this cases, we mark the example as not labeled successfully. The completion rate refers to the proportion of examples that were labeled successfully.</p>"},{"location":"guide/tasks/classification_task/#notebook","title":"Notebook","text":"<p>You can find a Jupyter notebook with code that you can run on your own here</p>"},{"location":"guide/tasks/entity_matching_task/","title":"Entity Matching Task","text":""},{"location":"guide/tasks/entity_matching_task/#introduction","title":"Introduction","text":"<p>Entity matching in natural language processing (NLP) is a task that involves identifying and matching entities from different sources or datasets based on various fields or attributes. The goal is to determine if two entities refer to the same real-world object or entity, even if they are described differently or come from different data sources.</p>"},{"location":"guide/tasks/entity_matching_task/#example","title":"Example","text":""},{"location":"guide/tasks/entity_matching_task/#dataset","title":"Dataset","text":"<p>Lets walk through using Autolabel for entity matching on the Walmart-Amazon dataset. This dataset consists of duplicate products listed on both Walmart and Amazon. These products would have different names and descriptions but would be the same product. The dataset consists of such examples, where given the name and the description, the task is to predict if the products are duplicate or not. An example from the Walmart-Amazon dataset,</p> <pre><code>{\n\"entity1\": \"Title: zotac geforce gt430 1gb ddr3 pci-express 2.0 graphics card; Category: electronics - general; Brand: zotac; ModelNo: zt-40604-10l; Price: 88.88;\",\n\"entity2\": \"Title: evga geforce gts450 superclocked 1 gb gddr5 pci-express 2.0 graphics card 01g-p3-1452-tr; Category: graphics cards; Brand: evga; ModelNo: 01g-p3-1452-tr; Price: 119.88;\",\n\"label\": \"not duplicate\"\n}\n</code></pre> <p>The the dataset consists of two columns <code>entity1</code> and <code>entity2</code> which define the two entities. There could also be multiple columns defining an entity. The <code>label</code> column here defines if the two entities are duplicates or not.</p>"},{"location":"guide/tasks/entity_matching_task/#config","title":"Config","text":"<p>In order to run Autolabel, we need a config defining the 3 important things - task, llm and dataset. Let's assume gpt-3.5-turbo as the LLM for this section.</p> <p><pre><code>config = {\n\"task_name\": \"ProductCatalogEntityMatch\",\n\"task_type\": \"entity_matching\",\n\"dataset\": {\n\"label_column\": \"label\",\n\"delimiter\": \",\"\n},\n\"model\": {\n\"provider\": \"openai\",\n\"name\": \"gpt-3.5-turbo\",\n\"params\": {}\n},\n\"prompt\": {\n\"task_guidelines\": \"You are an expert at identifying duplicate products from online product catalogs.\\nYou will be given information about two product entities, and your job is to tell if they are the same (duplicate) or different (not duplicate). Your answer must be from one of the following options:\\n{labels}\",\n\"labels\": [\n\"duplicate\",\n\"not duplicate\"\n],\n\"few_shot_examples\": [\n{\n\"entity1\": \"Title: lexmark extra high yield return pgm print cartridge - magenta; Category: printers; Brand: lexmark; ModelNo: c782u1mg; Price: 214.88;\",\n\"entity2\": \"Title: lexmark 18c1428 return program print cartridge black; Category: inkjet printer ink; Brand: lexmark; ModelNo: 18c1428; Price: 19.97;\",\n\"label\": \"not duplicate\"\n},\n{\n\"entity1\": \"Title: edge tech proshot 4gb sdhc class 6 memory card; Category: usb drives; Brand: edge tech; ModelNo: pe209780; Price: 10.88;\",\n\"entity2\": \"Title: 4gb edge proshot sdhc memory card class6; Category: computers accessories; Brand: edge; ModelNo: nan; Price: 17.83;\",\n\"label\": \"duplicate\"\n},\n{\n\"entity1\": \"Title: tomtom one carry case; Category: gps; Brand: tomtom; ModelNo: 9n00 .181; Price: 19.96;\",\n\"entity2\": \"Title: tomtom one carrying case; Category: cases; Brand: tomtom; ModelNo: 9n00 .181; Price: 4.99;\",\n\"label\": \"duplicate\"\n},\n{\n\"entity1\": \"Title: iosafe rugged 250gb usb 3.0 portable external hard drive; Category: hard drives; Brand: iosafe; ModelNo: pa50250u5yr; Price: 249.99;\",\n\"entity2\": \"Title: lacie rugged all-terrain 500 gb firewire 800 firewire 400 usb 2.0 portable external hard drive 301371; Category: external hard drives; Brand: lacie; ModelNo: 301371; Price: nan;\",\n\"label\": \"not duplicate\"\n}\n],\n\"few_shot_selection\": \"fixed\",\n\"few_shot_num\": 3,\n\"example_template\": \"Entity1: {entity1}\\nEntity2: {entity2}\\nOutput: {label}\"\n}\n}\n</code></pre> The <code>task_type</code> sets up the config for a specific task, entity_matching in this case.</p> <p>Take a look at the prompt section of the config. This defines the settings related to defining the task and the machinery around it.  </p> <p>The <code>task_guidelines</code> key is the most important key, it defines the task for the LLM to understand and execute on. In this case, we first set up the task and tell the model the kind of data present in the dataset, by telling it that it is an expert at identifying duplicate products. Next we explain the task to the model, saying that it has two identify if the given products are duplicate or not. We also make the output format clear by telling the model it has to choose from the options duplicate or not duplicate. </p> <p>The <code>example_template</code> is one of the most important keys to set for a task. This defines the format of every example that will be sent to the LLM. This creates a prompt using the columns from the input dataset, and sends this prompt to the LLM hoping for the llm to generate the column defined under the <code>label_column</code>, which is answer in our case. For every input, the model will be given the example with all the columns from the datapoint filled in according to the specification in the <code>example_template</code>. The <code>label_column</code> will be empty, and the LLM will generate the label. The <code>example_template</code> will be used to format all seed examples. Here we give the model both the entities separated by newlines and ask if the entities are duplicate or not duplicate.</p> <p>The <code>few_shot_examples</code> here is a list of json inputs which define handpicked examples to use as seed examples for the model. These labeled examples help the model understand the task better and how it supposed to answer a question. If there is a larger number of examples, we can specify a path to a csv instead of a list of examples.</p> <p><code>few_shot_num</code> defines the number of examples selected from the seed set and sent to the LLM. Experiment with this number based on the input token budget and performance degradation with longer inputs.</p> <p><code>few_shot_selection</code> is set to fixed in this case as we want to use all examples as seed examples. However, if we want to use a subset of examples as seed examples from a larger set, we can set the appropriate strategy like <code>semantic_similarity</code> here to get dynamic good seed examples.</p>"},{"location":"guide/tasks/entity_matching_task/#alternate-config-with-multiple-columns","title":"Alternate config with multiple columns","text":"<p>Let's consider the case in which there are multiple columns in the dataset which are combined to create an input for the model.</p> <pre><code>config = {\n\"task_name\": \"ProductCatalogEntityMatch\",\n\"task_type\": \"entity_matching\",\n\"dataset\": {\n\"label_column\": \"label\",\n\"delimiter\": \",\"\n},\n\"model\": {\n\"provider\": \"openai\",\n\"name\": \"gpt-3.5-turbo\"\n},\n\"prompt\": {\n\"task_guidelines\": \"You are an expert at identifying duplicate products from online product catalogs.\\nYou will be given information about two product entities, and your job is to tell if they are the same (duplicate) or different (not duplicate). Your answer must be from one of the following options:\\n{labels}\",\n\"labels\": [\n\"duplicate\",\n\"not duplicate\"\n],\n\"example_template\": \"Title of entity1: {Title_entity1}; category of entity1: {Category_entity1}; brand of entity1: {Brand_entity1}; model number of entity1: {ModelNo_entity1}; price of entity1: {Price_entity1}\\nTitle of entity2: {Title_entity2}; category of entity2: {Category_entity2}; brand of entity2: {Brand_entity2}; model number of entity2: {ModelNo_entity2}; price of entity2: {Price_entity2}\\nDuplicate or not: {label}\",\n\"few_shot_examples\": [\n{\n\"Title_entity1\": \"lexmark extra high yield return pgm print cartridge - magenta\",\n\"Category_entity1\": \"printers\",\n\"Brand_entity1\": \"lexmark\",\n\"ModelNo_entity1\": \"c782u1mg\",\n\"Price_entity1\": \"214.88\",\n\"Title_entity2\": \"lexmark 18c1428 return program print cartridge black\",\n\"Category_entity2\": \"inkjet printer ink\",\n\"Brand_entity2\": \"lexmark\",\n\"ModelNo_entity2\": \"18c1428\",\n\"Price_entity2\": \"19.97\",\n\"label\": \"not duplicate\"\n},\n{\n\"Title_entity1\": \"edge tech proshot 4gb sdhc class 6 memory card\",\n\"Category_entity1\": \"usb drives\",\n\"Brand_entity1\": \"edge tech\",\n\"ModelNo_entity1\": \"pe209780\",\n\"Price_entity1\": \"10.88\",\n\"Title_entity2\": \"4gb edge proshot sdhc memory card class6\",\n\"Category_entity2\": \"computers accessories\",\n\"Brand_entity2\": \"edge\",\n\"ModelNo_entity2\": \"nan\",\n\"Price_entity2\": \"17.83\",\n\"label\": \"duplicate\"\n}\n],\n\"few_shot_selection\": \"fixed\",\n\"few_shot_num\": 2\n}\n}\n</code></pre> <p>Notice how in this case, we specify how the different columns defining different aspects of every column are stitched together to form the final example template.</p>"},{"location":"guide/tasks/entity_matching_task/#run-the-task","title":"Run the task","text":"<pre><code>from autolabel import LabelingAgent\nagent = LabelingAgent(config)\nagent.plan('data/walmart_amazon_test.csv')\nagent.run('data/walmart_amazon_test.csv', max_items = 100)\n</code></pre>"},{"location":"guide/tasks/entity_matching_task/#evaluation-metrics","title":"Evaluation metrics","text":"<p>On running the above config, this is an example output expected for labeling 100 items. <pre><code>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 support \u2503 threshold \u2503 accuracy \u2503 completion_rate \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 100     \u2502 -inf      \u2502 0.96     \u2502 1.0             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre></p> <p>Accuracy - This measures the proportion of examples which are marked correctly by the model - for eg which mark duplicate entities correctly.</p> <p>Completion Rate - There can be errors while running the LLM related to labeling for eg. the LLM may give a label which is not in the label list or provide an answer which is not parsable by the library. In this cases, we mark the example as not labeled successfully. The completion rate refers to the proportion of examples that were labeled successfully.</p>"},{"location":"guide/tasks/named_entity_recognition_task/","title":"Named Entity Recognition Task","text":""},{"location":"guide/tasks/named_entity_recognition_task/#introduction","title":"Introduction","text":"<p>Named Entity Recognition (NER) is a crucial task in natural language processing (NLP) that involves identifying and classifying named entities in text. Named entities refer to specific individuals, organizations, locations, dates, quantities, and other named entities present in the text. The goal of NER is to extract and classify these entities accurately, providing valuable information for various NLP applications such as information extraction, question answering, and sentiment analysis.</p>"},{"location":"guide/tasks/named_entity_recognition_task/#example","title":"Example","text":""},{"location":"guide/tasks/named_entity_recognition_task/#dataset","title":"Dataset","text":"<p>Lets walk through using Autolabel for named entity recognition on the CONLL2003 dataset. The CONLL2003 dataset comprises of sentences with entities in the sentence labeled LOC (location), ORG (organization), PER (person) or MISC (Miscellaneous).  </p> <pre><code>{\n\"example\": \"The role of the 70,000 mainly Kurdish village guards who fight Kurdistan Workers Party ( PKK ) guerrillas in the southeast has been questioned recently after media allegations that many of them are involved in common crime .\",\n\"CategorizedLabels\": \"{'Location': [], 'Organization': ['Kurdistan Workers Party', 'PKK'], 'Person': [], 'Miscellaneous': ['Kurdish']}\"\n}\n</code></pre> <p>Thus the dataset consists of the <code>example</code> and <code>CategorizedLabels</code> columns. Here <code>example</code> mentions the sentence which needs to be labeled. The <code>CategorizedLabels</code> contains the entities for every label as a list.</p>"},{"location":"guide/tasks/named_entity_recognition_task/#config","title":"Config","text":"<p>In order to run Autolabel, we need a config defining the 3 important things - task, llm and dataset. Let's assume gpt-3.5-turbo as the LLM for this section.</p> <p><pre><code>config = {\n\"task_name\": \"PersonLocationOrgMiscNER\",\n\"task_type\": \"named_entity_recognition\",\n\"dataset\": {\n\"label_column\": \"CategorizedLabels\",\n\"text_column\": \"example\"\n},\n\"model\": {\n\"provider\": \"anthropic\",\n\"name\": \"claude-v1\"\n},\n\"prompt\": {\n\"task_guidelines\": \"You are an expert at extracting Person, Organization, Location, and Miscellaneous entities from text. Your job is to extract named entities mentioned in text, and classify them into one of the following categories.\\nCategories:\\n{labels}\\n \",\n\"labels\": [\n\"Location\",\n\"Organization\",\n\"Person\",\n\"Miscellaneous\"\n],\n\"example_template\": \"Example: {example}\\nOutput: {CategorizedLabels}\",\n\"few_shot_examples\": \"data/conll2003_seed.csv\",\n\"few_shot_selection\": \"semantic_similarity\",\n\"few_shot_num\": 5\n}\n}\n</code></pre> The <code>task_type</code> sets up the config for a specific task, named_entity_recognition in this case.</p> <p>Take a look at the prompt section of the config. This defines the settings related to defining the task and the machinery around it.  </p> <p>The <code>task_guidelines</code> key is the most important key, it defines the task for the LLM to understand and execute on. In this case, we first set up the task and tell the model the kind of data present in the dataset, by telling it that it is an expert at extracting entities from text and classifying them into the necessary labels. Next, we tell the model the list of categories that it should classify every entity into. This ensures that every entity is assigned to one category.  </p> <p>The <code>example_template</code> is one of the most important keys to set for a task. This defines the format of every example that will be sent to the LLM. This creates a prompt using the columns from the input dataset, and sends this prompt to the LLM hoping for the llm to generate the column defined under the <code>label_column</code>, which is <code>CategorizedLabels</code> in our case. For every input, the model will be given the example with all the columns from the datapoint filled in according to the specification in the <code>example_template</code>. The <code>label_column</code> will be empty, and the LLM will generate the label. The <code>example_template</code> will be used to format all seed examples.  </p> <p>The <code>few_shot_examples</code> here is a path to a csv which defines a set of labeled examples which the model can use to understand the task better. These examples will be used as a reference by the model.</p> <p><code>few_shot_num</code> defines the number of examples selected from the seed set and sent to the LLM. Experiment with this number based on the input token budget and performance degradation with longer inputs.</p> <p><code>few_shot_selection</code> is set to <code>semantic_similarity</code> in this case as we want to use a subset of examples as seed examples from a larger set to get dynamically good seed examples.</p>"},{"location":"guide/tasks/named_entity_recognition_task/#run-the-task","title":"Run the task","text":"<pre><code>from autolabel import LabelingAgent\nagent = LabelingAgent(config)\nagent.plan('examples/squad_v2/test.csv', max_items = 100)\nagent.run('examples/squad_v2/test.csv', max_items = 100)\n</code></pre>"},{"location":"guide/tasks/named_entity_recognition_task/#evaluation-metrics","title":"Evaluation metrics","text":"<p>On running the above config, this is an example output expected for labeling 100 items. <pre><code>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 f1     \u2503 support \u2503 threshold \u2503 accuracy \u2503 completion_rate \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 0.7834 \u2502 100     \u2502 -inf      \u2502 0.7834   \u2502 1.0             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre></p> <p>Accuracy - This is calculated by taking the exact match of the predicted tokens and their correct class. This may suffer from class imbalance.</p> <p>F1 - This is calculated using the precision and recall of the predicted tokens and their classes. We use a macro average to get to one F1 score for all classes.</p> <p>Completion Rate - There can be errors while running the LLM related to labeling for eg. the LLM may provide an answer which is not parsable by the library. In this cases, we mark the example as not labeled successfully. The completion rate refers to the proportion of examples that were labeled successfully.</p>"},{"location":"guide/tasks/question_answering_task/","title":"Question Answering Task","text":""},{"location":"guide/tasks/question_answering_task/#introduction","title":"Introduction","text":"<p>Question answering is the most fundamental task that can be solved using LLMs. Most tasks can be reduced to some form of question answering where the model is optionally given some context and then asked to answer a question. There can be a broad classification of question answering tasks into 2 categories -  </p> <ol> <li> <p>Open Book QA - In this variant, the model is given a context along with a question and then asked to answer using the context. Here, we do not rely on knowledge present in the model parameters and instead rely on the reasoning abilities and commonsense properties of the model to answer correctly.</p> </li> <li> <p>Closed Book QA - In this variant, the model is just given a question, without any context or knowledge source and asked to answer based on pretrained knowledge. This requires more knowledge to be present in the model parameters and thus favours bigger LLMs.</p> </li> </ol> <p>In addition to context, question answering tasks can also differ in the way that the answers are generated. The easiest form is one where there is a predefined set of options (for eg. yes or no) and the model needs to choose from one of these options. Another variant allows separate options for each question similar to SAT questions. The last variant is one where the model is free to generate its own answers. This variant is harder to evaluate because multiple answers could mean the same thing.</p>"},{"location":"guide/tasks/question_answering_task/#example","title":"Example","text":""},{"location":"guide/tasks/question_answering_task/#dataset","title":"Dataset","text":"<p>Lets walk through using Autolabel for question answering on the Squad dataset. The Squad dataset comprises of 100k questions and answers along with a context for each question which contains the answer for the question. Additionally, the correct answer is a continuous text span from the context. However, in addition to correct answers, it also contains 50k pairs where the question is unanswerable given the context, that is, the context does not have enough information to answer the question correctly. Here is an example datapoint from the dataset,</p> <pre><code>{\n\"question\": \"When did Beyonce start becoming popular?\",\n\"context\": \"Beyonc\u00e9 Giselle Knowles-Carter (/bi\u02d0\u02c8j\u0252nse\u026a/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&amp;B girl-group Destiny's Child. Managed by her father, Mathew Knowles, the group became one of the world's best-selling girl groups of all time. Their hiatus saw the release of Beyonc\u00e9's debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles 'Crazy in Love' and 'Baby Boy'.\",\n\"answer\": \"in the late 1990s\"\n}\n</code></pre> <p>Thus the dataset consists of the <code>question</code>, <code>context</code> and <code>answer</code>. For datasets like SciQ, there may be an additional field called <code>options</code> which is a list of strings which are possible answers for a particular question.</p>"},{"location":"guide/tasks/question_answering_task/#config","title":"Config","text":"<p>In order to run Autolabel, we need a config defining the 3 important things - task, llm and dataset. Let's assume gpt-3.5-turbo as the LLM for this section.</p> <p><pre><code>config = {\n\"task_name\": \"OpenbookQAWikipedia\",\n\"task_type\": \"question_answering\",\n\"dataset\": {\n\"label_column\": \"answer\",\n\"delimiter\": \",\"\n},\n\"model\": {\n\"provider\": \"openai\",\n\"name\": \"gpt-3.5-turbo\",\n\"params\": {}\n},\n\"prompt\": {\n\"task_guidelines\": \"You are an expert at answering questions based on wikipedia articles. Your job is to answer the following questions using the context provided with the question. The answer is a continuous span of words from the context. Use the context to answer the question. If the question cannot be answered using the context, answer the question as unanswerable.\",\n\"few_shot_examples\": [\n{\n\"question\": \"What was created by the modern Conservative Party in 1859 to define basic Conservative principles?\",\n\"answer\": \"unanswerable\",\n\"context\": \"The modern Conservative Party was created out of the 'Pittite' Tories of the early 19th century. In the late 1820s disputes over political reform broke up this grouping. A government led by the Duke of Wellington collapsed amidst dire election results. Following this disaster Robert Peel set about assembling a new coalition of forces. Peel issued the Tamworth Manifesto in 1834 which set out the basic principles of Conservatism; \u2013 the necessity in specific cases of reform in order to survive, but an opposition to unnecessary change, that could lead to 'a perpetual vortex of agitation'. Meanwhile, the Whigs, along with free trade Tory followers of Robert Peel, and independent Radicals, formed the Liberal Party under Lord Palmerston in 1859, and transformed into a party of the growing urban middle-class, under the long leadership of William Ewart Gladstone.\"\n},\n{\n\"question\": \"When is King Mom symbolically burnt?\",\n\"answer\": \"On the evening before Lent\",\n\"context\": \"Carnival means weeks of events that bring colourfully decorated floats, contagiously throbbing music, luxuriously costumed groups of celebrants of all ages, King and Queen elections, electrifying jump-ups and torchlight parades, the Jouvert morning: the Children's Parades and finally the Grand Parade. Aruba's biggest celebration is a month-long affair consisting of festive 'jump-ups' (street parades), spectacular parades and creative contests. Music and flamboyant costumes play a central role, from the Queen elections to the Grand Parade. Street parades continue in various districts throughout the month, with brass band, steel drum and roadmarch tunes. On the evening before Lent, Carnival ends with the symbolic burning of King Momo.\"\n},\n{\n\"question\": \"How far does the Alps range stretch?\",\n\"answer\": \"the Mediterranean Sea north above the Po basin, extending through France from Grenoble, eastward through mid and southern Switzerland\",\n\"context\": \"The Alps are a crescent shaped geographic feature of central Europe that ranges in a 800 km (500 mi) arc from east to west and is 200 km (120 mi) in width. The mean height of the mountain peaks is 2.5 km (1.6 mi). The range stretches from the Mediterranean Sea north above the Po basin, extending through France from Grenoble, eastward through mid and southern Switzerland. The range continues toward Vienna in Austria, and east to the Adriatic Sea and into Slovenia. To the south it dips into northern Italy and to the north extends to the south border of Bavaria in Germany. In areas like Chiasso, Switzerland, and Neuschwanstein, Bavaria, the demarcation between the mountain range and the flatlands are clear; in other places such as Geneva, the demarcation is less clear. The countries with the greatest alpine territory are Switzerland, France, Austria and Italy.\"\n}\n],\n\"few_shot_selection\": \"fixed\",\n\"few_shot_num\": 3,\n\"example_template\": \"Context: {context}\\nQuestion: {question}\\nAnswer: {answer}\"\n}\n}\n</code></pre> The <code>task_type</code> sets up the config for a specific task, question_answering in this case.</p> <p>Take a look at the prompt section of the config. This defines the settings related to defining the task and the machinery around it.  </p> <p>The <code>task_guidelines</code> key is the most important key, it defines the task for the LLM to understand and execute on. In this case, we first set up the task and tell the model the kind of data present in the dataset, by telling it that it is an expert at understanding wikipedia articles. Next, we define the task more concretely by telling the model how to answer the question given the context. We tell the model that the answer is a continuous text span from the context and that in some cases, the answer can be unanswerable and how the model should handle such questions.  </p> <p>The <code>example_template</code> is one of the most important keys to set for a task. This defines the format of every example that will be sent to the LLM. This creates a prompt using the columns from the input dataset, and sends this prompt to the LLM hoping for the llm to generate the column defined under the <code>label_column</code>, which is answer in our case. For every input, the model will be given the example with all the columns from the datapoint filled in according to the specification in the <code>example_template</code>. The <code>label_column</code> will be empty, and the LLM will generate the label. The <code>example_template</code> will be used to format all seed examples. Here we also see the ordering of the context followed by question and answer, and also see the <code>Context:</code> string to inform the model which part of the text is the context.</p> <p>The <code>few_shot_examples</code> here is a list of json inputs which define handpicked examples to use as seed examples for the model. These labeled examples help the model understand the task better and how it supposed to answer a question. If there is a larger number of examples, we can specify a path to a csv instead of a list of examples.</p> <p><code>few_shot_num</code> defines the number of examples selected from the seed set and sent to the LLM. Experiment with this number based on the input token budget and performance degradation with longer inputs.</p> <p><code>few_shot_selection</code> is set to fixed in this case as we want to use all examples as seed examples. However, if we want to use a subset of examples as seed examples from a larger set, we can set the appropriate strategy like <code>semantic_similarity</code> here to get dynamic good seed examples.</p>"},{"location":"guide/tasks/question_answering_task/#alternate-config-for-closedbook-qa","title":"Alternate config for ClosedBook QA","text":"<p>Let's consider a dataset like sciq which is a closed book QA with multiple choice questions. Here we have an example config for this dataset,</p> <pre><code>config = {\n\"task_name\": \"ClosedBookQAScienceQuestions\",\n\"task_type\": \"question_answering\",\n\"dataset\": {\n\"label_column\": \"answer\",\n\"delimiter\": \",\"\n},\n\"model\": {\n\"provider\": \"openai\",\n\"name\": \"gpt-3.5-turbo\",\n\"params\": {}\n},\n\"prompt\": {\n\"task_guidelines\": \"You are an expert at answering science questions. Choose an answer from the given options. Use your knowledge of science and common sense to best answer the question.\",\n\"few_shot_examples\": \"../examples/squad_v2/seed.csv\",\n\"few_shot_selection\": \"fixed\",\n\"few_shot_num\": 3,\n\"example_template\": \"Question: {question}\\nOptions: {options}\\nAnswer: {answer}\"\n}\n}\n</code></pre> <p>Notice in this case we don't have the <code>context</code> and pass in the <code>options</code> as list of string options. These are present in the dataset and are appropriately called in the example template.</p>"},{"location":"guide/tasks/question_answering_task/#run-the-task","title":"Run the task","text":"<pre><code>from autolabel import LabelingAgent\nagent = LabelingAgent(config)\nagent.plan('data/squad_v2_test.csv')\nagent.run('data/squad_v2_test.csv', max_items = 100)\n</code></pre>"},{"location":"guide/tasks/question_answering_task/#evaluation-metrics","title":"Evaluation metrics","text":"<p>On running the above config, this is an example output expected for labeling 100 items. <pre><code>Actual Cost: 0.13500600000000001\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 f1                 \u2503 support \u2503 threshold \u2503 accuracy \u2503 completion_rate \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 0.7018720299348971 \u2502 100     \u2502 -inf      \u2502 0.59     \u2502 1.0             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n</code></pre></p> <p>Accuracy - This is the exact match performance based on the reference answer. Here we give the model 1 if the answer matches exactly with the correct answer and 0 otherwise. This is particularly harsh for the model in cases where there isnt a multi choice given to the model for eg. Squad. Even if the model gets one word wrong without changing the meaning, the model will get penalized.</p> <p>F1 - This is calculated by treating the predicted and the ground truth tokens as a list of tokens. Using this, an F1 score is calculated for every examples. This score can then be averaged over the entire dataset to get the final score. An exact match would get an F1 score of 1. This metric allows the model to make small mistakes in the predicted tokens and might be a more accurate metric for cases where the answers are not restricted to a set of options.</p> <p>Completion Rate - There can be errors while running the LLM related to labeling for eg. the LLM may give a label which is not in the label list or provide an answer which is not parsable by the library. In this cases, we mark the example as not labeled successfully. The completion rate refers to the proportion of examples that were labeled successfully.</p>"},{"location":"reference/cache/","title":"Cache","text":""},{"location":"reference/cache/#src.autolabel.cache.base.BaseCache","title":"<code>BaseCache</code>","text":"<p>         Bases: <code>ABC</code></p> <p>used to store AutoLabeling results, allowing for interrupted labeling runs to be continued from a save point without the need to restart from the beginning. Any custom Cache classes should extend from BaseCache.</p> Source code in <code>src/autolabel/cache/base.py</code> <pre><code>class BaseCache(ABC):\n\"\"\"used to store AutoLabeling results, allowing for interrupted labeling runs to be continued from a save point without the need to restart from the beginning. Any custom Cache classes should extend from BaseCache.\"\"\"\ndef __init__(self) -&gt; None:\nsuper().__init__()\n@abstractmethod\ndef lookup(self, entry: CacheEntry) -&gt; List[Generation]:\n\"\"\"abstract method to retrieve a cached entry. Must be implemented by classes derived from BaseCache.\"\"\"\npass\n@abstractmethod\ndef update(self, entry: CacheEntry) -&gt; None:\n\"\"\"abstract method to update the cache with a new entry. Must be implemented by classes derived from BaseCache.\"\"\"\npass\n</code></pre>"},{"location":"reference/cache/#src.autolabel.cache.base.BaseCache.lookup","title":"<code>lookup(entry)</code>  <code>abstractmethod</code>","text":"<p>abstract method to retrieve a cached entry. Must be implemented by classes derived from BaseCache.</p> Source code in <code>src/autolabel/cache/base.py</code> <pre><code>@abstractmethod\ndef lookup(self, entry: CacheEntry) -&gt; List[Generation]:\n\"\"\"abstract method to retrieve a cached entry. Must be implemented by classes derived from BaseCache.\"\"\"\npass\n</code></pre>"},{"location":"reference/cache/#src.autolabel.cache.base.BaseCache.update","title":"<code>update(entry)</code>  <code>abstractmethod</code>","text":"<p>abstract method to update the cache with a new entry. Must be implemented by classes derived from BaseCache.</p> Source code in <code>src/autolabel/cache/base.py</code> <pre><code>@abstractmethod\ndef update(self, entry: CacheEntry) -&gt; None:\n\"\"\"abstract method to update the cache with a new entry. Must be implemented by classes derived from BaseCache.\"\"\"\npass\n</code></pre>"},{"location":"reference/cache/#src.autolabel.cache.sqlalchemy_cache.SQLAlchemyCache","title":"<code>SQLAlchemyCache</code>","text":"<p>         Bases: <code>BaseCache</code></p> <p>A cache system implemented with SQL Alchemy</p> Source code in <code>src/autolabel/cache/sqlalchemy_cache.py</code> <pre><code>class SQLAlchemyCache(BaseCache):\n\"\"\"A cache system implemented with SQL Alchemy\"\"\"\ndef __init__(self):\nself.engine = create_db_engine()\nself.base = Base\nself.session = None\nself.initialize()\ndef initialize(self):\nself.base.metadata.create_all(self.engine)\nself.session = sessionmaker(bind=self.engine, autocommit=True)()\ndef lookup(self, entry: CacheEntry) -&gt; List[Generation]:\n\"\"\"Retrieves an entry from the Cache. Returns an empty list [] if not found.\n        Args:\n            entry: CacheEntry we wish to retrieve from the Cache\n        Returns:\n            result: A list of langchain Generation objects, containing the results of the labeling run for this CacheEntry. Empty list [] if not found.\n        \"\"\"\ncache_entry = CacheEntryModel.get(self.session, entry)\nif cache_entry is None:\nlogger.debug(\"Cache miss\")\nreturn []\nlogger.debug(\"Cache hit\")\nreturn cache_entry.generations\ndef update(self, entry: CacheEntry) -&gt; None:\n\"\"\"Inserts the provided CacheEntry into the Cache, overriding it if it already exists\n        Args:\n            entry: CacheEntry we wish to put into the Cache\n        \"\"\"\nCacheEntryModel.insert(self.session, entry)\ndef clear(self) -&gt; None:\n\"\"\"Clears the entire Cache\"\"\"\nCacheEntryModel.clear(self.session)\n</code></pre>"},{"location":"reference/cache/#src.autolabel.cache.sqlalchemy_cache.SQLAlchemyCache.clear","title":"<code>clear()</code>","text":"<p>Clears the entire Cache</p> Source code in <code>src/autolabel/cache/sqlalchemy_cache.py</code> <pre><code>def clear(self) -&gt; None:\n\"\"\"Clears the entire Cache\"\"\"\nCacheEntryModel.clear(self.session)\n</code></pre>"},{"location":"reference/cache/#src.autolabel.cache.sqlalchemy_cache.SQLAlchemyCache.lookup","title":"<code>lookup(entry)</code>","text":"<p>Retrieves an entry from the Cache. Returns an empty list [] if not found.</p> <p>Parameters:</p> Name Type Description Default <code>entry</code> <code>CacheEntry</code> <p>CacheEntry we wish to retrieve from the Cache</p> required <p>Returns:</p> Name Type Description <code>result</code> <code>List[Generation]</code> <p>A list of langchain Generation objects, containing the results of the labeling run for this CacheEntry. Empty list [] if not found.</p> Source code in <code>src/autolabel/cache/sqlalchemy_cache.py</code> <pre><code>def lookup(self, entry: CacheEntry) -&gt; List[Generation]:\n\"\"\"Retrieves an entry from the Cache. Returns an empty list [] if not found.\n    Args:\n        entry: CacheEntry we wish to retrieve from the Cache\n    Returns:\n        result: A list of langchain Generation objects, containing the results of the labeling run for this CacheEntry. Empty list [] if not found.\n    \"\"\"\ncache_entry = CacheEntryModel.get(self.session, entry)\nif cache_entry is None:\nlogger.debug(\"Cache miss\")\nreturn []\nlogger.debug(\"Cache hit\")\nreturn cache_entry.generations\n</code></pre>"},{"location":"reference/cache/#src.autolabel.cache.sqlalchemy_cache.SQLAlchemyCache.update","title":"<code>update(entry)</code>","text":"<p>Inserts the provided CacheEntry into the Cache, overriding it if it already exists</p> <p>Parameters:</p> Name Type Description Default <code>entry</code> <code>CacheEntry</code> <p>CacheEntry we wish to put into the Cache</p> required Source code in <code>src/autolabel/cache/sqlalchemy_cache.py</code> <pre><code>def update(self, entry: CacheEntry) -&gt; None:\n\"\"\"Inserts the provided CacheEntry into the Cache, overriding it if it already exists\n    Args:\n        entry: CacheEntry we wish to put into the Cache\n    \"\"\"\nCacheEntryModel.insert(self.session, entry)\n</code></pre>"},{"location":"reference/configs/","title":"Config","text":""},{"location":"reference/configs/#src.autolabel.configs.base.BaseConfig","title":"<code>BaseConfig</code>","text":"<p>Used for parsing, validating, and storing information about the labeling task passed to the LabelingAgent. Additional config classes should extend from this base class.</p> Source code in <code>src/autolabel/configs/base.py</code> <pre><code>class BaseConfig:\n\"\"\"Used for parsing, validating, and storing information about the labeling task passed to the LabelingAgent. Additional config classes should extend from this base class.\"\"\"\ndef __init__(self, config: Union[str, Dict]) -&gt; None:\nif isinstance(config, str):\nself.config = self._safe_load_json(config)\nelse:\nself.config = config\nself._validate()\ndef _safe_load_json(self, json_file_path: str) -&gt; Dict:\n\"\"\"Loads config settings from a provided json file\"\"\"\ntry:\nwith open(json_file_path, \"r\") as config_file:\nreturn json.load(config_file)\nexcept ValueError as e:\nlogger.error(\nf\"JSON file: {json_file_path} not loaded successfully. Error: {repr(e)}\"\n)\nreturn {}\ndef _validate(self) -&gt; bool:\n\"\"\"Returns true if the config settings are valid\"\"\"\nreturn True\ndef get(self, key: str, default_value: Any = None) -&gt; Any:\nreturn self.config.get(key, default_value)\ndef keys(self) -&gt; List:\nreturn list(self.config.keys())\ndef __getitem__(self, key):\nreturn self.config[key]\ndef to_json(self) -&gt; str:\n\"\"\"Returns the BaseConfig object in JSON format\"\"\"\nreturn json.dumps(self.config, sort_keys=True)\ndef __str__(self):\nreturn self.to_json()\n</code></pre>"},{"location":"reference/configs/#src.autolabel.configs.base.BaseConfig.to_json","title":"<code>to_json()</code>","text":"<p>Returns the BaseConfig object in JSON format</p> Source code in <code>src/autolabel/configs/base.py</code> <pre><code>def to_json(self) -&gt; str:\n\"\"\"Returns the BaseConfig object in JSON format\"\"\"\nreturn json.dumps(self.config, sort_keys=True)\n</code></pre>"},{"location":"reference/configs/#src.autolabel.configs.config.AutolabelConfig","title":"<code>AutolabelConfig</code>","text":"<p>         Bases: <code>BaseConfig</code></p> <p>Class to parse and store configs passed to Autolabel agent.</p> Source code in <code>src/autolabel/configs/config.py</code> <pre><code>class AutolabelConfig(BaseConfig):\n\"\"\"Class to parse and store configs passed to Autolabel agent.\"\"\"\n# Top-level config keys\nTASK_NAME_KEY = \"task_name\"\nTASK_TYPE_KEY = \"task_type\"\nDATASET_CONFIG_KEY = \"dataset\"\nMODEL_CONFIG_KEY = \"model\"\nPROMPT_CONFIG_KEY = \"prompt\"\n# Dataset config keys (config[\"dataset\"][&lt;key&gt;])\nLABEL_COLUMN_KEY = \"label_column\"\nEXPLANATION_COLUMN_KEY = \"explanation_column\"\nTEXT_COLUMN_KEY = \"text_column\"\nDELIMITER_KEY = \"delimiter\"\n# Model config keys (config[\"model\"][&lt;key&gt;])\nPROVIDER_KEY = \"provider\"\nMODEL_NAME_KEY = \"name\"\nMODEL_PARAMS_KEY = \"params\"\nCOMPUTE_CONFIDENCE_KEY = \"compute_confidence\"\n# Prompt config keys (config[\"prompt\"][&lt;key&gt;])\nTASK_GUIDELINE_KEY = \"task_guidelines\"\nVALID_LABELS_KEY = \"labels\"\nFEW_SHOT_EXAMPLE_SET_KEY = \"few_shot_examples\"\nFEW_SHOT_SELECTION_ALGORITHM_KEY = \"few_shot_selection\"\nFEW_SHOT_NUM_KEY = \"few_shot_num\"\nEXAMPLE_TEMPLATE_KEY = \"example_template\"\nOUTPUT_GUIDELINE_KEY = \"output_guidelines\"\nOUTPUT_FORMAT_KEY = \"output_format\"\nCHAIN_OF_THOUGHT_KEY = \"chain_of_thought\"\ndef __init__(self, config: Union[str, Dict]) -&gt; None:\nsuper().__init__(config)\n@cached_property\ndef _dataset_config(self) -&gt; Dict:\n\"\"\"Returns information about the dataset being used for labeling (e.g. label_column, text_column, delimiter)\"\"\"\nreturn self.config.get(self.DATASET_CONFIG_KEY, {})\n@cached_property\ndef _model_config(self) -&gt; Dict:\n\"\"\"Returns information about the model being used for labeling (e.g. provider name, model name, parameters)\"\"\"\nreturn self.config[self.MODEL_CONFIG_KEY]\n@cached_property\ndef _prompt_config(self) -&gt; Dict:\n\"\"\"Returns information about the prompt we are passing to the model (e.g. task guidelines, examples, output formatting)\"\"\"\nreturn self.config[self.PROMPT_CONFIG_KEY]\n# project and task definition config\ndef task_name(self) -&gt; str:\nreturn self.config[self.TASK_NAME_KEY]\ndef task_type(self) -&gt; str:\n\"\"\"Returns the type of task we have configured the labeler to perform (e.g. Classification, Question Answering)\"\"\"\nreturn self.config[self.TASK_TYPE_KEY]\n# Dataset config\ndef label_column(self) -&gt; str:\n\"\"\"Returns the name of the column containing labels for the dataset. Used for comparing accuracy of autolabel results vs ground truth\"\"\"\nreturn self._dataset_config.get(self.LABEL_COLUMN_KEY, None)\ndef text_column(self) -&gt; str:\n\"\"\"Returns the name of the column containing text data we intend to label\"\"\"\nreturn self._dataset_config.get(self.TEXT_COLUMN_KEY, None)\ndef explanation_column(self) -&gt; str:\n\"\"\"Returns the name of the column containing an explanation as to why the data is labeled a certain way\"\"\"\nreturn self._dataset_config.get(self.EXPLANATION_COLUMN_KEY, None)\ndef delimiter(self) -&gt; str:\n\"\"\"Returns the token used to seperate cells in the dataset. Defaults to a comma ','\"\"\"\nreturn self._dataset_config.get(self.DELIMITER_KEY, \",\")\n# Model config\ndef provider(self) -&gt; str:\n\"\"\"Returns the name of the entity that provides the currently configured model (e.g. OpenAI, Anthropic, Refuel)\"\"\"\nreturn self._model_config[self.PROVIDER_KEY]\ndef model_name(self) -&gt; str:\n\"\"\"Returns the name of the model being used for labeling (e.g. gpt-4, claude-v1)\"\"\"\nreturn self._model_config[self.MODEL_NAME_KEY]\ndef model_params(self) -&gt; Dict:\n\"\"\"Returns a dict of configured settings for the model (e.g. hyperparameters)\"\"\"\nreturn self._model_config.get(self.MODEL_PARAMS_KEY, {})\ndef confidence(self) -&gt; bool:\n\"\"\"Returns true if the model is able to return a confidence score along with its predictions\"\"\"\nreturn self._model_config.get(self.COMPUTE_CONFIDENCE_KEY, False)\n# Prompt config\ndef task_guidelines(self) -&gt; str:\nreturn self._prompt_config.get(self.TASK_GUIDELINE_KEY, \"\")\ndef labels_list(self) -&gt; List[str]:\n\"\"\"Returns a list of valid labels\"\"\"\nreturn self._prompt_config.get(self.VALID_LABELS_KEY, [])\ndef few_shot_example_set(self) -&gt; Union[str, List]:\n\"\"\"Returns examples of how data should be labeled, used to guide context to the model about the task it is performing\"\"\"\nreturn self._prompt_config.get(self.FEW_SHOT_EXAMPLE_SET_KEY, [])\ndef few_shot_algorithm(self) -&gt; str:\n\"\"\"Returns which algorithm is being used to construct the set of examples being given to the model about the labeling task\"\"\"\nreturn self._prompt_config.get(self.FEW_SHOT_SELECTION_ALGORITHM_KEY, None)\ndef few_shot_num_examples(self) -&gt; int:\n\"\"\"Returns how many examples should be given to the model in its instruction prompt\"\"\"\nreturn self._prompt_config.get(self.FEW_SHOT_NUM_KEY, 0)\ndef example_template(self) -&gt; str:\n\"\"\"Returns a string containing a template for how examples will be formatted in the prompt\"\"\"\nexample_template = self._prompt_config.get(self.EXAMPLE_TEMPLATE_KEY, None)\nif not example_template:\nraise ValueError(\"An example template needs to be specified in the config.\")\nreturn example_template\ndef output_format(self) -&gt; str:\nreturn self._prompt_config.get(self.OUTPUT_FORMAT_KEY, None)\ndef output_guidelines(self) -&gt; str:\nreturn self._prompt_config.get(self.OUTPUT_GUIDELINE_KEY, None)\ndef chain_of_thought(self) -&gt; bool:\n\"\"\"Returns true if the model is able to perform chain of thought reasoning.\"\"\"\nreturn self._prompt_config.get(self.CHAIN_OF_THOUGHT_KEY, False)\n</code></pre>"},{"location":"reference/configs/#src.autolabel.configs.config.AutolabelConfig.chain_of_thought","title":"<code>chain_of_thought()</code>","text":"<p>Returns true if the model is able to perform chain of thought reasoning.</p> Source code in <code>src/autolabel/configs/config.py</code> <pre><code>def chain_of_thought(self) -&gt; bool:\n\"\"\"Returns true if the model is able to perform chain of thought reasoning.\"\"\"\nreturn self._prompt_config.get(self.CHAIN_OF_THOUGHT_KEY, False)\n</code></pre>"},{"location":"reference/configs/#src.autolabel.configs.config.AutolabelConfig.confidence","title":"<code>confidence()</code>","text":"<p>Returns true if the model is able to return a confidence score along with its predictions</p> Source code in <code>src/autolabel/configs/config.py</code> <pre><code>def confidence(self) -&gt; bool:\n\"\"\"Returns true if the model is able to return a confidence score along with its predictions\"\"\"\nreturn self._model_config.get(self.COMPUTE_CONFIDENCE_KEY, False)\n</code></pre>"},{"location":"reference/configs/#src.autolabel.configs.config.AutolabelConfig.delimiter","title":"<code>delimiter()</code>","text":"<p>Returns the token used to seperate cells in the dataset. Defaults to a comma ','</p> Source code in <code>src/autolabel/configs/config.py</code> <pre><code>def delimiter(self) -&gt; str:\n\"\"\"Returns the token used to seperate cells in the dataset. Defaults to a comma ','\"\"\"\nreturn self._dataset_config.get(self.DELIMITER_KEY, \",\")\n</code></pre>"},{"location":"reference/configs/#src.autolabel.configs.config.AutolabelConfig.example_template","title":"<code>example_template()</code>","text":"<p>Returns a string containing a template for how examples will be formatted in the prompt</p> Source code in <code>src/autolabel/configs/config.py</code> <pre><code>def example_template(self) -&gt; str:\n\"\"\"Returns a string containing a template for how examples will be formatted in the prompt\"\"\"\nexample_template = self._prompt_config.get(self.EXAMPLE_TEMPLATE_KEY, None)\nif not example_template:\nraise ValueError(\"An example template needs to be specified in the config.\")\nreturn example_template\n</code></pre>"},{"location":"reference/configs/#src.autolabel.configs.config.AutolabelConfig.explanation_column","title":"<code>explanation_column()</code>","text":"<p>Returns the name of the column containing an explanation as to why the data is labeled a certain way</p> Source code in <code>src/autolabel/configs/config.py</code> <pre><code>def explanation_column(self) -&gt; str:\n\"\"\"Returns the name of the column containing an explanation as to why the data is labeled a certain way\"\"\"\nreturn self._dataset_config.get(self.EXPLANATION_COLUMN_KEY, None)\n</code></pre>"},{"location":"reference/configs/#src.autolabel.configs.config.AutolabelConfig.few_shot_algorithm","title":"<code>few_shot_algorithm()</code>","text":"<p>Returns which algorithm is being used to construct the set of examples being given to the model about the labeling task</p> Source code in <code>src/autolabel/configs/config.py</code> <pre><code>def few_shot_algorithm(self) -&gt; str:\n\"\"\"Returns which algorithm is being used to construct the set of examples being given to the model about the labeling task\"\"\"\nreturn self._prompt_config.get(self.FEW_SHOT_SELECTION_ALGORITHM_KEY, None)\n</code></pre>"},{"location":"reference/configs/#src.autolabel.configs.config.AutolabelConfig.few_shot_example_set","title":"<code>few_shot_example_set()</code>","text":"<p>Returns examples of how data should be labeled, used to guide context to the model about the task it is performing</p> Source code in <code>src/autolabel/configs/config.py</code> <pre><code>def few_shot_example_set(self) -&gt; Union[str, List]:\n\"\"\"Returns examples of how data should be labeled, used to guide context to the model about the task it is performing\"\"\"\nreturn self._prompt_config.get(self.FEW_SHOT_EXAMPLE_SET_KEY, [])\n</code></pre>"},{"location":"reference/configs/#src.autolabel.configs.config.AutolabelConfig.few_shot_num_examples","title":"<code>few_shot_num_examples()</code>","text":"<p>Returns how many examples should be given to the model in its instruction prompt</p> Source code in <code>src/autolabel/configs/config.py</code> <pre><code>def few_shot_num_examples(self) -&gt; int:\n\"\"\"Returns how many examples should be given to the model in its instruction prompt\"\"\"\nreturn self._prompt_config.get(self.FEW_SHOT_NUM_KEY, 0)\n</code></pre>"},{"location":"reference/configs/#src.autolabel.configs.config.AutolabelConfig.label_column","title":"<code>label_column()</code>","text":"<p>Returns the name of the column containing labels for the dataset. Used for comparing accuracy of autolabel results vs ground truth</p> Source code in <code>src/autolabel/configs/config.py</code> <pre><code>def label_column(self) -&gt; str:\n\"\"\"Returns the name of the column containing labels for the dataset. Used for comparing accuracy of autolabel results vs ground truth\"\"\"\nreturn self._dataset_config.get(self.LABEL_COLUMN_KEY, None)\n</code></pre>"},{"location":"reference/configs/#src.autolabel.configs.config.AutolabelConfig.labels_list","title":"<code>labels_list()</code>","text":"<p>Returns a list of valid labels</p> Source code in <code>src/autolabel/configs/config.py</code> <pre><code>def labels_list(self) -&gt; List[str]:\n\"\"\"Returns a list of valid labels\"\"\"\nreturn self._prompt_config.get(self.VALID_LABELS_KEY, [])\n</code></pre>"},{"location":"reference/configs/#src.autolabel.configs.config.AutolabelConfig.model_name","title":"<code>model_name()</code>","text":"<p>Returns the name of the model being used for labeling (e.g. gpt-4, claude-v1)</p> Source code in <code>src/autolabel/configs/config.py</code> <pre><code>def model_name(self) -&gt; str:\n\"\"\"Returns the name of the model being used for labeling (e.g. gpt-4, claude-v1)\"\"\"\nreturn self._model_config[self.MODEL_NAME_KEY]\n</code></pre>"},{"location":"reference/configs/#src.autolabel.configs.config.AutolabelConfig.model_params","title":"<code>model_params()</code>","text":"<p>Returns a dict of configured settings for the model (e.g. hyperparameters)</p> Source code in <code>src/autolabel/configs/config.py</code> <pre><code>def model_params(self) -&gt; Dict:\n\"\"\"Returns a dict of configured settings for the model (e.g. hyperparameters)\"\"\"\nreturn self._model_config.get(self.MODEL_PARAMS_KEY, {})\n</code></pre>"},{"location":"reference/configs/#src.autolabel.configs.config.AutolabelConfig.provider","title":"<code>provider()</code>","text":"<p>Returns the name of the entity that provides the currently configured model (e.g. OpenAI, Anthropic, Refuel)</p> Source code in <code>src/autolabel/configs/config.py</code> <pre><code>def provider(self) -&gt; str:\n\"\"\"Returns the name of the entity that provides the currently configured model (e.g. OpenAI, Anthropic, Refuel)\"\"\"\nreturn self._model_config[self.PROVIDER_KEY]\n</code></pre>"},{"location":"reference/configs/#src.autolabel.configs.config.AutolabelConfig.task_type","title":"<code>task_type()</code>","text":"<p>Returns the type of task we have configured the labeler to perform (e.g. Classification, Question Answering)</p> Source code in <code>src/autolabel/configs/config.py</code> <pre><code>def task_type(self) -&gt; str:\n\"\"\"Returns the type of task we have configured the labeler to perform (e.g. Classification, Question Answering)\"\"\"\nreturn self.config[self.TASK_TYPE_KEY]\n</code></pre>"},{"location":"reference/configs/#src.autolabel.configs.config.AutolabelConfig.text_column","title":"<code>text_column()</code>","text":"<p>Returns the name of the column containing text data we intend to label</p> Source code in <code>src/autolabel/configs/config.py</code> <pre><code>def text_column(self) -&gt; str:\n\"\"\"Returns the name of the column containing text data we intend to label\"\"\"\nreturn self._dataset_config.get(self.TEXT_COLUMN_KEY, None)\n</code></pre>"},{"location":"reference/data_models/","title":"Data Models","text":"<p>The Data Model classes are used to save the progress of AutoLabel jobs in an SQL database. </p> <p>Saved data is stored in .autolabel.db</p> <p>Every Data Model class implements its own \"get\" and \"create\" methods for accessing this saved data.</p>"},{"location":"reference/data_models/#src.autolabel.data_models.annotation.AnnotationModel","title":"<code>AnnotationModel</code>","text":"<p>         Bases: <code>Base</code></p> Source code in <code>src/autolabel/data_models/annotation.py</code> <pre><code>class AnnotationModel(Base):\n__tablename__ = \"annotations\"\nid = Column(Integer, primary_key=True, autoincrement=True)\ncreated_at = Column(DateTime(timezone=True), server_default=func.now())\nindex = Column(Integer)\nllm_annotation = Column(JSON)\ntask_run_id = Column(Integer, ForeignKey(\"task_runs.id\"))\ntask_runs = relationship(\"TaskRunModel\", back_populates=\"annotations\")\ndef __repr__(self):\nreturn f\"&lt;AnnotationModel(id={self.id}, index={self.index}, annotation={self.llm_annotation})\"\n@classmethod\ndef create_from_llm_annotation(\ncls, db, llm_annotation: LLMAnnotation, index: int, task_run_id: int\n):\ndb_object = cls(\nllm_annotation=json.loads(llm_annotation.json()),\nindex=index,\ntask_run_id=task_run_id,\n)\ndb.add(db_object)\ndb_object = db.query(cls).order_by(cls.id.desc()).first()\nlogger.debug(f\"created new annotation: {db_object}\")\nreturn db_object\n@classmethod\ndef get_annotations_by_task_run_id(cls, db, task_run_id: int):\nannotations = (\ndb.query(cls)\n.filter(cls.task_run_id == task_run_id)\n.order_by(cls.index)\n.all()\n)\nfiltered_annotations = []\nids = {}\nfor annotation in annotations:\nif annotation.index not in ids:\nids[annotation.index] = True\nfiltered_annotations.append(annotation)\nreturn filtered_annotations\n@classmethod\ndef from_pydantic(cls, annotation: BaseModel):\nreturn cls(**json.loads(annotation.json()))\ndef delete(self, db):\ndb.delete(self)\n</code></pre>"},{"location":"reference/data_models/#src.autolabel.data_models.cache.CacheEntryModel","title":"<code>CacheEntryModel</code>","text":"<p>         Bases: <code>Base</code></p> <p>an SQLAlchemy based Cache system for storing and retriving CacheEntries</p> Source code in <code>src/autolabel/data_models/cache.py</code> <pre><code>class CacheEntryModel(Base):\n\"\"\"an SQLAlchemy based Cache system for storing and retriving CacheEntries\"\"\"\n__tablename__ = \"generation_cache\"\nid = Column(Integer, primary_key=True)\nmodel_name = Column(String(50))\nprompt = Column(Text)\nmodel_params = Column(Text)\ngenerations = Column(JSON)\ndef __repr__(self):\nreturn f\"&lt;Cache(model_name={self.model_name},prompt={self.prompt},model_params={self.model_params},generations={self.generations})&gt;\"\n@classmethod\ndef get(cls, db, cache_entry: CacheEntry):\nlooked_up_entry = (\ndb.query(cls)\n.filter(\ncls.model_name == cache_entry.model_name,\ncls.prompt == cache_entry.prompt,\ncls.model_params == cache_entry.model_params,\n)\n.first()\n)\nif not looked_up_entry:\nreturn None\ngenerations = json.loads(looked_up_entry.generations)[\"generations\"]\ngenerations = [Generation(**gen) for gen in generations]\nentry = CacheEntry(\nmodel_name=looked_up_entry.model_name,\nprompt=looked_up_entry.prompt,\nmodel_params=looked_up_entry.model_params,\ngenerations=generations,\n)\nreturn entry\n@classmethod\ndef insert(cls, db, cache_entry: BaseModel):\ngenerations = {\"generations\": [gen.dict() for gen in cache_entry.generations]}\ndb_object = cls(\nmodel_name=cache_entry.model_name,\nprompt=cache_entry.prompt,\nmodel_params=cache_entry.model_params,\ngenerations=json.dumps(generations),\n)\ndb.add(db_object)\nreturn cache_entry\n@classmethod\ndef clear(cls, db):\ndb.query(cls).delete()\n</code></pre>"},{"location":"reference/data_models/#src.autolabel.data_models.dataset.DatasetModel","title":"<code>DatasetModel</code>","text":"<p>         Bases: <code>Base</code></p> Source code in <code>src/autolabel/data_models/dataset.py</code> <pre><code>class DatasetModel(Base):\n__tablename__ = \"datasets\"\nid = Column(String(32), primary_key=True)\ninput_file = Column(String(50))\nstart_index = Column(Integer)\nend_index = Column(Integer)\ntask_runs = relationship(\"TaskRunModel\", back_populates=\"dataset\")\ndef __repr__(self):\nreturn f\"&lt;DatasetModel(id={self.id}, input_file={self.input_file}, start_index={self.start_index}, end_index={self.end_index})&gt;\"\n@classmethod\ndef create(cls, db, dataset: BaseModel):\ndb_object = cls(**json.loads(dataset.json()))\ndb.add(db_object)\nreturn db_object\n@classmethod\ndef get_by_id(cls, db, id: int):\nreturn db.query(cls).filter(cls.id == id).first()\n@classmethod\ndef get_by_input_file(cls, db, input_file: str):\nreturn db.query(cls).filter(cls.input_file == input_file).first()\ndef delete(self, db):\ndb.delete(self)\n</code></pre>"},{"location":"reference/data_models/#src.autolabel.data_models.task.TaskModel","title":"<code>TaskModel</code>","text":"<p>         Bases: <code>Base</code></p> Source code in <code>src/autolabel/data_models/task.py</code> <pre><code>class TaskModel(Base):\n__tablename__ = \"tasks\"\nid = Column(String(32), primary_key=True)\ntask_type = Column(String(50))\nprovider = Column(String(50))\nmodel_name = Column(String(50))\nconfig = Column(Text)\ntask_runs = relationship(\"TaskRunModel\", back_populates=\"task\")\ndef __repr__(self):\nreturn f\"&lt;TaskModel(id={self.id}, task_type={self.task_type}, provider={self.provider}, model_name={self.model_name})&gt;\"\n@classmethod\ndef create(cls, db, task: BaseModel):\ndb_object = cls(**json.loads(task.json()))\ndb.add(db_object)\nreturn db_object\n@classmethod\ndef get_by_id(cls, db, id: int):\nreturn db.query(cls).filter(cls.id == id).first()\ndef delete(self, db):\ndb.delete(self)\n</code></pre>"},{"location":"reference/data_models/#src.autolabel.data_models.task_run.TaskRunModel","title":"<code>TaskRunModel</code>","text":"<p>         Bases: <code>Base</code></p> Source code in <code>src/autolabel/data_models/task_run.py</code> <pre><code>class TaskRunModel(Base):\n__tablename__ = \"task_runs\"\nid = Column(\nInteger,\ndefault=lambda: uuid.uuid4().int &gt;&gt; (128 - 32),\nprimary_key=True,\n)\ntask_id = Column(String(32), ForeignKey(\"tasks.id\"))\ncreated_at = Column(DateTime(timezone=True), server_default=func.now())\ndataset_id = Column(String(32), ForeignKey(\"datasets.id\"))\ncurrent_index = Column(Integer)\nerror = Column(String(256))\nmetrics = Column(Text)\noutput_file = Column(String(50))\nstatus = Column(String(50))\ntask = relationship(\"TaskModel\", back_populates=\"task_runs\")\ndataset = relationship(\"DatasetModel\", back_populates=\"task_runs\")\nannotations = relationship(\"AnnotationModel\", back_populates=\"task_runs\")\ndef __repr__(self):\nreturn f\"&lt;TaskRunModel(id={self.id}, created_at={str(self.created_at)}, task_id={self.task_id}, dataset_id={self.dataset_id}, output_file={self.output_file}, current_index={self.current_index}, status={self.status}, error={self.error}, metrics={self.metrics})\"\n@classmethod\ndef create(cls, db, task_run: BaseModel):\nlogger.debug(f\"creating new task: {task_run}\")\ndb_object = cls(**task_run.dict())\ndb.add(db_object)\ndb.flush()\ndb.refresh(db_object)\nlogger.debug(f\"created new task: {db_object}\")\nreturn db_object\n@classmethod\ndef get(cls, db, task_id: str, dataset_id: str):\nreturn (\ndb.query(cls)\n.filter(cls.task_id == task_id, cls.dataset_id == dataset_id)\n.first()\n)\n@classmethod\ndef from_pydantic(cls, task_run: BaseModel):\nreturn cls(**json.loads(task_run.json()))\n@classmethod\ndef update(cls, db, task_run: BaseModel):\ntask_run_id = task_run.id\ntask_run_orm = db.query(cls).filter(cls.id == task_run_id).first()\nlogger.debug(f\"updating task_run: {task_run}\")\nfor key, value in task_run.dict().items():\nsetattr(task_run_orm, key, value)\nlogger.debug(f\"task_run updated: {task_run}\")\nreturn TaskRun.from_orm(task_run_orm)\n@classmethod\ndef delete_by_id(cls, db, id: int):\ndb.query(cls).filter(cls.id == id).delete()\ndef delete(self, db):\ndb.delete(self)\ndb.flush()\n</code></pre>"},{"location":"reference/data_models/#src.autolabel.database.state_manager.StateManager","title":"<code>StateManager</code>","text":"Source code in <code>src/autolabel/database/state_manager.py</code> <pre><code>class StateManager:\ndef __init__(self):\nself.engine = create_db_engine()\nself.base = Base\nself.session = None\ndef initialize(self):\nself.base.metadata.create_all(self.engine)\nself.session = sessionmaker(bind=self.engine, autocommit=True)()\ndef initialize_dataset(\nself,\ndataset: Union[str, pd.DataFrame],\nconfig: AutolabelConfig,\nstart_index: int,\nmax_items: Optional[int],\n):\n# TODO: Check if this works for max_items = None\ndataset_id = Dataset.create_id(dataset, config, start_index, max_items)\ndataset_orm = DatasetModel.get_by_id(self.session, dataset_id)\nif dataset_orm:\nreturn Dataset.from_orm(dataset_orm)\ndataset = Dataset(\nid=dataset_id,\ninput_file=dataset if isinstance(dataset, str) else \"\",\nstart_index=start_index,\nend_index=start_index + max_items if max_items else -1,\n)\nreturn Dataset.from_orm(DatasetModel.create(self.session, dataset))\ndef initialize_task(self, config: AutolabelConfig):\ntask_id = Task.create_id(config)\ntask_orm = TaskModel.get_by_id(self.session, task_id)\nif task_orm:\nreturn Task.from_orm(task_orm)\ntask = Task(\nid=task_id,\nconfig=config.to_json(),\ntask_type=config.task_type(),\nprovider=config.provider(),\nmodel_name=config.model_name(),\n)\nreturn Task.from_orm(TaskModel.create(self.session, task))\ndef get_task_run(self, task_id: str, dataset_id: str):\ntask_run_orm = TaskRunModel.get(self.session, task_id, dataset_id)\nif task_run_orm:\nreturn TaskRun.from_orm(task_run_orm)\nelse:\nreturn None\ndef create_task_run(\nself, output_file: str, task_id: str, dataset_id: str\n) -&gt; TaskRun:\nlogger.debug(f\"creating new task_run\")\nnew_task_run = TaskRun(\ntask_id=task_id,\ndataset_id=dataset_id,\nstatus=TaskStatus.ACTIVE,\ncurrent_index=0,\noutput_file=output_file,\ncreated_at=datetime.now(),\n)\ntask_run_orm = TaskRunModel.create(self.session, new_task_run)\nreturn TaskRun.from_orm(task_run_orm)\n</code></pre>"},{"location":"reference/data_models/#src.autolabel.database.engine.create_db_engine","title":"<code>create_db_engine(db_path=DB_PATH)</code>","text":"Source code in <code>src/autolabel/database/engine.py</code> <pre><code>def create_db_engine(db_path: Optional[str] = DB_PATH) -&gt; Engine:\nglobal DB_ENGINE\nif DB_ENGINE is None:\nDB_ENGINE = create_engine(f\"sqlite:///{db_path}\")\nreturn DB_ENGINE\n</code></pre>"},{"location":"reference/example_select/","title":"Example Selector","text":""},{"location":"reference/example_select/#src.autolabel.few_shot.fixed_example_selector.FixedExampleSelector","title":"<code>FixedExampleSelector</code>","text":"<p>         Bases: <code>BaseExampleSelector</code>, <code>BaseModel</code></p> <p>Example selector to handle the case of fixed few-shot context i.e. every input prompt to the labeling model has the same few-shot examples</p> Source code in <code>src/autolabel/few_shot/fixed_example_selector.py</code> <pre><code>class FixedExampleSelector(BaseExampleSelector, BaseModel):\n\"\"\"Example selector to handle the case of fixed few-shot context\n    i.e. every input prompt to the labeling model has the same few-shot examples\n    \"\"\"\nexamples: List[dict]\n\"\"\"A list of the examples that the prompt template expects.\"\"\"\nk: int = 4\n\"\"\"Number of examples to select\"\"\"\nclass Config:\n\"\"\"Configuration for this pydantic object.\"\"\"\nextra = Extra.forbid\narbitrary_types_allowed = True\ndef add_example(self, example: Dict[str, str]) -&gt; None:\nself.examples.append(example)\ndef select_examples(self, input_variables: Dict[str, str]) -&gt; List[dict]:\n\"\"\"Select which examples to use based on the input lengths.\"\"\"\nreturn self.examples[: self.k]\n@classmethod\ndef from_examples(\ncls,\nexamples: List,\nk: int = 4,\n) -&gt; FixedExampleSelector:\n\"\"\"Create pass-through example selector using example list\n        Returns:\n            The FixedExampleSelector instantiated\n        \"\"\"\nreturn cls(examples=examples, k=k)\n</code></pre>"},{"location":"reference/example_select/#src.autolabel.few_shot.fixed_example_selector.FixedExampleSelector.examples","title":"<code>examples: List[dict]</code>  <code>instance-attribute</code>","text":"<p>A list of the examples that the prompt template expects.</p>"},{"location":"reference/example_select/#src.autolabel.few_shot.fixed_example_selector.FixedExampleSelector.k","title":"<code>k: int = 4</code>  <code>instance-attribute</code> <code>class-attribute</code>","text":"<p>Number of examples to select</p>"},{"location":"reference/example_select/#src.autolabel.few_shot.fixed_example_selector.FixedExampleSelector.Config","title":"<code>Config</code>","text":"<p>Configuration for this pydantic object.</p> Source code in <code>src/autolabel/few_shot/fixed_example_selector.py</code> <pre><code>class Config:\n\"\"\"Configuration for this pydantic object.\"\"\"\nextra = Extra.forbid\narbitrary_types_allowed = True\n</code></pre>"},{"location":"reference/example_select/#src.autolabel.few_shot.fixed_example_selector.FixedExampleSelector.from_examples","title":"<code>from_examples(examples, k=4)</code>  <code>classmethod</code>","text":"<p>Create pass-through example selector using example list</p> <p>Returns:</p> Type Description <code>FixedExampleSelector</code> <p>The FixedExampleSelector instantiated</p> Source code in <code>src/autolabel/few_shot/fixed_example_selector.py</code> <pre><code>@classmethod\ndef from_examples(\ncls,\nexamples: List,\nk: int = 4,\n) -&gt; FixedExampleSelector:\n\"\"\"Create pass-through example selector using example list\n    Returns:\n        The FixedExampleSelector instantiated\n    \"\"\"\nreturn cls(examples=examples, k=k)\n</code></pre>"},{"location":"reference/example_select/#src.autolabel.few_shot.fixed_example_selector.FixedExampleSelector.select_examples","title":"<code>select_examples(input_variables)</code>","text":"<p>Select which examples to use based on the input lengths.</p> Source code in <code>src/autolabel/few_shot/fixed_example_selector.py</code> <pre><code>def select_examples(self, input_variables: Dict[str, str]) -&gt; List[dict]:\n\"\"\"Select which examples to use based on the input lengths.\"\"\"\nreturn self.examples[: self.k]\n</code></pre>"},{"location":"reference/example_select/#src.autolabel.few_shot.vector_store","title":"<code>vector_store</code>","text":""},{"location":"reference/example_select/#src.autolabel.few_shot.vector_store.VectorStoreWrapper","title":"<code>VectorStoreWrapper</code>","text":"<p>         Bases: <code>VectorStore</code></p> Source code in <code>src/autolabel/few_shot/vector_store.py</code> <pre><code>class VectorStoreWrapper(VectorStore):\ndef __init__(\nself,\nembedding_function: Optional[Embeddings] = None,\ncorpus_embeddings: Optional[Tensor] = None,\ntexts: Optional[List[str]] = None,\nmetadatas: Optional[List[Dict[str, str]]] = None,\n) -&gt; None:\nself._embedding_function = embedding_function\nself._corpus_embeddings = corpus_embeddings\nself._texts = texts\nself._metadatas = metadatas\ndef add_texts(\nself,\ntexts: Iterable[str],\nmetadatas: Optional[List[Dict[str, str]]] = None,\n) -&gt; List[str]:\n\"\"\"Run texts through the embeddings and add to the vectorstore. Currently, the vectorstore is reinitialized each time, because we do not require a persistent vector store for example selection.\n        Args:\n            texts (Iterable[str]): Texts to add to the vectorstore.\n            metadatas (Optional[List[dict]], optional): Optional list of metadatas.\n        Returns:\n            List[str]: List of IDs of the added texts.\n        \"\"\"\nembeddings = None\nif self._embedding_function is not None:\nembeddings = self._embedding_function.embed_documents(list(texts))\nself._corpus_embeddings = torch.tensor(embeddings)\nself._texts = texts\nself._metadatas = metadatas\nreturn metadatas\ndef similarity_search(\nself,\nquery: str,\nk: int = 4,\nfilter: Optional[Dict[str, str]] = None,\n**kwargs: Any,\n) -&gt; List[Document]:\n\"\"\"Run semantic similarity search.\n        Args:\n            query (str): Query text to search for.\n            k (int): Number of results to return. Defaults to 4.\n            filter (Optional[Dict[str, str]]): Filter by metadata. Defaults to None.\n        Returns:\n            List[Document]: List of documents most similar to the query text.\n        \"\"\"\ndocs_and_scores = self.similarity_search_with_score(query, k, filter=filter)\nreturn [doc for doc, _ in docs_and_scores]\ndef similarity_search_with_score(\nself,\nquery: str,\nk: int = 4,\nfilter: Optional[Dict[str, str]] = None,\n**kwargs: Any,\n) -&gt; List[Tuple[Document, float]]:\n\"\"\"Run semantic similarity search and retrieve distances.\n        Args:\n            query (str): Query text to search for.\n            k (int): Number of results to return. Defaults to 4.\n            filter (Optional[Dict[str, str]]): Filter by metadata. Defaults to None.\n        Returns:\n            List[Tuple[Document, float]]: List of documents most similar to the query\n                text with distance in float.\n        \"\"\"\nquery_embeddings = torch.tensor([self._embedding_function.embed_query(query)])\nresult_ids_and_scores = semantic_search(\ncorpus_embeddings=self._corpus_embeddings,\nquery_embeddings=query_embeddings,\ntop_k=k,\n)\nresult_ids = [result[\"corpus_id\"] for result in result_ids_and_scores[0]]\nscores = [result[\"score\"] for result in result_ids_and_scores[0]]\nresults = {}\nresults[\"documents\"] = [[self._texts[index] for index in result_ids]]\nresults[\"distances\"] = [scores]\nresults[\"metadatas\"] = [[self._metadatas[index] for index in result_ids]]\nreturn _results_to_docs_and_scores(results)\ndef max_marginal_relevance_search_by_vector(\nself,\nquery: str,\nk: int = 4,\nfetch_k: int = 20,\nlambda_mult: float = 0.5,\n**kwargs: Any,\n) -&gt; List[Document]:\nquery_embedding = self._embedding_function.embed_query(query)\nquery_embeddings = torch.tensor([query_embedding])\nresult_ids_and_scores = semantic_search(\ncorpus_embeddings=self._corpus_embeddings,\nquery_embeddings=query_embeddings,\ntop_k=fetch_k,\n)\nresult_ids = [result[\"corpus_id\"] for result in result_ids_and_scores[0]]\nscores = [result[\"score\"] for result in result_ids_and_scores[0]]\nfetched_embeddings = torch.index_select(\ninput=self._corpus_embeddings, dim=0, index=torch.tensor(result_ids)\n).tolist()\nmmr_selected = maximal_marginal_relevance(\nnp.array([query_embedding], dtype=np.float32),\nfetched_embeddings,\nk=k,\nlambda_mult=lambda_mult,\n)\nselected_result_ids = [result_ids[i] for i in mmr_selected]\nselected_scores = [scores[i] for i in mmr_selected]\nresults = {}\nresults[\"documents\"] = [[self._texts[index] for index in selected_result_ids]]\nresults[\"distances\"] = [selected_scores]\nresults[\"metadatas\"] = [\n[self._metadatas[index] for index in selected_result_ids]\n]\nreturn _results_to_docs_and_scores(results)\ndef max_marginal_relevance_search(\nself,\nquery: str,\nk: int = 4,\nfetch_k: int = 20,\nlambda_mult: float = 0.5,\n**kwargs: Any,\n) -&gt; List[Document]:\ndocs_and_scores = self.max_marginal_relevance_search_by_vector(\nquery, k, fetch_k, lambda_mult=lambda_mult\n)\nreturn [doc for doc, _ in docs_and_scores]\n@classmethod\ndef from_texts(\ncls: Type[VectorStoreWrapper],\ntexts: List[str],\nembedding: Optional[Embeddings] = None,\nmetadatas: Optional[List[dict]] = None,\n**kwargs: Any,\n) -&gt; VectorStoreWrapper:\n\"\"\"Create a vectorstore from raw text.\n        The data will be ephemeral in-memory.\n        Args:\n            texts (List[str]): List of texts to add to the collection.\n            embedding (Optional[Embeddings]): Embedding function. Defaults to None.\n            metadatas (Optional[List[dict]]): List of metadatas. Defaults to None.\n        Returns:\n            vector_store: Vectorstore with seedset embeddings\n        \"\"\"\nvector_store = cls(\nembedding_function=embedding, corpus_embeddings=None, texts=None, **kwargs\n)\nvector_store.add_texts(texts=texts, metadatas=metadatas)\nreturn vector_store\n</code></pre>"},{"location":"reference/example_select/#src.autolabel.few_shot.vector_store.VectorStoreWrapper.add_texts","title":"<code>add_texts(texts, metadatas=None)</code>","text":"<p>Run texts through the embeddings and add to the vectorstore. Currently, the vectorstore is reinitialized each time, because we do not require a persistent vector store for example selection.</p> <p>Parameters:</p> Name Type Description Default <code>texts</code> <code>Iterable[str]</code> <p>Texts to add to the vectorstore.</p> required <code>metadatas</code> <code>Optional[List[dict]]</code> <p>Optional list of metadatas.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: List of IDs of the added texts.</p> Source code in <code>src/autolabel/few_shot/vector_store.py</code> <pre><code>def add_texts(\nself,\ntexts: Iterable[str],\nmetadatas: Optional[List[Dict[str, str]]] = None,\n) -&gt; List[str]:\n\"\"\"Run texts through the embeddings and add to the vectorstore. Currently, the vectorstore is reinitialized each time, because we do not require a persistent vector store for example selection.\n    Args:\n        texts (Iterable[str]): Texts to add to the vectorstore.\n        metadatas (Optional[List[dict]], optional): Optional list of metadatas.\n    Returns:\n        List[str]: List of IDs of the added texts.\n    \"\"\"\nembeddings = None\nif self._embedding_function is not None:\nembeddings = self._embedding_function.embed_documents(list(texts))\nself._corpus_embeddings = torch.tensor(embeddings)\nself._texts = texts\nself._metadatas = metadatas\nreturn metadatas\n</code></pre>"},{"location":"reference/example_select/#src.autolabel.few_shot.vector_store.VectorStoreWrapper.from_texts","title":"<code>from_texts(texts, embedding=None, metadatas=None, **kwargs)</code>  <code>classmethod</code>","text":"<p>Create a vectorstore from raw text. The data will be ephemeral in-memory.</p> <p>Parameters:</p> Name Type Description Default <code>texts</code> <code>List[str]</code> <p>List of texts to add to the collection.</p> required <code>embedding</code> <code>Optional[Embeddings]</code> <p>Embedding function. Defaults to None.</p> <code>None</code> <code>metadatas</code> <code>Optional[List[dict]]</code> <p>List of metadatas. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>vector_store</code> <code>VectorStoreWrapper</code> <p>Vectorstore with seedset embeddings</p> Source code in <code>src/autolabel/few_shot/vector_store.py</code> <pre><code>@classmethod\ndef from_texts(\ncls: Type[VectorStoreWrapper],\ntexts: List[str],\nembedding: Optional[Embeddings] = None,\nmetadatas: Optional[List[dict]] = None,\n**kwargs: Any,\n) -&gt; VectorStoreWrapper:\n\"\"\"Create a vectorstore from raw text.\n    The data will be ephemeral in-memory.\n    Args:\n        texts (List[str]): List of texts to add to the collection.\n        embedding (Optional[Embeddings]): Embedding function. Defaults to None.\n        metadatas (Optional[List[dict]]): List of metadatas. Defaults to None.\n    Returns:\n        vector_store: Vectorstore with seedset embeddings\n    \"\"\"\nvector_store = cls(\nembedding_function=embedding, corpus_embeddings=None, texts=None, **kwargs\n)\nvector_store.add_texts(texts=texts, metadatas=metadatas)\nreturn vector_store\n</code></pre>"},{"location":"reference/example_select/#src.autolabel.few_shot.vector_store.VectorStoreWrapper.similarity_search","title":"<code>similarity_search(query, k=4, filter=None, **kwargs)</code>","text":"<p>Run semantic similarity search.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>Query text to search for.</p> required <code>k</code> <code>int</code> <p>Number of results to return. Defaults to 4.</p> <code>4</code> <code>filter</code> <code>Optional[Dict[str, str]]</code> <p>Filter by metadata. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Document]</code> <p>List[Document]: List of documents most similar to the query text.</p> Source code in <code>src/autolabel/few_shot/vector_store.py</code> <pre><code>def similarity_search(\nself,\nquery: str,\nk: int = 4,\nfilter: Optional[Dict[str, str]] = None,\n**kwargs: Any,\n) -&gt; List[Document]:\n\"\"\"Run semantic similarity search.\n    Args:\n        query (str): Query text to search for.\n        k (int): Number of results to return. Defaults to 4.\n        filter (Optional[Dict[str, str]]): Filter by metadata. Defaults to None.\n    Returns:\n        List[Document]: List of documents most similar to the query text.\n    \"\"\"\ndocs_and_scores = self.similarity_search_with_score(query, k, filter=filter)\nreturn [doc for doc, _ in docs_and_scores]\n</code></pre>"},{"location":"reference/example_select/#src.autolabel.few_shot.vector_store.VectorStoreWrapper.similarity_search_with_score","title":"<code>similarity_search_with_score(query, k=4, filter=None, **kwargs)</code>","text":"<p>Run semantic similarity search and retrieve distances.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>Query text to search for.</p> required <code>k</code> <code>int</code> <p>Number of results to return. Defaults to 4.</p> <code>4</code> <code>filter</code> <code>Optional[Dict[str, str]]</code> <p>Filter by metadata. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Tuple[Document, float]]</code> <p>List[Tuple[Document, float]]: List of documents most similar to the query text with distance in float.</p> Source code in <code>src/autolabel/few_shot/vector_store.py</code> <pre><code>def similarity_search_with_score(\nself,\nquery: str,\nk: int = 4,\nfilter: Optional[Dict[str, str]] = None,\n**kwargs: Any,\n) -&gt; List[Tuple[Document, float]]:\n\"\"\"Run semantic similarity search and retrieve distances.\n    Args:\n        query (str): Query text to search for.\n        k (int): Number of results to return. Defaults to 4.\n        filter (Optional[Dict[str, str]]): Filter by metadata. Defaults to None.\n    Returns:\n        List[Tuple[Document, float]]: List of documents most similar to the query\n            text with distance in float.\n    \"\"\"\nquery_embeddings = torch.tensor([self._embedding_function.embed_query(query)])\nresult_ids_and_scores = semantic_search(\ncorpus_embeddings=self._corpus_embeddings,\nquery_embeddings=query_embeddings,\ntop_k=k,\n)\nresult_ids = [result[\"corpus_id\"] for result in result_ids_and_scores[0]]\nscores = [result[\"score\"] for result in result_ids_and_scores[0]]\nresults = {}\nresults[\"documents\"] = [[self._texts[index] for index in result_ids]]\nresults[\"distances\"] = [scores]\nresults[\"metadatas\"] = [[self._metadatas[index] for index in result_ids]]\nreturn _results_to_docs_and_scores(results)\n</code></pre>"},{"location":"reference/example_select/#src.autolabel.few_shot.vector_store.cos_sim","title":"<code>cos_sim(a, b)</code>","text":"<p>Computes the cosine similarity cos_sim(a[i], b[j]) for all i and j.</p> <p>Returns:</p> Name Type Description <code>cos_sim</code> <code>Tensor</code> <p>Matrix with res(i)(j) = cos_sim(a[i], b[j])</p> Source code in <code>src/autolabel/few_shot/vector_store.py</code> <pre><code>def cos_sim(a: Tensor, b: Tensor) -&gt; Tensor:\n\"\"\"\n    Computes the cosine similarity cos_sim(a[i], b[j]) for all i and j.\n    Returns:\n        cos_sim: Matrix with res(i)(j) = cos_sim(a[i], b[j])\n    \"\"\"\nif not isinstance(a, torch.Tensor):\na = torch.tensor(a)\nif not isinstance(b, torch.Tensor):\nb = torch.tensor(b)\nif len(a.shape) == 1:\na = a.unsqueeze(0)\nif len(b.shape) == 1:\nb = b.unsqueeze(0)\na_norm = torch.nn.functional.normalize(a, p=2, dim=1)\nb_norm = torch.nn.functional.normalize(b, p=2, dim=1)\nreturn torch.mm(a_norm, b_norm.transpose(0, 1))\n</code></pre>"},{"location":"reference/example_select/#src.autolabel.few_shot.vector_store.semantic_search","title":"<code>semantic_search(query_embeddings, corpus_embeddings, query_chunk_size=100, corpus_chunk_size=500000, top_k=10, score_function=cos_sim)</code>","text":"<p>Semantic similarity search based on cosine similarity score. Implementation from this project: https://github.com/UKPLab/sentence-transformers</p> Source code in <code>src/autolabel/few_shot/vector_store.py</code> <pre><code>def semantic_search(\nquery_embeddings: Tensor,\ncorpus_embeddings: Tensor,\nquery_chunk_size: int = 100,\ncorpus_chunk_size: int = 500000,\ntop_k: int = 10,\nscore_function: Callable[[Tensor, Tensor], Tensor] = cos_sim,\n):\n\"\"\"\n    Semantic similarity search based on cosine similarity score. Implementation from this project: https://github.com/UKPLab/sentence-transformers\n    \"\"\"\nif isinstance(query_embeddings, (np.ndarray, np.generic)):\nquery_embeddings = torch.from_numpy(query_embeddings)\nelif isinstance(query_embeddings, list):\nquery_embeddings = torch.stack(query_embeddings)\nif len(query_embeddings.shape) == 1:\nquery_embeddings = query_embeddings.unsqueeze(0)\nif isinstance(corpus_embeddings, (np.ndarray, np.generic)):\ncorpus_embeddings = torch.from_numpy(corpus_embeddings)\nelif isinstance(corpus_embeddings, list):\ncorpus_embeddings = torch.stack(corpus_embeddings)\n# Check that corpus and queries are on the same device\nif corpus_embeddings.device != query_embeddings.device:\nquery_embeddings = query_embeddings.to(corpus_embeddings.device)\nqueries_result_list = [[] for _ in range(len(query_embeddings))]\nfor query_start_idx in range(0, len(query_embeddings), query_chunk_size):\n# Iterate over chunks of the corpus\nfor corpus_start_idx in range(0, len(corpus_embeddings), corpus_chunk_size):\n# Compute cosine similarities\ncos_scores = score_function(\nquery_embeddings[query_start_idx : query_start_idx + query_chunk_size],\ncorpus_embeddings[\ncorpus_start_idx : corpus_start_idx + corpus_chunk_size\n],\n)\n# Get top-k scores\ncos_scores_top_k_values, cos_scores_top_k_idx = torch.topk(\ncos_scores,\nmin(top_k, len(cos_scores[0])),\ndim=1,\nlargest=True,\nsorted=False,\n)\ncos_scores_top_k_values = cos_scores_top_k_values.cpu().tolist()\ncos_scores_top_k_idx = cos_scores_top_k_idx.cpu().tolist()\nfor query_itr in range(len(cos_scores)):\nfor sub_corpus_id, score in zip(\ncos_scores_top_k_idx[query_itr], cos_scores_top_k_values[query_itr]\n):\ncorpus_id = corpus_start_idx + sub_corpus_id\nquery_id = query_start_idx + query_itr\nif len(queries_result_list[query_id]) &lt; top_k:\nheapq.heappush(\nqueries_result_list[query_id], (score, corpus_id)\n)  # heaqp tracks the quantity of the first element in the tuple\nelse:\nheapq.heappushpop(\nqueries_result_list[query_id], (score, corpus_id)\n)\n# change the data format and sort\nfor query_id in range(len(queries_result_list)):\nfor doc_itr in range(len(queries_result_list[query_id])):\nscore, corpus_id = queries_result_list[query_id][doc_itr]\nqueries_result_list[query_id][doc_itr] = {\n\"corpus_id\": corpus_id,\n\"score\": score,\n}\nqueries_result_list[query_id] = sorted(\nqueries_result_list[query_id], key=lambda x: x[\"score\"], reverse=True\n)\nreturn queries_result_list\n</code></pre>"},{"location":"reference/labeler/","title":"AutoLabeler","text":""},{"location":"reference/labeler/#src.autolabel.labeler.LabelingAgent","title":"<code>LabelingAgent</code>","text":"Source code in <code>src/autolabel/labeler.py</code> <pre><code>class LabelingAgent:\nCHUNK_SIZE = 5\nCOST_KEY = \"Cost in $\"\ndef __init__(\nself,\nconfig: Union[str, Dict],\ncache: Optional[bool] = True,\n) -&gt; None:\nself.db = StateManager()\nself.cache = SQLAlchemyCache() if cache else None\nself.config = AutolabelConfig(config)\nself.task = TaskFactory.from_config(self.config)\nself.llm: BaseModel = ModelFactory.from_config(self.config, cache=self.cache)\nself.confidence = ConfidenceCalculator(\nscore_type=\"logprob_average\", llm=self.llm\n)\ndef run(\nself,\ndataset: Union[str, pd.DataFrame],\nmax_items: Optional[int] = None,\noutput_name: Optional[str] = None,\nstart_index: Optional[int] = 0,\neval_every: Optional[int] = 50,\n) -&gt; Tuple[pd.Series, pd.DataFrame, List[MetricResult]]:\n\"\"\"Labels data in a given dataset. Output written to new CSV file.\n        Args:\n            dataset: path to CSV dataset to be annotated\n            max_items: maximum items in dataset to be annotated\n            output_name: custom name of output CSV file\n            start_index: skips annotating [0, start_index)\n        \"\"\"\nself.db.initialize()\nself.dataset = self.db.initialize_dataset(\ndataset, self.config, start_index, max_items\n)\nself.task_object = self.db.initialize_task(self.config)\ncsv_file_name = (\noutput_name if output_name else f\"{dataset.replace('.csv','')}_labeled.csv\"\n)\ndataset_loader = DatasetLoader(dataset, self.config, max_items, start_index)\n# Initialize task run and check if it already exists\nself.task_run = self.db.get_task_run(self.task_object.id, self.dataset.id)\n# Resume/Delete the task if it already exists or create a new task run\nif self.task_run:\nlogger.info(\"Task run already exists.\")\nself.task_run = self.handle_existing_task_run(\nself.task_run, csv_file_name, gt_labels=dataset_loader.gt_labels\n)\nelse:\nself.task_run = self.db.create_task_run(\ncsv_file_name, self.task_object.id, self.dataset.id\n)\n# Get the seed examples from the dataset config\nseed_examples = self.config.few_shot_example_set()\n# If this dataset config is a string, read the corrresponding csv file\nif isinstance(seed_examples, str):\nseed_loader = DatasetLoader(seed_examples, self.config)\nseed_examples = seed_loader.inputs\n# Check explanations are present in data if explanation_column is passed in\nif (\nself.config.explanation_column()\nand len(seed_examples) &gt; 0\nand self.config.explanation_column() not in list(seed_examples[0].keys())\n):\nraise ValueError(\nf\"Explanation column {self.config.explanation_column()} not found in dataset.\\nMake sure that explanations were generated using labeler.generate_explanations(seed_file).\"\n)\nself.example_selector = ExampleSelectorFactory.initialize_selector(\nself.config, seed_examples, dataset_loader.dat.keys().tolist()\n)\nnum_failures = 0\ncurrent_index = self.task_run.current_index\ncost = 0.0\npostfix_dict = {}\nindices = range(current_index, len(dataset_loader.inputs), self.CHUNK_SIZE)\nfor current_index in track_with_stats(\nindices,\npostfix_dict,\ntotal=len(dataset_loader.inputs) - current_index,\nadvance=self.CHUNK_SIZE,\nconsole=console,\n):\nchunk = dataset_loader.inputs[\ncurrent_index : current_index + self.CHUNK_SIZE\n]\nfinal_prompts = []\nfor i, input_i in enumerate(chunk):\n# Fetch few-shot seed examples\nif self.example_selector:\nexamples = self.example_selector.select_examples(input_i)\nelse:\nexamples = []\n# Construct Prompt to pass to LLM\nfinal_prompt = self.task.construct_prompt(input_i, examples)\nfinal_prompts.append(final_prompt)\n# Get response from LLM\ntry:\nresponse, curr_cost = self.llm.label(final_prompts)\nexcept Exception as e:\n# TODO (dhruva): We need to handle this case carefully\n# When we erorr out, we will have less elements in the llm_labels\n# than the gt_labels array, with the 1:1 mapping not being\n# maintained either. We should either remove the elements we errored\n# out on from gt_labels or add None labels to the llm_labels.\nlogger.error(\n\"Error in generating response:\" + repr(e), \"Prompt: \", chunk\n)\nfor i in range(len(chunk)):\nannotation = LLMAnnotation(\nsuccessfully_labeled=False,\nlabel=self.task.NULL_LABEL_TOKEN,\nraw_response=\"\",\ncurr_sample=chunk[i],\nprompt=final_prompts[i],\nconfidence_score=0,\n)\nAnnotationModel.create_from_llm_annotation(\nself.db.session,\nannotation,\ncurrent_index + i,\nself.task_run.id,\n)\nnum_failures += len(chunk)\nresponse = None\nif response is not None:\nfor i in range(len(response.generations)):\nresponse_item = response.generations[i]\nannotations = []\nfor generation in response_item:\nif self.config.confidence():\nannotation = self.confidence.calculate(\nmodel_generation=self.task.parse_llm_response(\ngeneration, chunk[i], final_prompts[i]\n),\nprompt=final_prompts[i],\n)\nelse:\nannotation = self.task.parse_llm_response(\ngeneration, chunk[i], final_prompts[i]\n)\nannotations.append(annotation)\nfinal_annotation = self.majority_annotation(annotations)\nAnnotationModel.create_from_llm_annotation(\nself.db.session,\nfinal_annotation,\ncurrent_index + i,\nself.task_run.id,\n)\ncost += curr_cost\npostfix_dict[self.COST_KEY] = f\"{cost:.2f}\"\n# Evaluate the task every eval_every examples\nif (current_index + self.CHUNK_SIZE) % eval_every == 0:\ndb_result = AnnotationModel.get_annotations_by_task_run_id(\nself.db.session, self.task_run.id\n)\nllm_labels = [LLMAnnotation(**a.llm_annotation) for a in db_result]\nif dataset_loader.gt_labels:\neval_result = self.task.eval(\nllm_labels, dataset_loader.gt_labels[: len(llm_labels)]\n)\nfor m in eval_result:\nif not isinstance(m.value, list) or len(m.value) &lt; 1:\ncontinue\nelif isinstance(m.value[0], float):\npostfix_dict[m.name] = f\"{m.value[0]:.4f}\"\nelif isinstance(m.value[0], int):\npostfix_dict[m.name] = f\"{m.value[0]}\"\nelif len(m.value[0]) &gt; 0:\npostfix_dict[m.name] = f\"{m.value[0][0]:.4f}\"\n# Update task run state\nself.task_run = self.save_task_run_state(\ncurrent_index=current_index + len(chunk)\n)\ndb_result = AnnotationModel.get_annotations_by_task_run_id(\nself.db.session, self.task_run.id\n)\nllm_labels = [LLMAnnotation(**a.llm_annotation) for a in db_result]\neval_result = None\n# if true labels are provided, evaluate accuracy of predictions\nif dataset_loader.gt_labels:\neval_result = self.task.eval(\nllm_labels, dataset_loader.gt_labels[: len(llm_labels)]\n)\ntable = {}\n# TODO: serialize and write to file\nfor m in eval_result:\nif isinstance(m.value, list) and len(m.value) &gt; 0:\ntable[m.name] = m.value\nelse:\nprint(f\"Metric: {m.name}: {maybe_round(m.value)}\")\nprint(f\"Actual Cost: {maybe_round(cost)}\")\nprint_table(table, console=console, default_style=METRIC_TABLE_STYLE)\n# Write output to CSV\noutput_df = dataset_loader.dat.copy()\noutput_df[self.config.task_name() + \"_llm_labeled_successfully\"] = [\nl.successfully_labeled for l in llm_labels\n]\noutput_df[self.config.task_name() + \"_llm_label\"] = [\nl.label for l in llm_labels\n]\nif self.config.confidence():\noutput_df[\"llm_confidence\"] = [l.confidence_score for l in llm_labels]\n# Only save to csv if output_name is provided or dataset is a string\nif not output_name and isinstance(dataset, str):\noutput_name = (\ndataset.rsplit(\".\", 1)[0] + \"_labeled.\" + dataset.rsplit(\".\", 1)[1]\n)\nif output_name:\nif output_name.endswith(\".csv\"):\noutput_df.to_csv(\nstr(output_name),\nsep=self.config.delimiter(),\nheader=True,\nindex=False,\n)\nelif output_name.endswith(\".jsonl\"):\noutput_df.to_json(\nstr(output_name),\norient=\"records\",\nlines=True,\nforce_ascii=False,\n)\nelse:\nraise ValueError(f\"Unsupported output file format: {output_name}\")\npprint(f\"Total number of failures: {num_failures}\")\nreturn (\noutput_df[self.config.task_name() + \"_llm_label\"],\noutput_df,\neval_result,\n)\ndef plan(\nself,\ndataset: Union[str, pd.DataFrame],\nmax_items: int = None,\nstart_index: int = 0,\n) -&gt; None:\n\"\"\"Calculates and prints the cost of calling autolabel.run() on a given dataset\n        Args:\n            dataset: path to a CSV dataset\n        \"\"\"\ndataset_loader = DatasetLoader(dataset, self.config, max_items, start_index)\nprompt_list = []\ntotal_cost = 0\n# Get the seed examples from the dataset config\nseed_examples = self.config.few_shot_example_set()\n# If this dataset config is a string, read the corrresponding csv file\nif isinstance(seed_examples, str):\nseed_loader = DatasetLoader(seed_examples, self.config)\nseed_examples = seed_loader.inputs\n# Check explanations are present in data if explanation_column is passed in\nif (\nself.config.explanation_column()\nand len(seed_examples) &gt; 0\nand self.config.explanation_column() not in list(seed_examples[0].keys())\n):\nraise ValueError(\nf\"Explanation column {self.config.explanation_column()} not found in dataset.\\nMake sure that explanations were generated using labeler.generate_explanations(seed_file).\"\n)\nself.example_selector = ExampleSelectorFactory.initialize_selector(\nself.config, seed_examples, dataset_loader.dat.keys().tolist()\n)\ninput_limit = min(len(dataset_loader.inputs), 100)\nfor input_i in track(\ndataset_loader.inputs[:input_limit],\ndescription=\"Generating Prompts...\",\nconsole=console,\n):\n# TODO: Check if this needs to use the example selector\nif self.example_selector:\nexamples = self.example_selector.select_examples(input_i)\nelse:\nexamples = []\nfinal_prompt = self.task.construct_prompt(input_i, examples)\nprompt_list.append(final_prompt)\n# Calculate the number of tokens\ncurr_cost = self.llm.get_cost(prompt=final_prompt, label=\"\")\ntotal_cost += curr_cost\ntotal_cost = total_cost * (len(dataset_loader.inputs) / input_limit)\ntable = {\n\"Total Estimated Cost\": f\"${maybe_round(total_cost)}\",\n\"Number of Examples\": len(dataset_loader.inputs),\n\"Average cost per example\": f\"${maybe_round(total_cost / len(dataset_loader.inputs))}\",\n}\ntable = {\"parameter\": list(table.keys()), \"value\": list(table.values())}\nprint_table(table, show_header=False, console=console, styles=COST_TABLE_STYLES)\nconsole.rule(\"Prompt Example\")\nprint(f\"{prompt_list[0]}\")\nconsole.rule()\ndef handle_existing_task_run(\nself, task_run: TaskRun, csv_file_name: str, gt_labels: List[str] = None\n) -&gt; TaskRun:\n\"\"\"\n        Allows for continuing an existing labeling task. The user will be asked whether they wish to continue from where the run previously left off, or restart from the beginning.\n        Args:\n            task_run: TaskRun to retry\n            csv_file_name: path to the dataset we wish to label (only used if user chooses to restart the task)\n            gt_labels: If ground truth labels are provided, performance metrics will be displayed, such as label accuracy\n        \"\"\"\npprint(f\"There is an existing task with following details: {task_run}\")\ndb_result = AnnotationModel.get_annotations_by_task_run_id(\nself.db.session, task_run.id\n)\nllm_labels = [LLMAnnotation(**a.llm_annotation) for a in db_result]\nif gt_labels and len(llm_labels) &gt; 0:\npprint(\"Evaluating the existing task...\")\ngt_labels = gt_labels[: len(llm_labels)]\neval_result = self.task.eval(llm_labels, gt_labels)\ntable = {}\nfor m in eval_result:\nif isinstance(m.value, list) and len(m.value) &gt; 0:\ntable[m.name] = m.value\nelse:\nprint(f\"Metric: {m.name}: {m.value}\")\nprint_table(table, console=console, default_style=METRIC_TABLE_STYLE)\npprint(f\"{task_run.current_index} examples labeled so far.\")\nif len(llm_labels) &gt; 0:\nconsole.rule(\"Last Annotated Example\")\npprint(\"[bold blue]Prompt[/bold blue]: \", end=\"\")\nprint(llm_labels[-1].prompt)\npprint(\"[bold blue]Annotation[/bold blue]: \", end=\"\")\nprint(llm_labels[-1].label)\nconsole.rule()\nif not Confirm.ask(\"Do you want to resume the task?\"):\nTaskRunModel.delete_by_id(self.db.session, task_run.id)\npprint(\"Deleted the existing task and starting a new one...\")\ntask_run = self.db.create_task_run(\ncsv_file_name, self.task_object.id, self.dataset.id\n)\nreturn task_run\ndef save_task_run_state(\nself, current_index: int = None, status: TaskStatus = \"\", error: str = \"\"\n) -&gt; TaskRun:\n\"\"\"Saves the current state of the Task being performed\"\"\"\n# Save the current state of the task\nif error:\nself.task_run.error = error\nif status:\nself.task_run.status = status\nif current_index:\nself.task_run.current_index = current_index\nreturn TaskRunModel.update(self.db.session, self.task_run)\ndef majority_annotation(\nself, annotation_list: List[LLMAnnotation]\n) -&gt; LLMAnnotation:\nlabels = [a.label for a in annotation_list]\ncounts = {}\nfor ind, label in enumerate(labels):\n# Needed for named entity recognition which outputs lists instead of strings\nlabel = str(label)\nif label not in counts:\ncounts[label] = (1, ind)\nelse:\ncounts[label] = (counts[label][0] + 1, counts[label][1])\nmax_label = max(counts, key=lambda x: counts[x][0])\nreturn annotation_list[counts[max_label][1]]\ndef generate_explanations(\nself,\nseed_examples: Union[str, List[Dict]],\n) -&gt; List[Dict]:\n\"\"\"Use LLM to generate explanations for why examples are labeled the way that they are.\"\"\"\nout_file = None\nif isinstance(seed_examples, str):\nout_file = seed_examples\n_, seed_examples, _ = DatasetLoader.read_file(seed_examples, self.config)\nexplanation_column = self.config.explanation_column()\nif not explanation_column:\nraise ValueError(\n\"The explanation column needs to be specified in the dataset config.\"\n)\nfor seed_example in track(\nseed_examples, description=\"Generating explanations\", console=console\n):\nexplanation_prompt = self.task.get_explanation_prompt(seed_example)\nexplanation, _ = self.llm.label([explanation_prompt])\nexplanation = explanation.generations[0][0].text\nseed_example[\"explanation\"] = str(explanation) if explanation else \"\"\nif out_file:\ndf = pd.DataFrame.from_records(seed_examples)\ndf.to_csv(out_file, index=False)\nreturn seed_examples\ndef clear_cache(self):\nif self.cache:\nself.cache.clear()\nelse:\nlogger.error(\"No cache to clear\")\n</code></pre>"},{"location":"reference/labeler/#src.autolabel.labeler.LabelingAgent.generate_explanations","title":"<code>generate_explanations(seed_examples)</code>","text":"<p>Use LLM to generate explanations for why examples are labeled the way that they are.</p> Source code in <code>src/autolabel/labeler.py</code> <pre><code>def generate_explanations(\nself,\nseed_examples: Union[str, List[Dict]],\n) -&gt; List[Dict]:\n\"\"\"Use LLM to generate explanations for why examples are labeled the way that they are.\"\"\"\nout_file = None\nif isinstance(seed_examples, str):\nout_file = seed_examples\n_, seed_examples, _ = DatasetLoader.read_file(seed_examples, self.config)\nexplanation_column = self.config.explanation_column()\nif not explanation_column:\nraise ValueError(\n\"The explanation column needs to be specified in the dataset config.\"\n)\nfor seed_example in track(\nseed_examples, description=\"Generating explanations\", console=console\n):\nexplanation_prompt = self.task.get_explanation_prompt(seed_example)\nexplanation, _ = self.llm.label([explanation_prompt])\nexplanation = explanation.generations[0][0].text\nseed_example[\"explanation\"] = str(explanation) if explanation else \"\"\nif out_file:\ndf = pd.DataFrame.from_records(seed_examples)\ndf.to_csv(out_file, index=False)\nreturn seed_examples\n</code></pre>"},{"location":"reference/labeler/#src.autolabel.labeler.LabelingAgent.handle_existing_task_run","title":"<code>handle_existing_task_run(task_run, csv_file_name, gt_labels=None)</code>","text":"<p>Allows for continuing an existing labeling task. The user will be asked whether they wish to continue from where the run previously left off, or restart from the beginning.</p> <p>Parameters:</p> Name Type Description Default <code>task_run</code> <code>TaskRun</code> <p>TaskRun to retry</p> required <code>csv_file_name</code> <code>str</code> <p>path to the dataset we wish to label (only used if user chooses to restart the task)</p> required <code>gt_labels</code> <code>List[str]</code> <p>If ground truth labels are provided, performance metrics will be displayed, such as label accuracy</p> <code>None</code> Source code in <code>src/autolabel/labeler.py</code> <pre><code>def handle_existing_task_run(\nself, task_run: TaskRun, csv_file_name: str, gt_labels: List[str] = None\n) -&gt; TaskRun:\n\"\"\"\n    Allows for continuing an existing labeling task. The user will be asked whether they wish to continue from where the run previously left off, or restart from the beginning.\n    Args:\n        task_run: TaskRun to retry\n        csv_file_name: path to the dataset we wish to label (only used if user chooses to restart the task)\n        gt_labels: If ground truth labels are provided, performance metrics will be displayed, such as label accuracy\n    \"\"\"\npprint(f\"There is an existing task with following details: {task_run}\")\ndb_result = AnnotationModel.get_annotations_by_task_run_id(\nself.db.session, task_run.id\n)\nllm_labels = [LLMAnnotation(**a.llm_annotation) for a in db_result]\nif gt_labels and len(llm_labels) &gt; 0:\npprint(\"Evaluating the existing task...\")\ngt_labels = gt_labels[: len(llm_labels)]\neval_result = self.task.eval(llm_labels, gt_labels)\ntable = {}\nfor m in eval_result:\nif isinstance(m.value, list) and len(m.value) &gt; 0:\ntable[m.name] = m.value\nelse:\nprint(f\"Metric: {m.name}: {m.value}\")\nprint_table(table, console=console, default_style=METRIC_TABLE_STYLE)\npprint(f\"{task_run.current_index} examples labeled so far.\")\nif len(llm_labels) &gt; 0:\nconsole.rule(\"Last Annotated Example\")\npprint(\"[bold blue]Prompt[/bold blue]: \", end=\"\")\nprint(llm_labels[-1].prompt)\npprint(\"[bold blue]Annotation[/bold blue]: \", end=\"\")\nprint(llm_labels[-1].label)\nconsole.rule()\nif not Confirm.ask(\"Do you want to resume the task?\"):\nTaskRunModel.delete_by_id(self.db.session, task_run.id)\npprint(\"Deleted the existing task and starting a new one...\")\ntask_run = self.db.create_task_run(\ncsv_file_name, self.task_object.id, self.dataset.id\n)\nreturn task_run\n</code></pre>"},{"location":"reference/labeler/#src.autolabel.labeler.LabelingAgent.plan","title":"<code>plan(dataset, max_items=None, start_index=0)</code>","text":"<p>Calculates and prints the cost of calling autolabel.run() on a given dataset</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Union[str, pd.DataFrame]</code> <p>path to a CSV dataset</p> required Source code in <code>src/autolabel/labeler.py</code> <pre><code>def plan(\nself,\ndataset: Union[str, pd.DataFrame],\nmax_items: int = None,\nstart_index: int = 0,\n) -&gt; None:\n\"\"\"Calculates and prints the cost of calling autolabel.run() on a given dataset\n    Args:\n        dataset: path to a CSV dataset\n    \"\"\"\ndataset_loader = DatasetLoader(dataset, self.config, max_items, start_index)\nprompt_list = []\ntotal_cost = 0\n# Get the seed examples from the dataset config\nseed_examples = self.config.few_shot_example_set()\n# If this dataset config is a string, read the corrresponding csv file\nif isinstance(seed_examples, str):\nseed_loader = DatasetLoader(seed_examples, self.config)\nseed_examples = seed_loader.inputs\n# Check explanations are present in data if explanation_column is passed in\nif (\nself.config.explanation_column()\nand len(seed_examples) &gt; 0\nand self.config.explanation_column() not in list(seed_examples[0].keys())\n):\nraise ValueError(\nf\"Explanation column {self.config.explanation_column()} not found in dataset.\\nMake sure that explanations were generated using labeler.generate_explanations(seed_file).\"\n)\nself.example_selector = ExampleSelectorFactory.initialize_selector(\nself.config, seed_examples, dataset_loader.dat.keys().tolist()\n)\ninput_limit = min(len(dataset_loader.inputs), 100)\nfor input_i in track(\ndataset_loader.inputs[:input_limit],\ndescription=\"Generating Prompts...\",\nconsole=console,\n):\n# TODO: Check if this needs to use the example selector\nif self.example_selector:\nexamples = self.example_selector.select_examples(input_i)\nelse:\nexamples = []\nfinal_prompt = self.task.construct_prompt(input_i, examples)\nprompt_list.append(final_prompt)\n# Calculate the number of tokens\ncurr_cost = self.llm.get_cost(prompt=final_prompt, label=\"\")\ntotal_cost += curr_cost\ntotal_cost = total_cost * (len(dataset_loader.inputs) / input_limit)\ntable = {\n\"Total Estimated Cost\": f\"${maybe_round(total_cost)}\",\n\"Number of Examples\": len(dataset_loader.inputs),\n\"Average cost per example\": f\"${maybe_round(total_cost / len(dataset_loader.inputs))}\",\n}\ntable = {\"parameter\": list(table.keys()), \"value\": list(table.values())}\nprint_table(table, show_header=False, console=console, styles=COST_TABLE_STYLES)\nconsole.rule(\"Prompt Example\")\nprint(f\"{prompt_list[0]}\")\nconsole.rule()\n</code></pre>"},{"location":"reference/labeler/#src.autolabel.labeler.LabelingAgent.run","title":"<code>run(dataset, max_items=None, output_name=None, start_index=0, eval_every=50)</code>","text":"<p>Labels data in a given dataset. Output written to new CSV file.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Union[str, pd.DataFrame]</code> <p>path to CSV dataset to be annotated</p> required <code>max_items</code> <code>Optional[int]</code> <p>maximum items in dataset to be annotated</p> <code>None</code> <code>output_name</code> <code>Optional[str]</code> <p>custom name of output CSV file</p> <code>None</code> <code>start_index</code> <code>Optional[int]</code> <p>skips annotating [0, start_index)</p> <code>0</code> Source code in <code>src/autolabel/labeler.py</code> <pre><code>def run(\nself,\ndataset: Union[str, pd.DataFrame],\nmax_items: Optional[int] = None,\noutput_name: Optional[str] = None,\nstart_index: Optional[int] = 0,\neval_every: Optional[int] = 50,\n) -&gt; Tuple[pd.Series, pd.DataFrame, List[MetricResult]]:\n\"\"\"Labels data in a given dataset. Output written to new CSV file.\n    Args:\n        dataset: path to CSV dataset to be annotated\n        max_items: maximum items in dataset to be annotated\n        output_name: custom name of output CSV file\n        start_index: skips annotating [0, start_index)\n    \"\"\"\nself.db.initialize()\nself.dataset = self.db.initialize_dataset(\ndataset, self.config, start_index, max_items\n)\nself.task_object = self.db.initialize_task(self.config)\ncsv_file_name = (\noutput_name if output_name else f\"{dataset.replace('.csv','')}_labeled.csv\"\n)\ndataset_loader = DatasetLoader(dataset, self.config, max_items, start_index)\n# Initialize task run and check if it already exists\nself.task_run = self.db.get_task_run(self.task_object.id, self.dataset.id)\n# Resume/Delete the task if it already exists or create a new task run\nif self.task_run:\nlogger.info(\"Task run already exists.\")\nself.task_run = self.handle_existing_task_run(\nself.task_run, csv_file_name, gt_labels=dataset_loader.gt_labels\n)\nelse:\nself.task_run = self.db.create_task_run(\ncsv_file_name, self.task_object.id, self.dataset.id\n)\n# Get the seed examples from the dataset config\nseed_examples = self.config.few_shot_example_set()\n# If this dataset config is a string, read the corrresponding csv file\nif isinstance(seed_examples, str):\nseed_loader = DatasetLoader(seed_examples, self.config)\nseed_examples = seed_loader.inputs\n# Check explanations are present in data if explanation_column is passed in\nif (\nself.config.explanation_column()\nand len(seed_examples) &gt; 0\nand self.config.explanation_column() not in list(seed_examples[0].keys())\n):\nraise ValueError(\nf\"Explanation column {self.config.explanation_column()} not found in dataset.\\nMake sure that explanations were generated using labeler.generate_explanations(seed_file).\"\n)\nself.example_selector = ExampleSelectorFactory.initialize_selector(\nself.config, seed_examples, dataset_loader.dat.keys().tolist()\n)\nnum_failures = 0\ncurrent_index = self.task_run.current_index\ncost = 0.0\npostfix_dict = {}\nindices = range(current_index, len(dataset_loader.inputs), self.CHUNK_SIZE)\nfor current_index in track_with_stats(\nindices,\npostfix_dict,\ntotal=len(dataset_loader.inputs) - current_index,\nadvance=self.CHUNK_SIZE,\nconsole=console,\n):\nchunk = dataset_loader.inputs[\ncurrent_index : current_index + self.CHUNK_SIZE\n]\nfinal_prompts = []\nfor i, input_i in enumerate(chunk):\n# Fetch few-shot seed examples\nif self.example_selector:\nexamples = self.example_selector.select_examples(input_i)\nelse:\nexamples = []\n# Construct Prompt to pass to LLM\nfinal_prompt = self.task.construct_prompt(input_i, examples)\nfinal_prompts.append(final_prompt)\n# Get response from LLM\ntry:\nresponse, curr_cost = self.llm.label(final_prompts)\nexcept Exception as e:\n# TODO (dhruva): We need to handle this case carefully\n# When we erorr out, we will have less elements in the llm_labels\n# than the gt_labels array, with the 1:1 mapping not being\n# maintained either. We should either remove the elements we errored\n# out on from gt_labels or add None labels to the llm_labels.\nlogger.error(\n\"Error in generating response:\" + repr(e), \"Prompt: \", chunk\n)\nfor i in range(len(chunk)):\nannotation = LLMAnnotation(\nsuccessfully_labeled=False,\nlabel=self.task.NULL_LABEL_TOKEN,\nraw_response=\"\",\ncurr_sample=chunk[i],\nprompt=final_prompts[i],\nconfidence_score=0,\n)\nAnnotationModel.create_from_llm_annotation(\nself.db.session,\nannotation,\ncurrent_index + i,\nself.task_run.id,\n)\nnum_failures += len(chunk)\nresponse = None\nif response is not None:\nfor i in range(len(response.generations)):\nresponse_item = response.generations[i]\nannotations = []\nfor generation in response_item:\nif self.config.confidence():\nannotation = self.confidence.calculate(\nmodel_generation=self.task.parse_llm_response(\ngeneration, chunk[i], final_prompts[i]\n),\nprompt=final_prompts[i],\n)\nelse:\nannotation = self.task.parse_llm_response(\ngeneration, chunk[i], final_prompts[i]\n)\nannotations.append(annotation)\nfinal_annotation = self.majority_annotation(annotations)\nAnnotationModel.create_from_llm_annotation(\nself.db.session,\nfinal_annotation,\ncurrent_index + i,\nself.task_run.id,\n)\ncost += curr_cost\npostfix_dict[self.COST_KEY] = f\"{cost:.2f}\"\n# Evaluate the task every eval_every examples\nif (current_index + self.CHUNK_SIZE) % eval_every == 0:\ndb_result = AnnotationModel.get_annotations_by_task_run_id(\nself.db.session, self.task_run.id\n)\nllm_labels = [LLMAnnotation(**a.llm_annotation) for a in db_result]\nif dataset_loader.gt_labels:\neval_result = self.task.eval(\nllm_labels, dataset_loader.gt_labels[: len(llm_labels)]\n)\nfor m in eval_result:\nif not isinstance(m.value, list) or len(m.value) &lt; 1:\ncontinue\nelif isinstance(m.value[0], float):\npostfix_dict[m.name] = f\"{m.value[0]:.4f}\"\nelif isinstance(m.value[0], int):\npostfix_dict[m.name] = f\"{m.value[0]}\"\nelif len(m.value[0]) &gt; 0:\npostfix_dict[m.name] = f\"{m.value[0][0]:.4f}\"\n# Update task run state\nself.task_run = self.save_task_run_state(\ncurrent_index=current_index + len(chunk)\n)\ndb_result = AnnotationModel.get_annotations_by_task_run_id(\nself.db.session, self.task_run.id\n)\nllm_labels = [LLMAnnotation(**a.llm_annotation) for a in db_result]\neval_result = None\n# if true labels are provided, evaluate accuracy of predictions\nif dataset_loader.gt_labels:\neval_result = self.task.eval(\nllm_labels, dataset_loader.gt_labels[: len(llm_labels)]\n)\ntable = {}\n# TODO: serialize and write to file\nfor m in eval_result:\nif isinstance(m.value, list) and len(m.value) &gt; 0:\ntable[m.name] = m.value\nelse:\nprint(f\"Metric: {m.name}: {maybe_round(m.value)}\")\nprint(f\"Actual Cost: {maybe_round(cost)}\")\nprint_table(table, console=console, default_style=METRIC_TABLE_STYLE)\n# Write output to CSV\noutput_df = dataset_loader.dat.copy()\noutput_df[self.config.task_name() + \"_llm_labeled_successfully\"] = [\nl.successfully_labeled for l in llm_labels\n]\noutput_df[self.config.task_name() + \"_llm_label\"] = [\nl.label for l in llm_labels\n]\nif self.config.confidence():\noutput_df[\"llm_confidence\"] = [l.confidence_score for l in llm_labels]\n# Only save to csv if output_name is provided or dataset is a string\nif not output_name and isinstance(dataset, str):\noutput_name = (\ndataset.rsplit(\".\", 1)[0] + \"_labeled.\" + dataset.rsplit(\".\", 1)[1]\n)\nif output_name:\nif output_name.endswith(\".csv\"):\noutput_df.to_csv(\nstr(output_name),\nsep=self.config.delimiter(),\nheader=True,\nindex=False,\n)\nelif output_name.endswith(\".jsonl\"):\noutput_df.to_json(\nstr(output_name),\norient=\"records\",\nlines=True,\nforce_ascii=False,\n)\nelse:\nraise ValueError(f\"Unsupported output file format: {output_name}\")\npprint(f\"Total number of failures: {num_failures}\")\nreturn (\noutput_df[self.config.task_name() + \"_llm_label\"],\noutput_df,\neval_result,\n)\n</code></pre>"},{"location":"reference/labeler/#src.autolabel.labeler.LabelingAgent.save_task_run_state","title":"<code>save_task_run_state(current_index=None, status='', error='')</code>","text":"<p>Saves the current state of the Task being performed</p> Source code in <code>src/autolabel/labeler.py</code> <pre><code>def save_task_run_state(\nself, current_index: int = None, status: TaskStatus = \"\", error: str = \"\"\n) -&gt; TaskRun:\n\"\"\"Saves the current state of the Task being performed\"\"\"\n# Save the current state of the task\nif error:\nself.task_run.error = error\nif status:\nself.task_run.status = status\nif current_index:\nself.task_run.current_index = current_index\nreturn TaskRunModel.update(self.db.session, self.task_run)\n</code></pre>"},{"location":"reference/models/","title":"Models","text":""},{"location":"reference/models/#src.autolabel.models.base.BaseModel","title":"<code>BaseModel</code>","text":"<p>         Bases: <code>ABC</code></p> Source code in <code>src/autolabel/models/base.py</code> <pre><code>class BaseModel(ABC):\ndef __init__(self, config: AutolabelConfig, cache: BaseCache) -&gt; None:\nself.config = config\nself.cache = cache\nself.model_params = config.model_params()\n# Specific classes that implement this interface should run initialization steps here\n# E.g. initializing the LLM model with required parameters from ModelConfig\ndef label(self, prompts: List[str]) -&gt; Tuple[LLMResult, float]:\n\"\"\"Label a list of prompts.\"\"\"\nexisting_prompts = {}\nmissing_prompt_idxs = list(range(len(prompts)))\nmissing_prompts = prompts\nllm_output = {}\ncost = 0.0\nif self.cache:\n(\nexisting_prompts,\nmissing_prompt_idxs,\nmissing_prompts,\n) = self.get_cached_prompts(prompts)\n# label missing prompts\nif len(missing_prompts) &gt; 0:\nnew_results = self._label(missing_prompts)\nfor ind, prompt in enumerate(missing_prompts):\ncost += self.get_cost(\nprompt, label=new_results.generations[ind][0].text\n)\n# Set the existing prompts to the new results\nfor i, result in zip(missing_prompt_idxs, new_results.generations):\nexisting_prompts[i] = result\nif self.cache:\nself.update_cache(missing_prompt_idxs, new_results, prompts)\nllm_output = new_results.llm_output\ngenerations = [existing_prompts[i] for i in range(len(prompts))]\nreturn LLMResult(generations=generations, llm_output=llm_output), cost\n@abstractmethod\ndef _label(self, prompts: List[str]) -&gt; LLMResult:\n# TODO: change return type to do parsing in the Model class\npass\n@abstractmethod\ndef get_cost(self, prompt: str, label: Optional[str] = \"\") -&gt; float:\npass\ndef get_cached_prompts(self, prompts: List[str]) -&gt; Optional[str]:\n\"\"\"Get prompts that are already cached.\"\"\"\nmodel_params_string = str(\nsorted([(k, v) for k, v in self.model_params.items()])\n)\nmissing_prompts = []\nmissing_prompt_idxs = []\nexisting_prompts = {}\nfor i, prompt in enumerate(prompts):\ncache_entry = CacheEntry(\nprompt=prompt,\nmodel_name=self.model_name,\nmodel_params=model_params_string,\n)\ncache_val = self.cache.lookup(cache_entry)\nif cache_val:\nexisting_prompts[i] = cache_val\nelse:\nmissing_prompts.append(prompt)\nmissing_prompt_idxs.append(i)\nreturn (\nexisting_prompts,\nmissing_prompt_idxs,\nmissing_prompts,\n)\ndef update_cache(self, missing_prompt_idxs, new_results, prompts):\n\"\"\"Update the cache with new results.\"\"\"\nmodel_params_string = str(\nsorted([(k, v) for k, v in self.model_params.items()])\n)\nfor i, result in zip(missing_prompt_idxs, new_results.generations):\n# If the result is empty, don't cache it\n# This result was likely produced due to an error\nif result[0].text == \"\":\ncontinue\ncache_entry = CacheEntry(\nprompt=prompts[i],\nmodel_name=self.model_name,\nmodel_params=model_params_string,\ngenerations=result,\n)\nself.cache.update(cache_entry)\n@abstractmethod\ndef returns_token_probs(self) -&gt; bool:\n\"\"\"Whether the LLM supports returning logprobs of generated tokens\n        Returns:\n            bool: whether the LLM returns supports returning logprobs of generated tokens\n        \"\"\"\npass\n</code></pre>"},{"location":"reference/models/#src.autolabel.models.base.BaseModel.get_cached_prompts","title":"<code>get_cached_prompts(prompts)</code>","text":"<p>Get prompts that are already cached.</p> Source code in <code>src/autolabel/models/base.py</code> <pre><code>def get_cached_prompts(self, prompts: List[str]) -&gt; Optional[str]:\n\"\"\"Get prompts that are already cached.\"\"\"\nmodel_params_string = str(\nsorted([(k, v) for k, v in self.model_params.items()])\n)\nmissing_prompts = []\nmissing_prompt_idxs = []\nexisting_prompts = {}\nfor i, prompt in enumerate(prompts):\ncache_entry = CacheEntry(\nprompt=prompt,\nmodel_name=self.model_name,\nmodel_params=model_params_string,\n)\ncache_val = self.cache.lookup(cache_entry)\nif cache_val:\nexisting_prompts[i] = cache_val\nelse:\nmissing_prompts.append(prompt)\nmissing_prompt_idxs.append(i)\nreturn (\nexisting_prompts,\nmissing_prompt_idxs,\nmissing_prompts,\n)\n</code></pre>"},{"location":"reference/models/#src.autolabel.models.base.BaseModel.label","title":"<code>label(prompts)</code>","text":"<p>Label a list of prompts.</p> Source code in <code>src/autolabel/models/base.py</code> <pre><code>def label(self, prompts: List[str]) -&gt; Tuple[LLMResult, float]:\n\"\"\"Label a list of prompts.\"\"\"\nexisting_prompts = {}\nmissing_prompt_idxs = list(range(len(prompts)))\nmissing_prompts = prompts\nllm_output = {}\ncost = 0.0\nif self.cache:\n(\nexisting_prompts,\nmissing_prompt_idxs,\nmissing_prompts,\n) = self.get_cached_prompts(prompts)\n# label missing prompts\nif len(missing_prompts) &gt; 0:\nnew_results = self._label(missing_prompts)\nfor ind, prompt in enumerate(missing_prompts):\ncost += self.get_cost(\nprompt, label=new_results.generations[ind][0].text\n)\n# Set the existing prompts to the new results\nfor i, result in zip(missing_prompt_idxs, new_results.generations):\nexisting_prompts[i] = result\nif self.cache:\nself.update_cache(missing_prompt_idxs, new_results, prompts)\nllm_output = new_results.llm_output\ngenerations = [existing_prompts[i] for i in range(len(prompts))]\nreturn LLMResult(generations=generations, llm_output=llm_output), cost\n</code></pre>"},{"location":"reference/models/#src.autolabel.models.base.BaseModel.returns_token_probs","title":"<code>returns_token_probs()</code>  <code>abstractmethod</code>","text":"<p>Whether the LLM supports returning logprobs of generated tokens</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>whether the LLM returns supports returning logprobs of generated tokens</p> Source code in <code>src/autolabel/models/base.py</code> <pre><code>@abstractmethod\ndef returns_token_probs(self) -&gt; bool:\n\"\"\"Whether the LLM supports returning logprobs of generated tokens\n    Returns:\n        bool: whether the LLM returns supports returning logprobs of generated tokens\n    \"\"\"\npass\n</code></pre>"},{"location":"reference/models/#src.autolabel.models.base.BaseModel.update_cache","title":"<code>update_cache(missing_prompt_idxs, new_results, prompts)</code>","text":"<p>Update the cache with new results.</p> Source code in <code>src/autolabel/models/base.py</code> <pre><code>def update_cache(self, missing_prompt_idxs, new_results, prompts):\n\"\"\"Update the cache with new results.\"\"\"\nmodel_params_string = str(\nsorted([(k, v) for k, v in self.model_params.items()])\n)\nfor i, result in zip(missing_prompt_idxs, new_results.generations):\n# If the result is empty, don't cache it\n# This result was likely produced due to an error\nif result[0].text == \"\":\ncontinue\ncache_entry = CacheEntry(\nprompt=prompts[i],\nmodel_name=self.model_name,\nmodel_params=model_params_string,\ngenerations=result,\n)\nself.cache.update(cache_entry)\n</code></pre>"},{"location":"reference/models/#src.autolabel.models.ModelFactory","title":"<code>ModelFactory</code>","text":"<p>The ModelFactory class is used to create a BaseModel object from the given AutoLabelConfig configuration.</p> Source code in <code>src/autolabel/models/__init__.py</code> <pre><code>class ModelFactory:\n\"\"\"The ModelFactory class is used to create a BaseModel object from the given AutoLabelConfig configuration.\"\"\"\n@staticmethod\ndef from_config(config: AutolabelConfig, cache: BaseCache = None) -&gt; BaseModel:\n\"\"\"\n        Returns a BaseModel object configured with the settings found in the provided AutolabelConfig.\n        Args:\n            config: AutolabelConfig object containing project settings\n            cache: cache allows for saving results in between labeling runs for future use\n        Returns:\n            model: a fully configured BaseModel object\n        \"\"\"\nmodel_provider = ModelProvider(config.provider())\ntry:\nif model_provider == ModelProvider.OPENAI:\nfrom .openai import OpenAILLM\nmodel_cls = OpenAILLM\nelif model_provider == ModelProvider.ANTHROPIC:\nfrom .anthropic import AnthropicLLM\nmodel_cls = AnthropicLLM\nelif model_provider == ModelProvider.HUGGINGFACE_PIPELINE:\nfrom .hf_pipeline import HFPipelineLLM\nmodel_cls = HFPipelineLLM\nelif model_provider == ModelProvider.REFUEL:\nfrom .refuel import RefuelLLM\nmodel_cls = RefuelLLM\nelif model_provider == ModelProvider.GOOGLE:\nfrom .palm import PaLMLLM\nmodel_cls = PaLMLLM\nelse:\nraise ValueError\nexcept ValueError as e:\nlogger.error(\nf\"{config.provider()} is not in the list of supported providers: \\\n{list(ModelProvider.__members__.keys())}\"\n)\nreturn None\nreturn model_cls(config, cache)\n</code></pre>"},{"location":"reference/models/#src.autolabel.models.ModelFactory.from_config","title":"<code>from_config(config, cache=None)</code>  <code>staticmethod</code>","text":"<p>Returns a BaseModel object configured with the settings found in the provided AutolabelConfig.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>AutolabelConfig</code> <p>AutolabelConfig object containing project settings</p> required <code>cache</code> <code>BaseCache</code> <p>cache allows for saving results in between labeling runs for future use</p> <code>None</code> <p>Returns:</p> Name Type Description <code>model</code> <code>BaseModel</code> <p>a fully configured BaseModel object</p> Source code in <code>src/autolabel/models/__init__.py</code> <pre><code>@staticmethod\ndef from_config(config: AutolabelConfig, cache: BaseCache = None) -&gt; BaseModel:\n\"\"\"\n    Returns a BaseModel object configured with the settings found in the provided AutolabelConfig.\n    Args:\n        config: AutolabelConfig object containing project settings\n        cache: cache allows for saving results in between labeling runs for future use\n    Returns:\n        model: a fully configured BaseModel object\n    \"\"\"\nmodel_provider = ModelProvider(config.provider())\ntry:\nif model_provider == ModelProvider.OPENAI:\nfrom .openai import OpenAILLM\nmodel_cls = OpenAILLM\nelif model_provider == ModelProvider.ANTHROPIC:\nfrom .anthropic import AnthropicLLM\nmodel_cls = AnthropicLLM\nelif model_provider == ModelProvider.HUGGINGFACE_PIPELINE:\nfrom .hf_pipeline import HFPipelineLLM\nmodel_cls = HFPipelineLLM\nelif model_provider == ModelProvider.REFUEL:\nfrom .refuel import RefuelLLM\nmodel_cls = RefuelLLM\nelif model_provider == ModelProvider.GOOGLE:\nfrom .palm import PaLMLLM\nmodel_cls = PaLMLLM\nelse:\nraise ValueError\nexcept ValueError as e:\nlogger.error(\nf\"{config.provider()} is not in the list of supported providers: \\\n{list(ModelProvider.__members__.keys())}\"\n)\nreturn None\nreturn model_cls(config, cache)\n</code></pre>"},{"location":"reference/models/#src.autolabel.models.anthropic.AnthropicLLM","title":"<code>AnthropicLLM</code>","text":"<p>         Bases: <code>BaseModel</code></p> Source code in <code>src/autolabel/models/anthropic.py</code> <pre><code>class AnthropicLLM(BaseModel):\nDEFAULT_MODEL = \"claude-instant-v1\"\nDEFAULT_PARAMS = {\n\"max_tokens_to_sample\": 1000,\n\"temperature\": 0.0,\n}\n# Reference: https://cdn2.assets-servd.host/anthropic-website/production/images/apr-pricing-tokens.pdf\nCOST_PER_PROMPT_TOKEN = {\n# $11.02 per million tokens\n\"claude-v1\": (11.02 / 1000000),\n\"claude-instant-v1\": (1.63 / 1000000),\n}\nCOST_PER_COMPLETION_TOKEN = {\n# $32.68 per million tokens\n\"claude-v1\": (32.68 / 1000000),\n\"claude-instant-v1\": (5.51 / 1000000),\n}\ndef __init__(self, config: AutolabelConfig, cache: BaseCache = None) -&gt; None:\nsuper().__init__(config, cache)\n# populate model name\nself.model_name = config.model_name() or self.DEFAULT_MODEL\n# populate model params\nmodel_params = config.model_params()\nself.model_params = {**self.DEFAULT_PARAMS, **model_params}\n# initialize LLM\nself.llm = ChatAnthropic(model=self.model_name, **self.model_params)\ndef _label(self, prompts: List[str]) -&gt; LLMResult:\nprompts = [[HumanMessage(content=prompt)] for prompt in prompts]\ntry:\nresponse = self.llm.generate(prompts)\nreturn response\nexcept Exception as e:\nprint(f\"Error generating from LLM: {e}, returning empty result\")\ngenerations = [[Generation(text=\"\")] for _ in prompts]\nreturn LLMResult(generations=generations)\ndef get_cost(self, prompt: str, label: Optional[str] = \"\") -&gt; float:\nnum_prompt_toks = tokenizer.count_tokens(prompt)\nif label:\nnum_label_toks = tokenizer.count_tokens(label)\nelse:\n# get an upper bound\nnum_label_toks = self.model_params[\"max_tokens_to_sample\"]\ncost_per_prompt_token = self.COST_PER_PROMPT_TOKEN[self.model_name]\ncost_per_completion_token = self.COST_PER_COMPLETION_TOKEN[self.model_name]\nreturn (num_prompt_toks * cost_per_prompt_token) + (\nnum_label_toks * cost_per_completion_token\n)\ndef returns_token_probs(self) -&gt; bool:\nreturn False\n</code></pre>"},{"location":"reference/models/#src.autolabel.models.hf_pipeline.HFPipelineLLM","title":"<code>HFPipelineLLM</code>","text":"<p>         Bases: <code>BaseModel</code></p> Source code in <code>src/autolabel/models/hf_pipeline.py</code> <pre><code>class HFPipelineLLM(BaseModel):\nDEFAULT_MODEL = \"google/flan-t5-xxl\"\nDEFAULT_PARAMS = {\"max_new_tokens\": 1000, \"temperature\": 0.0, \"quantize\": 8}\ndef __init__(self, config: AutolabelConfig, cache: BaseCache = None) -&gt; None:\ntry:\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer, pipeline\nexcept ImportError:\nraise ValueError(\n\"Could not import transformers python package. \"\n\"Please it install it with `pip install transformers`.\"\n)\ntry:\nimport torch\nexcept ImportError:\nraise ValueError(\n\"Could not import torch package. \"\n\"Please it install it with `pip install torch`.\"\n)\nsuper().__init__(config, cache)\n# populate model name\nself.model_name = config.model_name() or self.DEFAULT_MODEL\n# populate model params\nmodel_params = config.model_params()\nself.model_params = {**self.DEFAULT_PARAMS, **model_params}\n# initialize HF pipeline\ntokenizer = AutoTokenizer.from_pretrained(self.model_name)\nquantize_bits = self.model_params[\"quantize\"]\nif quantize_bits == 8:\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\nself.model_name, load_in_8bit=True, device_map=\"auto\"\n)\nelif quantize_bits == \"16\":\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\nself.model_name, torch_dtype=torch.float16, device_map=\"auto\"\n)\nelse:\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\nself.model_name, device_map=\"auto\"\n)\nmodel_kwargs = dict(self.model_params)  # make a copy of the model params\nmodel_kwargs.pop(\"quantize\", None)  # remove quantize from the model params\npipe = pipeline(\n\"text2text-generation\",\nmodel=model,\ntokenizer=tokenizer,\n**model_kwargs,\n)\n# initialize LLM\nself.llm = HuggingFacePipeline(pipeline=pipe, model_kwargs=model_kwargs)\ndef _label(self, prompts: List[str]) -&gt; LLMResult:\ntry:\nreturn self.llm.generate(prompts)\nexcept Exception as e:\nprint(f\"Error generating from LLM: {e}, returning empty result\")\ngenerations = [[Generation(text=\"\")] for _ in prompts]\nreturn LLMResult(generations=generations)\ndef get_cost(self, prompt: str, label: Optional[str] = \"\") -&gt; float:\n# Model inference for this model is being run locally\n# Revisit this in the future when we support HF inference endpoints\nreturn 0.0\ndef returns_token_probs(self) -&gt; bool:\nreturn False\n</code></pre>"},{"location":"reference/models/#src.autolabel.models.openai.OpenAILLM","title":"<code>OpenAILLM</code>","text":"<p>         Bases: <code>BaseModel</code></p> Source code in <code>src/autolabel/models/openai.py</code> <pre><code>class OpenAILLM(BaseModel):\nCHAT_ENGINE_MODELS = [\n\"gpt-3.5-turbo\",\n\"gpt-3.5-turbo-0613\",\n\"gpt-3.5-turbo-16k\",\n\"gpt-3.5-turbo-16k-0613\",\n\"gpt-4\",\n\"gpt-4-0613\",\n\"gpt-4-32k\",\n\"gpt-4-32k-0613\",\n]\nMODELS_WITH_TOKEN_PROBS = [\"text-curie-001\", \"text-davinci-003\"]\n# Default parameters for OpenAILLM\nDEFAULT_MODEL = \"gpt-3.5-turbo\"\nDEFAULT_PARAMS_COMPLETION_ENGINE = {\n\"max_tokens\": 1000,\n\"temperature\": 0.0,\n\"model_kwargs\": {\"logprobs\": 1},\n}\nDEFAULT_PARAMS_CHAT_ENGINE = {\n\"max_tokens\": 1000,\n\"temperature\": 0.0,\n}\n# Reference: https://openai.com/pricing\nCOST_PER_PROMPT_TOKEN = {\n\"text-davinci-003\": 0.02 / 1000,\n\"text-curie-001\": 0.002 / 1000,\n\"gpt-3.5-turbo\": 0.0015 / 1000,\n\"gpt-3.5-turbo-0613\": 0.0015 / 1000,\n\"gpt-3.5-turbo-16k\": 0.003 / 1000,\n\"gpt-3.5-turbo-16k-0613\": 0.003 / 1000,\n\"gpt-4\": 0.03 / 1000,\n\"gpt-4-0613\": 0.03 / 1000,\n\"gpt-4-32k\": 0.06 / 1000,\n\"gpt-4-32k-0613\": 0.06 / 1000,\n}\nCOST_PER_COMPLETION_TOKEN = {\n\"text-davinci-003\": 0.02 / 1000,\n\"text-curie-001\": 0.002 / 1000,\n\"gpt-3.5-turbo\": 0.002 / 1000,\n\"gpt-3.5-turbo-0613\": 0.002 / 1000,\n\"gpt-3.5-turbo-16k\": 0.004 / 1000,\n\"gpt-3.5-turbo-16k-0613\": 0.004 / 1000,\n\"gpt-4\": 0.06 / 1000,\n\"gpt-4-0613\": 0.06 / 1000,\n\"gpt-4-32k\": 0.12 / 1000,\n\"gpt-4-32k-0613\": 0.12 / 1000,\n}\n@cached_property\ndef _engine(self) -&gt; str:\nif self.model_name is not None and self.model_name in self.CHAT_ENGINE_MODELS:\nreturn \"chat\"\nelse:\nreturn \"completion\"\ndef __init__(self, config: AutolabelConfig, cache: BaseCache = None) -&gt; None:\nsuper().__init__(config, cache)\n# populate model name\nself.model_name = config.model_name() or self.DEFAULT_MODEL\n# populate model params and initialize the LLM\nmodel_params = config.model_params()\nif self._engine == \"chat\":\nself.model_params = {**self.DEFAULT_PARAMS_CHAT_ENGINE, **model_params}\nself.llm = ChatOpenAI(model_name=self.model_name, **self.model_params)\nelse:\nself.model_params = {\n**self.DEFAULT_PARAMS_COMPLETION_ENGINE,\n**model_params,\n}\nself.llm = OpenAI(model_name=self.model_name, **self.model_params)\ndef _label(self, prompts: List[str]) -&gt; LLMResult:\nif self._engine == \"chat\":\n# Need to convert list[prompts] -&gt; list[messages]\n# Currently the entire prompt is stuck into the \"human message\"\n# We might consider breaking this up into human vs system message in future\nprompts = [[HumanMessage(content=prompt)] for prompt in prompts]\ntry:\nreturn self.llm.generate(prompts)\nexcept Exception as e:\nprint(f\"Error generating from LLM: {e}, returning empty result\")\ngenerations = [[Generation(text=\"\")] for _ in prompts]\nreturn LLMResult(generations=generations)\ndef get_cost(self, prompt: str, label: Optional[str] = \"\") -&gt; float:\nencoding = tiktoken.encoding_for_model(self.model_name)\nnum_prompt_toks = len(encoding.encode(prompt))\nif label:\nnum_label_toks = len(encoding.encode(label))\nelse:\n# get an upper bound\nnum_label_toks = self.model_params[\"max_tokens\"]\ncost_per_prompt_token = self.COST_PER_PROMPT_TOKEN[self.model_name]\ncost_per_completion_token = self.COST_PER_COMPLETION_TOKEN[self.model_name]\nreturn (num_prompt_toks * cost_per_prompt_token) + (\nnum_label_toks * cost_per_completion_token\n)\ndef returns_token_probs(self) -&gt; bool:\nreturn (\nself.model_name is not None\nand self.model_name in self.MODELS_WITH_TOKEN_PROBS\n)\n</code></pre>"},{"location":"reference/models/#src.autolabel.models.palm.PaLMLLM","title":"<code>PaLMLLM</code>","text":"<p>         Bases: <code>BaseModel</code></p> Source code in <code>src/autolabel/models/palm.py</code> <pre><code>class PaLMLLM(BaseModel):\nSEP_REPLACEMENT_TOKEN = \"@@\"\nCHAT_ENGINE_MODELS = [\"chat-bison@001\"]\nDEFAULT_MODEL = \"text-bison@001\"\nDEFAULT_PARAMS = {\"temperature\": 0}\n# Reference: https://cloud.google.com/vertex-ai/pricing\nCOST_PER_CHARACTER = {\n\"text-bison@001\": 0.001 / 1000,\n\"chat-bison@001\": 0.0005 / 1000,\n\"textembedding-gecko@001\": 0.0001 / 1000,\n}\n@cached_property\ndef _engine(self) -&gt; str:\nif self.model_name is not None and self.model_name in self.CHAT_ENGINE_MODELS:\nreturn \"chat\"\nelse:\nreturn \"completion\"\ndef __init__(self, config: AutolabelConfig, cache: BaseCache = None) -&gt; None:\nsuper().__init__(config, cache)\n# populate model name\nself.model_name = config.model_name() or self.DEFAULT_MODEL\n# populate model params and initialize the LLM\nmodel_params = config.model_params()\nself.model_params = {\n**self.DEFAULT_PARAMS,\n**model_params,\n}\nif self._engine == \"chat\":\nself.llm = ChatVertexAI(model_name=self.model_name, **self.model_params)\nelse:\nself.llm = VertexAI(model_name=self.model_name, **self.model_params)\n@retry(\nreraise=True,\nstop=stop_after_attempt(5),\nwait=wait_exponential(multiplier=1, min=2, max=10),\nbefore_sleep=before_sleep_log(logger, logging.WARNING),\n)\ndef _label_with_retry(self, prompts: List[str]) -&gt; LLMResult:\nreturn self.llm.generate(prompts)\ndef _label(self, prompts: List[str]) -&gt; LLMResult:\nfor prompt in prompts:\nif self.SEP_REPLACEMENT_TOKEN in prompt:\nlogger.warning(\nf\"\"\"Current prompt contains {self.SEP_REPLACEMENT_TOKEN}                                 which is currently used as a separator token by refuel\n                                llm. It is highly recommended to avoid having any\n                                occurences of this substring in the prompt.\n                            \"\"\"\n)\nprompts = [\nprompt.replace(\"\\n\", self.SEP_REPLACEMENT_TOKEN) for prompt in prompts\n]\nif self._engine == \"chat\":\n# Need to convert list[prompts] -&gt; list[messages]\n# Currently the entire prompt is stuck into the \"human message\"\n# We might consider breaking this up into human vs system message in future\nprompts = [[HumanMessage(content=prompt)] for prompt in prompts]\ntry:\nresult = self._label_with_retry(prompts)\nfor generations in result.generations:\nfor generation in generations:\ngeneration.text = generation.text.replace(\nself.SEP_REPLACEMENT_TOKEN, \"\\n\"\n)\nreturn result\nexcept Exception as e:\nlogger.error(f\"Error generating from LLM: {e}.\")\ngenerations = [[Generation(text=\"\")] for _ in prompts]\nreturn LLMResult(generations=generations)\ndef get_cost(self, prompt: str, label: Optional[str] = \"\") -&gt; float:\nif self.model_name is None:\nreturn 0.0\ncost_per_char = self.COST_PER_CHARACTER.get(self.model_name, 0.0)\nreturn cost_per_char * len(prompt)\ndef returns_token_probs(self) -&gt; bool:\nreturn False\n</code></pre>"},{"location":"reference/models/#src.autolabel.models.refuel.RefuelLLM","title":"<code>RefuelLLM</code>","text":"<p>         Bases: <code>BaseModel</code></p> Source code in <code>src/autolabel/models/refuel.py</code> <pre><code>class RefuelLLM(BaseModel):\nDEFAULT_PARAMS = {\n\"max_new_tokens\": 128,\n\"temperature\": 0.0,\n}\ndef __init__(self, config: AutolabelConfig, cache: BaseCache = None) -&gt; None:\nsuper().__init__(config, cache)\n# populate model name\n# This is unused today, but in the future could\n# be used to decide which refuel model is queried\nself.model_name = config.model_name()\nmodel_params = config.model_params()\nself.model_params = {**self.DEFAULT_PARAMS, **model_params}\n# initialize runtime\nself.BASE_API = \"https://refuel-llm.refuel.ai/\"\nself.SEP_REPLACEMENT_TOKEN = \"@@\"\nself.REFUEL_API_ENV = \"REFUEL_API_KEY\"\nif self.REFUEL_API_ENV in os.environ and os.environ[self.REFUEL_API_ENV]:\nself.REFUEL_API_KEY = os.environ[self.REFUEL_API_ENV]\nelse:\nraise ValueError(\nf\"Did not find {self.REFUEL_API_ENV}, please add an environment variable\"\nf\" `{self.REFUEL_API_ENV}` which contains it\"\n)\n@retry(\nreraise=True,\nstop=stop_after_attempt(5),\nwait=wait_exponential(multiplier=1, min=2, max=10),\nbefore_sleep=before_sleep_log(logger, logging.WARNING),\n)\ndef _label_with_retry(self, prompt: str) -&gt; requests.Response:\npayload = {\n\"data\": {\"model_input\": prompt, \"model_params\": {**self.model_params}},\n\"task\": \"generate\",\n}\nheaders = {\"refuel_api_key\": self.REFUEL_API_KEY}\nresponse = requests.post(self.BASE_API, json=payload, headers=headers)\n# raise Exception if status != 200\nresponse.raise_for_status()\nreturn response\ndef _label(self, prompts: List[str]) -&gt; LLMResult:\ngenerations = []\nfor prompt in prompts:\ntry:\nif self.SEP_REPLACEMENT_TOKEN in prompt:\nlogger.warning(\nf\"\"\"Current prompt contains {self.SEP_REPLACEMENT_TOKEN}                             which is currently used as a separator token by refuel\n                            llm. It is highly recommended to avoid having any\n                            occurences of this substring in the prompt.\n                        \"\"\"\n)\nseparated_prompt = prompt.replace(\"\\n\", self.SEP_REPLACEMENT_TOKEN)\nresponse = self._label_with_retry(separated_prompt)\nresponse = json.loads(response.json()[\"body\"]).replace(\nself.SEP_REPLACEMENT_TOKEN, \"\\n\"\n)\ngenerations.append([Generation(text=response)])\nexcept Exception as e:\n# This signifies an error in generating the response using RefuelLLm\nlogger.error(\nf\"Unable to generate prediction: {e}\",\n)\ngenerations.append([Generation(text=\"\")])\nreturn LLMResult(generations=generations)\ndef get_cost(self, prompt: str, label: Optional[str] = \"\") -&gt; float:\nreturn 0\ndef returns_token_probs(self) -&gt; bool:\nreturn False\n</code></pre>"},{"location":"reference/schema/","title":"Schema","text":""},{"location":"reference/schema/#src.autolabel.schema.Dataset","title":"<code>Dataset</code>","text":"<p>         Bases: <code>BaseModel</code></p> <p>Contains Dataset parameters, including input file path, indexes for state management (e.g. job batching and retries), and a unique ID</p> Source code in <code>src/autolabel/schema.py</code> <pre><code>class Dataset(BaseModel):\n\"\"\"Contains Dataset parameters, including input file path, indexes for state management (e.g. job batching and retries), and a unique ID\"\"\"\nid: str\ninput_file: str\nstart_index: int\nend_index: int\nclass Config:\norm_mode = True\n@classmethod\ndef create_id(\nself,\ndataset: Union[str, pd.DataFrame],\nconfig: AutolabelConfig,\nstart_index: int,\nmax_items: int,\n) -&gt; str:\n\"\"\"\n        Generates a unique ID for the given Dataset configuration\n        Args:\n            dataset: either 1) input file name or 2) pandas Dataframe\n            config:  AutolabelConfig object containing project settings\n            start_index: index to begin labeling job at (used for job batching, retries, state management)\n            max_items: number of data points to label, beginning at start_index\n        Returns:\n            filehash: a unique ID generated from an MD5 hash of the functions parameters\n        \"\"\"\nif isinstance(dataset, str):\nfilehash = calculate_md5(\n[open(dataset, \"rb\"), config._dataset_config, start_index, max_items]\n)\nelse:\nfilehash = calculate_md5(\n[dataset.to_csv(), config._dataset_config, start_index, max_items]\n)\nreturn filehash\n</code></pre>"},{"location":"reference/schema/#src.autolabel.schema.Dataset.create_id","title":"<code>create_id(dataset, config, start_index, max_items)</code>  <code>classmethod</code>","text":"<p>Generates a unique ID for the given Dataset configuration</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Union[str, pd.DataFrame]</code> <p>either 1) input file name or 2) pandas Dataframe</p> required <code>config</code> <code>AutolabelConfig</code> <p>AutolabelConfig object containing project settings</p> required <code>start_index</code> <code>int</code> <p>index to begin labeling job at (used for job batching, retries, state management)</p> required <code>max_items</code> <code>int</code> <p>number of data points to label, beginning at start_index</p> required <p>Returns:</p> Name Type Description <code>filehash</code> <code>str</code> <p>a unique ID generated from an MD5 hash of the functions parameters</p> Source code in <code>src/autolabel/schema.py</code> <pre><code>@classmethod\ndef create_id(\nself,\ndataset: Union[str, pd.DataFrame],\nconfig: AutolabelConfig,\nstart_index: int,\nmax_items: int,\n) -&gt; str:\n\"\"\"\n    Generates a unique ID for the given Dataset configuration\n    Args:\n        dataset: either 1) input file name or 2) pandas Dataframe\n        config:  AutolabelConfig object containing project settings\n        start_index: index to begin labeling job at (used for job batching, retries, state management)\n        max_items: number of data points to label, beginning at start_index\n    Returns:\n        filehash: a unique ID generated from an MD5 hash of the functions parameters\n    \"\"\"\nif isinstance(dataset, str):\nfilehash = calculate_md5(\n[open(dataset, \"rb\"), config._dataset_config, start_index, max_items]\n)\nelse:\nfilehash = calculate_md5(\n[dataset.to_csv(), config._dataset_config, start_index, max_items]\n)\nreturn filehash\n</code></pre>"},{"location":"reference/schema/#src.autolabel.schema.FewShotAlgorithm","title":"<code>FewShotAlgorithm</code>","text":"<p>         Bases: <code>str</code>, <code>Enum</code></p> <p>Enum of supported algorithms for choosing which examples to provide the LLM in its instruction prompt</p> Source code in <code>src/autolabel/schema.py</code> <pre><code>class FewShotAlgorithm(str, Enum):\n\"\"\"Enum of supported algorithms for choosing which examples to provide the LLM in its instruction prompt\"\"\"\nFIXED = \"fixed\"\nSEMANTIC_SIMILARITY = \"semantic_similarity\"\nMAX_MARGINAL_RELEVANCE = \"max_marginal_relevance\"\n</code></pre>"},{"location":"reference/schema/#src.autolabel.schema.LLMAnnotation","title":"<code>LLMAnnotation</code>","text":"<p>         Bases: <code>BaseModel</code></p> <p>Contains label information of a given data point, including the generated label, the prompt given to the LLM, and the LLMs response. Optionally includes a confidence_score if supported by the model</p> Source code in <code>src/autolabel/schema.py</code> <pre><code>class LLMAnnotation(BaseModel):\n\"\"\"Contains label information of a given data point, including the generated label, the prompt given to the LLM, and the LLMs response. Optionally includes a confidence_score if supported by the model\"\"\"\nsuccessfully_labeled: bool\nlabel: Any\ncurr_sample: Optional[str] = \"\"\nconfidence_score: Optional[float] = None\ngeneration_info: Optional[Dict[str, Any]] = None\nraw_response: Optional[str] = \"\"\nprompt: Optional[str] = \"\"\n</code></pre>"},{"location":"reference/schema/#src.autolabel.schema.Metric","title":"<code>Metric</code>","text":"<p>         Bases: <code>str</code>, <code>Enum</code></p> <p>Enum of supported performance metrics. Some metrics are always available (task agnostic), while others are only supported by certain types of tasks</p> Source code in <code>src/autolabel/schema.py</code> <pre><code>class Metric(str, Enum):\n\"\"\"Enum of supported performance metrics. Some metrics are always available (task agnostic), while others are only supported by certain types of tasks\"\"\"\n# Task agnostic\nSUPPORT = \"support\"\nCOMPLETION_RATE = \"completion_rate\"\n# Classification metrics\nACCURACY = \"accuracy\"\nCONFUSION_MATRIX = \"confusion_matrix\"\nLABEL_DISTRIBUTION = \"label_distribution\"\nF1 = \"f1\"\n# Confidence metrics\nAUROC = \"auroc\"\nTHRESHOLD = \"threshold\"\n</code></pre>"},{"location":"reference/schema/#src.autolabel.schema.MetricResult","title":"<code>MetricResult</code>","text":"<p>         Bases: <code>BaseModel</code></p> <p>Contains performance metrics gathered from autolabeler runs</p> Source code in <code>src/autolabel/schema.py</code> <pre><code>class MetricResult(BaseModel):\n\"\"\"Contains performance metrics gathered from autolabeler runs\"\"\"\nmetric_type: Metric\nname: str\nvalue: Any\n</code></pre>"},{"location":"reference/schema/#src.autolabel.schema.ModelProvider","title":"<code>ModelProvider</code>","text":"<p>         Bases: <code>str</code>, <code>Enum</code></p> <p>Enum containing all LLM providers currently supported by autolabeler</p> Source code in <code>src/autolabel/schema.py</code> <pre><code>class ModelProvider(str, Enum):\n\"\"\"Enum containing all LLM providers currently supported by autolabeler\"\"\"\nOPENAI = \"openai\"\nANTHROPIC = \"anthropic\"\nHUGGINGFACE_PIPELINE = \"huggingface_pipeline\"\nREFUEL = \"refuel\"\nGOOGLE = \"google\"\n</code></pre>"},{"location":"reference/schema/#src.autolabel.schema.TaskType","title":"<code>TaskType</code>","text":"<p>         Bases: <code>str</code>, <code>Enum</code></p> <p>Enum containing all the types of tasks that autolabeler currently supports</p> Source code in <code>src/autolabel/schema.py</code> <pre><code>class TaskType(str, Enum):\n\"\"\"Enum containing all the types of tasks that autolabeler currently supports\"\"\"\nCLASSIFICATION = \"classification\"\nNAMED_ENTITY_RECOGNITION = \"named_entity_recognition\"\nQUESTION_ANSWERING = \"question_answering\"\nENTITY_MATCHING = \"entity_matching\"\n</code></pre>"},{"location":"reference/tasks/","title":"Tasks","text":""},{"location":"reference/tasks/#src.autolabel.tasks.base.BaseTask","title":"<code>BaseTask</code>","text":"<p>         Bases: <code>ABC</code></p> Source code in <code>src/autolabel/tasks/base.py</code> <pre><code>class BaseTask(ABC):\nZERO_SHOT_TEMPLATE = \"{task_guidelines}\\n\\n{output_guidelines}\\n\\nNow I want you to label the following example:\\n{current_example}\"\nFEW_SHOT_TEMPLATE = \"{task_guidelines}\\n\\n{output_guidelines}\\n\\nSome examples with their output answers are provided below:\\n\\n{seed_examples}\\n\\nNow I want you to label the following example:\\n{current_example}\"\n# Downstream classes should override these\nNULL_LABEL_TOKEN = \"NO_LABEL\"\nDEFAULT_TASK_GUIDELINES = \"\"\nDEFAULT_OUTPUT_GUIDELINES = \"\"\ndef __init__(self, config: AutolabelConfig) -&gt; None:\nself.config = config\n# Update the default prompt template with the prompt template from the config\nself.task_guidelines = (\nself.config.task_guidelines() or self.DEFAULT_TASK_GUIDELINES\n)\nself.output_guidelines = (\nself.config.output_guidelines() or self.DEFAULT_OUTPUT_GUIDELINES\n)\nif self._is_few_shot_mode():\nself.prompt_template = PromptTemplate(\ninput_variables=get_format_variables(self.FEW_SHOT_TEMPLATE),\ntemplate=self.FEW_SHOT_TEMPLATE,\n)\nelse:\nself.prompt_template = PromptTemplate(\ninput_variables=get_format_variables(self.ZERO_SHOT_TEMPLATE),\ntemplate=self.ZERO_SHOT_TEMPLATE,\n)\ndef _is_few_shot_mode(self) -&gt; bool:\nreturn self.config.few_shot_algorithm() in [x.value for x in FewShotAlgorithm]\n@abstractmethod\ndef construct_prompt(self, input: str, examples: List) -&gt; str:\npass\n@abstractmethod\ndef eval(self, llm_labels: List, gt_labels: List) -&gt; List[MetricResult]:\npass\n@abstractmethod\ndef get_explanation_prompt(self, example: Dict) -&gt; str:\nraise NotImplementedError(\n\"Explanation generation not implemented for this task\"\n)\ndef parse_llm_response(\nself, response: Generation, curr_sample: Dict, prompt: str\n) -&gt; LLMAnnotation:\n# The last line of the response is the label\n# This is done to handle the case where the model generates an explanation before generating the label\nif self.config.chain_of_thought():\ntry:\ncompletion_text = extract_valid_json_substring(\nresponse.text.strip().split(\"\\n\")[-1].strip()\n)\ncompletion_text = json.loads(completion_text)[\"label\"]\nexcept:\ncompletion_text = None\nelse:\ncompletion_text = response.text.strip().split(\"\\n\")[-1].strip()\nif len(response.text.strip()) == 0:\nsuccessfully_labeled = False\nllm_label = self.NULL_LABEL_TOKEN\nlogger.warning(f\"LLM response is empty\")\nelif not completion_text:\nsuccessfully_labeled = False\nllm_label = self.NULL_LABEL_TOKEN\nlogger.error(f\"Error parsing LLM response: {response.text}\")\nelse:\nllm_label = completion_text.strip()\nif self.config.task_type() in [\nTaskType.CLASSIFICATION,\nTaskType.ENTITY_MATCHING,\n]:\nif llm_label in self.config.labels_list():\nsuccessfully_labeled = True\nelse:\nlogger.warning(f\"LLM response is not in the labels list\")\nsuccessfully_labeled = False\nelse:\nsuccessfully_labeled = True\nreturn LLMAnnotation(\nsuccessfully_labeled=successfully_labeled,\nlabel=llm_label,\ngeneration_info=response.generation_info,\nraw_response=response.text,\nprompt=prompt,\ncurr_sample=json.dumps(curr_sample),\n)\n</code></pre>"},{"location":"reference/tasks/#src.autolabel.tasks.classification.ClassificationTask","title":"<code>ClassificationTask</code>","text":"<p>         Bases: <code>BaseTask</code></p> Source code in <code>src/autolabel/tasks/classification.py</code> <pre><code>class ClassificationTask(BaseTask):\nDEFAULT_OUTPUT_GUIDELINES = (\n'You will return the answer with just one element: \"the correct label\"'\n)\nDEFAULT_TASK_GUIDELINES = \"Your job is to correctly label the provided input example into one of the following {num_labels} categories.\\nCategories:\\n{labels}\\n\"\nGENERATE_EXPLANATION_PROMPT = \"You are an expert at providing a well reasoned explanation for the output of a given task. \\n\\nBEGIN TASK DESCRIPTION\\n{task_guidelines}\\nEND TASK DESCRIPTION\\nYou will be given an input example and the corresponding output. Your job is to provide an explanation for why the output is correct for the task above.\\nThink step by step and generate an explanation. The last line of the explanation should be - So, the answer is &lt;label&gt;.\\n{labeled_example}\\nExplanation: \"\ndef __init__(self, config: AutolabelConfig) -&gt; None:\nsuper().__init__(config)\ndef construct_prompt(self, input: Dict, examples: List) -&gt; str:\n# Copy over the input so that we can modify it\ninput = input.copy()\n# prepare task guideline\nlabels_list = self.config.labels_list()\nnum_labels = len(labels_list)\nfmt_task_guidelines = self.task_guidelines.format(\nnum_labels=num_labels, labels=\"\\n\".join(labels_list)\n)\n# prepare seed examples\nexample_template = self.config.example_template()\nlabel_column = self.config.label_column()\nfmt_examples = []\nfor eg in examples:\neg_copy = eg.copy()\n# If chain of thought is enabled\nif label_column and self.config.chain_of_thought():\neg_copy[label_column] = json.dumps({\"label\": eg[label_column]})\nfmt_examples.append(example_template.format_map(defaultdict(str, eg_copy)))\n# populate the current example in the prompt\nif label_column:\ninput[label_column] = \"\"\n# populate the explanation column with empty string for current example\nexplanation_column = self.config.explanation_column()\nif explanation_column:\ninput[explanation_column] = \"\"\n# populate the current example in the prompt\ncurrent_example = example_template.format_map(defaultdict(str, input))\nif self._is_few_shot_mode():\nreturn self.prompt_template.format(\ntask_guidelines=fmt_task_guidelines,\noutput_guidelines=self.output_guidelines,\nseed_examples=\"\\n\\n\".join(fmt_examples),\ncurrent_example=current_example,\n)\nelse:\nreturn self.prompt_template.format(\ntask_guidelines=fmt_task_guidelines,\noutput_guidelines=self.output_guidelines,\ncurrent_example=current_example,\n)\ndef get_explanation_prompt(self, example: Dict) -&gt; str:\npt = PromptTemplate(\ninput_variables=get_format_variables(self.GENERATE_EXPLANATION_PROMPT),\ntemplate=self.GENERATE_EXPLANATION_PROMPT,\n)\n# prepare task guideline\nlabels_list = self.config.labels_list()\nnum_labels = len(labels_list)\nfmt_task_guidelines = self.task_guidelines.format(\nnum_labels=num_labels, labels=\"\\n\".join(labels_list)\n)\n# prepare labeled example\nexample_template = self.config.example_template()\nfmt_example = example_template.format_map(defaultdict(str, example))\nreturn pt.format(\ntask_guidelines=fmt_task_guidelines,\nlabeled_example=fmt_example,\n)\ndef auroc_score_labels(\nself, gt_labels, llm_labels\n) -&gt; Tuple[List[int], List[float]]:\nlabels = []\nconfidences = []\nfor index, llm_label in enumerate(llm_labels):\nlabels.append(llm_label.label.lower() == gt_labels[index].lower())\nconfidences.append(llm_label.confidence_score)\nreturn labels, confidences\ndef get_labels_predictions_with_threshold(self, gt_labels, llm_labels, threshold):\nanswered_gt_labels, answered_llm_preds = [], []\nfor index, l in enumerate(llm_labels):\nif l.label != self.NULL_LABEL_TOKEN and (\nl.confidence_score is None or l.confidence_score &gt;= threshold\n):\nanswered_llm_preds.append(l.label.lower())\nanswered_gt_labels.append(gt_labels[index].lower())\nreturn answered_gt_labels, answered_llm_preds\ndef eval(\nself, llm_labels: List[LLMAnnotation], gt_labels: List[str]\n) -&gt; List[MetricResult]:\n\"\"\"Evaluate the LLM generated labels by comparing them against ground truth\n        Args:\n            llm_labels (List[LLMAnnotation]): _description_\n            gt_labels (List[str]): _description_\n        Returns:\n            List[MetricResult]: list of metrics and corresponding values\n        \"\"\"\neval_metrics_map = {\nMetric.SUPPORT: [],\nMetric.ACCURACY: [],\nMetric.COMPLETION_RATE: [],\n}\neval_metrics = []\nthresholds = []\nif self.config.confidence():\neval_metrics_map[Metric.THRESHOLD] = []\nlabels, confidences = self.auroc_score_labels(gt_labels, llm_labels)\nvalue, meaningful_thresholds = ConfidenceCalculator.compute_auroc(\nlabels, confidences\n)\nthresholds.extend(meaningful_thresholds)\neval_metrics.append(\nMetricResult(\nmetric_type=Metric.AUROC,\nname=\"auroc\",\nvalue=value,\n)\n)\nelse:\nthresholds.append(float(\"-inf\"))\nfor index, threshold in enumerate(thresholds):\n(\ncurr_gt_labels,\ncurr_llm_labels,\n) = self.get_labels_predictions_with_threshold(\ngt_labels, llm_labels, threshold\n)\nif len(gt_labels) &gt; 0:\neval_metrics_map[Metric.COMPLETION_RATE].append(\nlen(curr_gt_labels) / float(len(gt_labels))\n)\neval_metrics_map[Metric.SUPPORT].append(len(curr_gt_labels))\nif len(curr_gt_labels) &gt; 0:\neval_metrics_map[Metric.ACCURACY].append(\naccuracy_score(curr_gt_labels, curr_llm_labels)\n)\nelse:\neval_metrics_map[Metric.ACCURACY].append(0.0)\nif self.config.confidence():\neval_metrics_map[Metric.THRESHOLD].append(threshold)\neval_metrics.extend(\n[\nMetricResult(\nmetric_type=i,\nname=i.value,\nvalue=eval_metrics_map[i],\n)\nfor i in eval_metrics_map.keys()\n]\n)\nreturn eval_metrics\n</code></pre>"},{"location":"reference/tasks/#src.autolabel.tasks.classification.ClassificationTask.eval","title":"<code>eval(llm_labels, gt_labels)</code>","text":"<p>Evaluate the LLM generated labels by comparing them against ground truth</p> <p>Parameters:</p> Name Type Description Default <code>llm_labels</code> <code>List[LLMAnnotation]</code> <p>description</p> required <code>gt_labels</code> <code>List[str]</code> <p>description</p> required <p>Returns:</p> Type Description <code>List[MetricResult]</code> <p>List[MetricResult]: list of metrics and corresponding values</p> Source code in <code>src/autolabel/tasks/classification.py</code> <pre><code>def eval(\nself, llm_labels: List[LLMAnnotation], gt_labels: List[str]\n) -&gt; List[MetricResult]:\n\"\"\"Evaluate the LLM generated labels by comparing them against ground truth\n    Args:\n        llm_labels (List[LLMAnnotation]): _description_\n        gt_labels (List[str]): _description_\n    Returns:\n        List[MetricResult]: list of metrics and corresponding values\n    \"\"\"\neval_metrics_map = {\nMetric.SUPPORT: [],\nMetric.ACCURACY: [],\nMetric.COMPLETION_RATE: [],\n}\neval_metrics = []\nthresholds = []\nif self.config.confidence():\neval_metrics_map[Metric.THRESHOLD] = []\nlabels, confidences = self.auroc_score_labels(gt_labels, llm_labels)\nvalue, meaningful_thresholds = ConfidenceCalculator.compute_auroc(\nlabels, confidences\n)\nthresholds.extend(meaningful_thresholds)\neval_metrics.append(\nMetricResult(\nmetric_type=Metric.AUROC,\nname=\"auroc\",\nvalue=value,\n)\n)\nelse:\nthresholds.append(float(\"-inf\"))\nfor index, threshold in enumerate(thresholds):\n(\ncurr_gt_labels,\ncurr_llm_labels,\n) = self.get_labels_predictions_with_threshold(\ngt_labels, llm_labels, threshold\n)\nif len(gt_labels) &gt; 0:\neval_metrics_map[Metric.COMPLETION_RATE].append(\nlen(curr_gt_labels) / float(len(gt_labels))\n)\neval_metrics_map[Metric.SUPPORT].append(len(curr_gt_labels))\nif len(curr_gt_labels) &gt; 0:\neval_metrics_map[Metric.ACCURACY].append(\naccuracy_score(curr_gt_labels, curr_llm_labels)\n)\nelse:\neval_metrics_map[Metric.ACCURACY].append(0.0)\nif self.config.confidence():\neval_metrics_map[Metric.THRESHOLD].append(threshold)\neval_metrics.extend(\n[\nMetricResult(\nmetric_type=i,\nname=i.value,\nvalue=eval_metrics_map[i],\n)\nfor i in eval_metrics_map.keys()\n]\n)\nreturn eval_metrics\n</code></pre>"},{"location":"reference/tasks/#src.autolabel.tasks.entity_matching.EntityMatchingTask","title":"<code>EntityMatchingTask</code>","text":"<p>         Bases: <code>BaseTask</code></p> Source code in <code>src/autolabel/tasks/entity_matching.py</code> <pre><code>class EntityMatchingTask(BaseTask):\nDEFAULT_OUTPUT_GUIDELINES = (\n'You will return the answer with one element: \"the correct option\"\\n'\n)\nDEFAULT_TASK_GUIDELINES = \"Your job is to tell if the two given entities are duplicates or not. You will return the answer from one of the choices. Choices:\\n{labels}\\n\"\nGENERATE_EXPLANATION_PROMPT = \"You are an expert at providing a well reasoned explanation for the output of a given task. \\n\\nBEGIN TASK DESCRIPTION\\n{task_guidelines}\\nEND TASK DESCRIPTION\\nYou will be given an input example and the corresponding output. Your job is to provide an explanation for why the output is correct for the task above.\\nThink step by step and generate an explanation. The last line of the explanation should be - So, the answer is &lt;label&gt;.\\n{labeled_example}\\nExplanation: \"\ndef __init__(self, config: AutolabelConfig) -&gt; None:\nsuper().__init__(config)\ndef construct_prompt(self, input: Dict, examples: List[Dict]) -&gt; str:\n# Copy over the input so that we can modify it\ninput = input.copy()\n# prepare task guideline\nlabels_list = self.config.labels_list()\nnum_labels = len(labels_list)\nfmt_task_guidelines = self.task_guidelines.format(\nnum_labels=num_labels, labels=\"\\n\".join(labels_list)\n)\n# prepare seed examples\nexample_template = self.config.example_template()\nlabel_column = self.config.label_column()\nfmt_examples = []\nfor eg in examples:\neg_copy = eg.copy()\n# If chain of thought is enabled\nif label_column and self.config.chain_of_thought():\neg_copy[label_column] = json.dumps({\"label\": eg[label_column]})\nfmt_examples.append(example_template.format_map(defaultdict(str, eg_copy)))\n# populate the current example in the prompt\nif label_column:\ninput[label_column] = \"\"\n# populate the explanation column with empty string for current example\nexplanation_column = self.config.explanation_column()\nif explanation_column:\ninput[explanation_column] = \"\"\n# populate the current example in the prompt\ncurrent_example = example_template.format_map(defaultdict(str, input))\nif self._is_few_shot_mode():\nreturn self.prompt_template.format(\ntask_guidelines=fmt_task_guidelines,\noutput_guidelines=self.output_guidelines,\nseed_examples=\"\\n\\n\".join(fmt_examples),\ncurrent_example=current_example,\n)\nelse:\nreturn self.prompt_template.format(\ntask_guidelines=fmt_task_guidelines,\noutput_guidelines=self.output_guidelines,\ncurrent_example=current_example,\n)\ndef get_explanation_prompt(self, example: Dict) -&gt; str:\npt = PromptTemplate(\ninput_variables=get_format_variables(self.GENERATE_EXPLANATION_PROMPT),\ntemplate=self.GENERATE_EXPLANATION_PROMPT,\n)\n# prepare task guideline\nlabels_list = self.config.labels_list()\nnum_labels = len(labels_list)\nfmt_task_guidelines = self.task_guidelines.format(\nnum_labels=num_labels, labels=\"\\n\".join(labels_list)\n)\n# prepare labeled example\nexample_template = self.config.example_template()\nfmt_example = example_template.format_map(defaultdict(str, example))\nreturn pt.format(\ntask_guidelines=fmt_task_guidelines,\nlabeled_example=fmt_example,\n)\ndef auroc_score_labels(\nself, gt_labels, llm_labels\n) -&gt; Tuple[List[int], List[float]]:\nlabels = []\nconfidences = []\nfor index, llm_label in enumerate(llm_labels):\nlabels.append(llm_label.label.lower() == gt_labels[index].lower())\nconfidences.append(llm_label.confidence_score)\nreturn labels, confidences\ndef get_labels_predictions_with_threshold(self, gt_labels, llm_labels, threshold):\nanswered_gt_labels, answered_llm_preds = [], []\nfor index, l in enumerate(llm_labels):\nif l.label != self.NULL_LABEL_TOKEN and (\nl.confidence_score is None or l.confidence_score &gt;= threshold\n):\nanswered_llm_preds.append(l.label.lower())\nanswered_gt_labels.append(gt_labels[index].lower())\nreturn answered_gt_labels, answered_llm_preds\ndef eval(\nself, llm_labels: List[LLMAnnotation], gt_labels: List[str]\n) -&gt; List[MetricResult]:\n\"\"\"Evaluate the LLM generated labels by comparing them against ground truth\n        Args:\n            llm_labels (List[LLMAnnotation]): _description_\n            gt_labels (List[str]): _description_\n        Returns:\n            List[MetricResult]: list of metrics and corresponding values\n        \"\"\"\neval_metrics_map = {\nMetric.SUPPORT: [],\nMetric.ACCURACY: [],\nMetric.COMPLETION_RATE: [],\n}\neval_metrics = []\nthresholds = []\nif self.config.confidence():\neval_metrics_map[Metric.THRESHOLD] = []\nlabels, confidences = self.auroc_score_labels(gt_labels, llm_labels)\nvalue, meaningful_thresholds = ConfidenceCalculator.compute_auroc(\nlabels, confidences\n)\nthresholds.extend(meaningful_thresholds)\neval_metrics.append(\nMetricResult(\nmetric_type=Metric.AUROC,\nname=\"auroc\",\nvalue=value,\n)\n)\nelse:\nthresholds.append(float(\"-inf\"))\nfor index, threshold in enumerate(thresholds):\n(\ncurr_gt_labels,\ncurr_llm_labels,\n) = self.get_labels_predictions_with_threshold(\ngt_labels, llm_labels, threshold\n)\nif len(gt_labels) &gt; 0:\neval_metrics_map[Metric.COMPLETION_RATE].append(\nlen(curr_gt_labels) / float(len(gt_labels))\n)\neval_metrics_map[Metric.SUPPORT].append(len(curr_gt_labels))\nif len(curr_gt_labels) &gt; 0:\neval_metrics_map[Metric.ACCURACY].append(\naccuracy_score(curr_gt_labels, curr_llm_labels)\n)\nelse:\neval_metrics_map[Metric.ACCURACY].append(0.0)\nif self.config.confidence():\neval_metrics_map[Metric.THRESHOLD].append(threshold)\neval_metrics.extend(\n[\nMetricResult(\nmetric_type=i,\nname=i.value,\nvalue=eval_metrics_map[i],\n)\nfor i in eval_metrics_map.keys()\n]\n)\nreturn eval_metrics\n</code></pre>"},{"location":"reference/tasks/#src.autolabel.tasks.entity_matching.EntityMatchingTask.eval","title":"<code>eval(llm_labels, gt_labels)</code>","text":"<p>Evaluate the LLM generated labels by comparing them against ground truth</p> <p>Parameters:</p> Name Type Description Default <code>llm_labels</code> <code>List[LLMAnnotation]</code> <p>description</p> required <code>gt_labels</code> <code>List[str]</code> <p>description</p> required <p>Returns:</p> Type Description <code>List[MetricResult]</code> <p>List[MetricResult]: list of metrics and corresponding values</p> Source code in <code>src/autolabel/tasks/entity_matching.py</code> <pre><code>def eval(\nself, llm_labels: List[LLMAnnotation], gt_labels: List[str]\n) -&gt; List[MetricResult]:\n\"\"\"Evaluate the LLM generated labels by comparing them against ground truth\n    Args:\n        llm_labels (List[LLMAnnotation]): _description_\n        gt_labels (List[str]): _description_\n    Returns:\n        List[MetricResult]: list of metrics and corresponding values\n    \"\"\"\neval_metrics_map = {\nMetric.SUPPORT: [],\nMetric.ACCURACY: [],\nMetric.COMPLETION_RATE: [],\n}\neval_metrics = []\nthresholds = []\nif self.config.confidence():\neval_metrics_map[Metric.THRESHOLD] = []\nlabels, confidences = self.auroc_score_labels(gt_labels, llm_labels)\nvalue, meaningful_thresholds = ConfidenceCalculator.compute_auroc(\nlabels, confidences\n)\nthresholds.extend(meaningful_thresholds)\neval_metrics.append(\nMetricResult(\nmetric_type=Metric.AUROC,\nname=\"auroc\",\nvalue=value,\n)\n)\nelse:\nthresholds.append(float(\"-inf\"))\nfor index, threshold in enumerate(thresholds):\n(\ncurr_gt_labels,\ncurr_llm_labels,\n) = self.get_labels_predictions_with_threshold(\ngt_labels, llm_labels, threshold\n)\nif len(gt_labels) &gt; 0:\neval_metrics_map[Metric.COMPLETION_RATE].append(\nlen(curr_gt_labels) / float(len(gt_labels))\n)\neval_metrics_map[Metric.SUPPORT].append(len(curr_gt_labels))\nif len(curr_gt_labels) &gt; 0:\neval_metrics_map[Metric.ACCURACY].append(\naccuracy_score(curr_gt_labels, curr_llm_labels)\n)\nelse:\neval_metrics_map[Metric.ACCURACY].append(0.0)\nif self.config.confidence():\neval_metrics_map[Metric.THRESHOLD].append(threshold)\neval_metrics.extend(\n[\nMetricResult(\nmetric_type=i,\nname=i.value,\nvalue=eval_metrics_map[i],\n)\nfor i in eval_metrics_map.keys()\n]\n)\nreturn eval_metrics\n</code></pre>"},{"location":"reference/tasks/#src.autolabel.tasks.question_answering.QuestionAnsweringTask","title":"<code>QuestionAnsweringTask</code>","text":"<p>         Bases: <code>BaseTask</code></p> Source code in <code>src/autolabel/tasks/question_answering.py</code> <pre><code>class QuestionAnsweringTask(BaseTask):\nDEFAULT_OUTPUT_GUIDELINES = (\n'You will return the answer one element: \"the correct label\"\\n'\n)\nDEFAULT_TASK_GUIDELINES = \"Your job is to answer the following questions using the options provided for each question. Choose the best answer for the question.\\n\"\nNULL_LABEL_TOKEN = \"NO_LABEL\"\nGENERATE_EXPLANATION_PROMPT = \"You are an expert at providing a well reasoned explanation for the output of a given task. \\n\\nBEGIN TASK DESCRIPTION\\n{task_guidelines}\\nEND TASK DESCRIPTION\\nYou will be given an input example and the corresponding output. You will be given a question and an answer. Your job is to provide an explanation for why the answer is correct for the task above.\\nThink step by step and generate an explanation. The last line of the explanation should be - So, the answer is &lt;label&gt;.\\n{labeled_example}\\nExplanation: \"\ndef __init__(self, config: AutolabelConfig) -&gt; None:\nsuper().__init__(config)\ndef construct_prompt(self, input: Dict, examples: List[Dict]) -&gt; str:\n# Copy over the input so that we can modify it\ninput = input.copy()\n# prepare seed examples\nexample_template = self.config.example_template()\nlabel_column = self.config.label_column()\nfmt_examples = []\nfor eg in examples:\neg_copy = eg.copy()\n# If chain of thought is enabled\nif label_column and self.config.chain_of_thought():\neg_copy[label_column] = json.dumps({\"label\": eg[label_column]})\nfmt_examples.append(example_template.format_map(defaultdict(str, eg_copy)))\n# populate the current example in the prompt\nif label_column:\ninput[label_column] = \"\"\n# populate the explanation column with empty string for current example\nexplanation_column = self.config.explanation_column()\nif explanation_column:\ninput[explanation_column] = \"\"\n# populate the current example in the prompt\ncurrent_example = example_template.format_map(defaultdict(str, input))\nif self._is_few_shot_mode():\nreturn self.prompt_template.format(\ntask_guidelines=self.task_guidelines,\noutput_guidelines=self.output_guidelines,\nseed_examples=\"\\n\\n\".join(fmt_examples),\ncurrent_example=current_example,\n)\nelse:\nreturn self.prompt_template.format(\ntask_guidelines=self.task_guidelines,\noutput_guidelines=self.output_guidelines,\ncurrent_example=current_example,\n)\ndef auroc_score_labels(\nself, gt_labels, llm_labels\n) -&gt; Tuple[List[int], List[float]]:\nlabels = []\nconfidences = []\nfor index, llm_label in enumerate(llm_labels):\nlabels.append(\nnormalize_text(llm_label.label.lower())\n== normalize_text(gt_labels[index].lower())\n)\nconfidences.append(llm_label.confidence_score)\nreturn labels, confidences\ndef get_labels_predictions_with_threshold(self, gt_labels, llm_labels, threshold):\nanswered_gt_labels, answered_llm_preds = [], []\nfor index, l in enumerate(llm_labels):\nif l.label != self.NULL_LABEL_TOKEN and (\nl.confidence_score is None or l.confidence_score &gt;= threshold\n):\nanswered_llm_preds.append(normalize_text(l.label.lower()))\nanswered_gt_labels.append(normalize_text(gt_labels[index].lower()))\nreturn answered_gt_labels, answered_llm_preds\ndef get_explanation_prompt(self, example: Dict) -&gt; str:\npt = PromptTemplate(\ninput_variables=get_format_variables(self.GENERATE_EXPLANATION_PROMPT),\ntemplate=self.GENERATE_EXPLANATION_PROMPT,\n)\nexample_template = self.config.example_template()\nfmt_example = example_template.format_map(defaultdict(str, example))\nreturn pt.format(\ntask_guidelines=self.task_guidelines,\nlabeled_example=fmt_example,\n)\ndef eval(\nself, llm_labels: List[LLMAnnotation], gt_labels: List[str]\n) -&gt; List[MetricResult]:\n\"\"\"Evaluate the LLM generated labels by comparing them against ground truth\n        Args:\n            llm_labels (List[LLMAnnotation]): _description_\n            gt_labels (List[str]): _description_\n        Returns:\n            List[MetricResult]: list of metrics and corresponding values\n        \"\"\"\neval_metrics_map = {\nMetric.F1: [],\nMetric.SUPPORT: [],\nMetric.ACCURACY: [],\nMetric.COMPLETION_RATE: [],\n}\neval_metrics = []\nthresholds = []\nif self.config.confidence():\neval_metrics_map[Metric.THRESHOLD] = []\nlabels, confidences = self.auroc_score_labels(gt_labels, llm_labels)\nvalue, meaningful_thresholds = ConfidenceCalculator.compute_auroc(\nlabels, confidences\n)\nthresholds.extend(meaningful_thresholds)\neval_metrics.append(\nMetricResult(\nmetric_type=Metric.AUROC,\nname=\"auroc\",\nvalue=value,\n)\n)\nelse:\nthresholds.append(float(\"-inf\"))\nfor index, threshold in enumerate(thresholds):\n(\ncurr_gt_labels,\ncurr_llm_labels,\n) = self.get_labels_predictions_with_threshold(\ngt_labels, llm_labels, threshold\n)\nif len(gt_labels) &gt; 0:\neval_metrics_map[Metric.COMPLETION_RATE].append(\nlen(curr_gt_labels) / float(len(gt_labels))\n)\neval_metrics_map[Metric.SUPPORT].append(len(curr_gt_labels))\nif len(curr_gt_labels) &gt; 0:\neval_metrics_map[Metric.ACCURACY].append(\naccuracy_score(curr_gt_labels, curr_llm_labels)\n)\nelse:\neval_metrics_map[Metric.ACCURACY].append(0.0)\nif self.config.confidence():\neval_metrics_map[Metric.THRESHOLD].append(threshold)\nf1 = sum(\n[\ncompute_f1(curr_llm_labels[index], curr_gt_labels[index])\nfor index in range(len(curr_llm_labels))\n]\n)\neval_metrics_map[Metric.F1].append(\n(float(f1) / len(curr_llm_labels)) if len(curr_llm_labels) &gt; 0 else 0.0\n)\neval_metrics.extend(\n[\nMetricResult(\nmetric_type=i,\nname=i.value,\nvalue=eval_metrics_map[i],\n)\nfor i in eval_metrics_map.keys()\n]\n)\nreturn eval_metrics\n</code></pre>"},{"location":"reference/tasks/#src.autolabel.tasks.question_answering.QuestionAnsweringTask.eval","title":"<code>eval(llm_labels, gt_labels)</code>","text":"<p>Evaluate the LLM generated labels by comparing them against ground truth</p> <p>Parameters:</p> Name Type Description Default <code>llm_labels</code> <code>List[LLMAnnotation]</code> <p>description</p> required <code>gt_labels</code> <code>List[str]</code> <p>description</p> required <p>Returns:</p> Type Description <code>List[MetricResult]</code> <p>List[MetricResult]: list of metrics and corresponding values</p> Source code in <code>src/autolabel/tasks/question_answering.py</code> <pre><code>def eval(\nself, llm_labels: List[LLMAnnotation], gt_labels: List[str]\n) -&gt; List[MetricResult]:\n\"\"\"Evaluate the LLM generated labels by comparing them against ground truth\n    Args:\n        llm_labels (List[LLMAnnotation]): _description_\n        gt_labels (List[str]): _description_\n    Returns:\n        List[MetricResult]: list of metrics and corresponding values\n    \"\"\"\neval_metrics_map = {\nMetric.F1: [],\nMetric.SUPPORT: [],\nMetric.ACCURACY: [],\nMetric.COMPLETION_RATE: [],\n}\neval_metrics = []\nthresholds = []\nif self.config.confidence():\neval_metrics_map[Metric.THRESHOLD] = []\nlabels, confidences = self.auroc_score_labels(gt_labels, llm_labels)\nvalue, meaningful_thresholds = ConfidenceCalculator.compute_auroc(\nlabels, confidences\n)\nthresholds.extend(meaningful_thresholds)\neval_metrics.append(\nMetricResult(\nmetric_type=Metric.AUROC,\nname=\"auroc\",\nvalue=value,\n)\n)\nelse:\nthresholds.append(float(\"-inf\"))\nfor index, threshold in enumerate(thresholds):\n(\ncurr_gt_labels,\ncurr_llm_labels,\n) = self.get_labels_predictions_with_threshold(\ngt_labels, llm_labels, threshold\n)\nif len(gt_labels) &gt; 0:\neval_metrics_map[Metric.COMPLETION_RATE].append(\nlen(curr_gt_labels) / float(len(gt_labels))\n)\neval_metrics_map[Metric.SUPPORT].append(len(curr_gt_labels))\nif len(curr_gt_labels) &gt; 0:\neval_metrics_map[Metric.ACCURACY].append(\naccuracy_score(curr_gt_labels, curr_llm_labels)\n)\nelse:\neval_metrics_map[Metric.ACCURACY].append(0.0)\nif self.config.confidence():\neval_metrics_map[Metric.THRESHOLD].append(threshold)\nf1 = sum(\n[\ncompute_f1(curr_llm_labels[index], curr_gt_labels[index])\nfor index in range(len(curr_llm_labels))\n]\n)\neval_metrics_map[Metric.F1].append(\n(float(f1) / len(curr_llm_labels)) if len(curr_llm_labels) &gt; 0 else 0.0\n)\neval_metrics.extend(\n[\nMetricResult(\nmetric_type=i,\nname=i.value,\nvalue=eval_metrics_map[i],\n)\nfor i in eval_metrics_map.keys()\n]\n)\nreturn eval_metrics\n</code></pre>"},{"location":"reference/tasks/#src.autolabel.tasks.named_entity_recognition.NamedEntityRecognitionTask","title":"<code>NamedEntityRecognitionTask</code>","text":"<p>         Bases: <code>BaseTask</code></p> Source code in <code>src/autolabel/tasks/named_entity_recognition.py</code> <pre><code>class NamedEntityRecognitionTask(BaseTask):\nDEFAULT_OUTPUT_GUIDELINES = \"You will return the answer in CSV format, with two columns seperated by the % character. First column is the extracted entity and second column is the category. Rows in the CSV are separated by new line character.\"\nDEFAULT_TASK_GUIDELINES = \"Your job is to extract named entities mentioned in text, and classify them into one of the following {num_labels} categories.\\nCategories:\\n{labels}\\n \"\nNULL_LABEL = {}\ndef __init__(self, config: AutolabelConfig) -&gt; None:\nsuper().__init__(config)\ndef _json_to_llm_format(self, input_label: str) -&gt; str:\n# `label` format: {\"entity type\": [list of entities of this type]}\ntry:\nlabels = json.loads(input_label)\nrows = []\nfor entity_type, detected_entites in labels.items():\nfor e in detected_entites:\nrow = \"%\".join([e, entity_type])\nrows.append(row)\nllm_formatted_label = \"\\n\".join(rows)\nreturn llm_formatted_label\nexcept json.JSONDecodeError as e:\nlogger.error(\nf\"Could not parse label: {input_label}. Few-shot examples might be formatted incorrectly\"\n)\nreturn input_label\ndef _llm_to_json_format(self, response: str):\nsplit_response = response.split(\"\\n\")\njson_output = {i: [] for i in self.config.labels_list()}\nfor row in split_response:\nparts = row.split(\"%\")\nif len(parts) != 2 or parts[1] not in json_output.keys():\nlogger.debug(f\"Malformed LLM response: {row}\")\ncontinue\nnamed_entity = parts[0]\ncategory = parts[1]\njson_output[category].append(named_entity)\nreturn json_output\ndef construct_prompt(self, input: Dict, examples: List) -&gt; str:\n# prepare task guideline\nlabels_list = self.config.labels_list()\nnum_labels = len(labels_list)\nfmt_task_guidelines = self.task_guidelines.format(\nnum_labels=num_labels, labels=\"\\n\".join(labels_list)\n)\n# prepare seed examples\nlabel_column = self.config.label_column()\nexample_template = self.config.example_template()\nfmt_examples = []\nfor eg in examples:\neg_copy = deepcopy(eg)\nif label_column:\neg_copy[label_column] = self._json_to_llm_format(eg_copy[label_column])\nfmt_examples.append(example_template.format_map(defaultdict(str, eg_copy)))\n# populate the current example in the prompt\nif label_column:\ninput[label_column] = \"\"\n# populate the explanation column with empty string for current example\nexplanation_column = self.config.explanation_column()\nif explanation_column:\ninput[explanation_column] = \"\"\n# populate the current example in the prompt\ncurrent_example = example_template.format_map(defaultdict(str, input))\nif self._is_few_shot_mode():\nreturn self.prompt_template.format(\ntask_guidelines=fmt_task_guidelines,\noutput_guidelines=self.output_guidelines,\nseed_examples=\"\\n\\n\".join(fmt_examples),\ncurrent_example=current_example,\n)\nelse:\nreturn self.prompt_template.format(\ntask_guidelines=fmt_task_guidelines,\noutput_guidelines=self.output_guidelines,\ncurrent_example=current_example,\n)\ndef get_explanation_prompt(self, example: Dict) -&gt; str:\nraise NotImplementedError(\n\"Explanation generation not implemented for this task\"\n)\ndef add_text_spans(self, raw_output: dict, input: str) -&gt; list:\nprocessed_output = []\nfor entity_type in raw_output:\nfor curr_entity in raw_output[entity_type]:\nprocessed_output.append({\"type\": entity_type, \"text\": curr_entity})\n# create a frequency dict of each named entity in the input to determine text spans for repeated entities\nfrequency_count = {label[\"text\"]: 0 for label in processed_output}\nfor label in processed_output:\ntext = label[\"text\"]\nmatches = [i.start() for i in re.finditer(text, input)]\ncount = frequency_count[text]\n# if count of the named entity is greater than the number of matches, default to last found match\nif count &gt;= len(matches):\ncount = -1\n# if no occurence of named entity in input, default text span to start: -1, end: -1\nif len(matches) == 0:\nlabel[\"start\"] = -1\nlabel[\"end\"] = -1\nelse:\nlabel[\"start\"] = matches[count]\nlabel[\"end\"] = matches[count] + len(text)\nfrequency_count[text] += 1\nreturn processed_output\ndef parse_llm_response(\nself, response: Generation, curr_sample: Dict, prompt: str\n) -&gt; LLMAnnotation:\noutput = {}\nsuccessfully_labeled = False\ntext_column = self.config.text_column()\ninput_str = curr_sample[text_column]\ntry:\ncompletion_text = response.text\noutput = self._llm_to_json_format(completion_text.strip())\nllm_label = self.add_text_spans(output, input_str)\nexcept Exception as e:\nlogger.error(f\"Error parsing LLM response: {response.text}, Error: {e}\")\nllm_label = self.NULL_LABEL\nsuccessfully_labeled = False if llm_label == self.NULL_LABEL else True\n# TODO: parse generation info correctly to fetch &amp; transform logprobs -&gt; score\nreturn LLMAnnotation(\ncurr_sample=input_str,\nsuccessfully_labeled=successfully_labeled,\nlabel=llm_label,\ngeneration_info=response.generation_info,\nraw_response=response.text,\nprompt=prompt,\n)\ndef auroc_score_labels(\nself, gt_labels, llm_labels_with_conf\n) -&gt; Tuple[List[int], List[float]]:\nlabels = []\nconfidences = []\nfor index, pred_entities in enumerate(llm_labels_with_conf):\ngt_entities = gt_labels[index]\npred_conf = pred_entities[0][\"conf\"] if len(pred_entities) &gt; 0 else 0\nfor gt_entity in gt_entities:\nmatch_found = False\npred_index = 0\nwhile not match_found and pred_index &lt; len(pred_entities):\ncurr_match = True\nfor key in gt_entity:\nif gt_entity[key] != pred_entities[pred_index][key]:\ncurr_match = False\nif curr_match:\nmatch_found = True\npred_index += 1\nlabels.append(int(match_found))\nconfidences.append(pred_conf)\nreturn labels, confidences\ndef get_labels_predictions_with_threshold(self, gt_labels, llm_labels, threshold):\nanswered_gt_labels, answered_llm_preds = [], []\nfor index, l in enumerate(llm_labels):\nif l.successfully_labeled and (\nl.confidence_score is None or l.confidence_score &gt;= threshold\n):\nanswered_gt_labels.append(\n[{**entity, \"label\": entity[\"type\"]} for entity in gt_labels[index]]\n)\nanswered_llm_preds.append(\n[\n{\n**entity,\n\"label\": entity[\"type\"],\n\"conf\": l.confidence_score,\n}\nfor entity in l.label\n],\n)\nreturn answered_gt_labels, answered_llm_preds\ndef run_metrics(\nself,\nanswered_gt_labels,\nanswered_llm_preds,\nentity_types_set,\n) -&gt; List[MetricResult]:\neval_metrics = []\nevaluator = Evaluator(\nanswered_gt_labels, answered_llm_preds, tags=entity_types_set\n)\nresults, _ = evaluator.evaluate()\n# f1 score\neval_metrics.append(\nMetricResult(\nmetric_type=Metric.F1,\nname=f\"f1\",\nvalue=results[\"exact\"][\"f1\"],\n)\n)\n# accuracy\naccuracy = (\nresults.get(\"strict\").get(\"correct\")\n/ (results.get(\"strict\").get(\"possible\"))\nif results.get(\"strict\").get(\"possible\") &gt; 0\nelse 0.0\n)\neval_metrics.append(\nMetricResult(\nmetric_type=Metric.ACCURACY,\nname=f\"accuracy\",\nvalue=accuracy,\n)\n)\neval_metrics.append(\nMetricResult(\nmetric_type=Metric.SUPPORT,\nname=f\"support\",\nvalue=len(answered_gt_labels),\n)\n)\nreturn eval_metrics\ndef eval(\nself, llm_labels: List[LLMAnnotation], gt_labels: List[str]\n) -&gt; List[MetricResult]:\n\"\"\"Evaluate the LLM generated labels by comparing them against ground truth\n        Args:\n            llm_labels (List[LLMAnnotation]): _description_\n            gt_labels (List[str]): _description_\n        Returns:\n            List[MetricResult]: list of metrics and corresponding values\n        \"\"\"\ngt_labels = [\nself.add_text_spans(\njson.loads(gt_labels[index]), llm_labels[index].curr_sample\n)\nfor index in range(len(gt_labels))\n]\neval_metrics_map = {\nMetric.F1: [],\nMetric.SUPPORT: [],\nMetric.ACCURACY: [],\nMetric.COMPLETION_RATE: [],\n}\neval_metrics = []\nthresholds = []\nif self.config.confidence():\neval_metrics_map[Metric.THRESHOLD] = []\nall_gt_labels, all_llm_preds = self.get_labels_predictions_with_threshold(\ngt_labels, llm_labels, float(\"-inf\")\n)\nlabels, confidences = self.auroc_score_labels(all_gt_labels, all_llm_preds)\nvalue, meaningful_thresholds = ConfidenceCalculator.compute_auroc(\nlabels, confidences\n)\nthresholds.extend(meaningful_thresholds)\neval_metrics.append(\nMetricResult(\nmetric_type=Metric.AUROC,\nname=\"auroc\",\nvalue=value,\n)\n)\nelse:\nthresholds.append(float(\"-inf\"))\nfor index, threshold in enumerate(thresholds):\n(\ncurr_gt_labels,\ncurr_llm_labels,\n) = self.get_labels_predictions_with_threshold(\ngt_labels, llm_labels, threshold\n)\nentity_types_set = list(\nset(\n[\ngt_entity.get(\"label\")\nfor gt_label in curr_gt_labels\nfor gt_entity in gt_label\n]\n)\n)\ncurr_threshold_metrics = self.run_metrics(\ncurr_gt_labels,\ncurr_llm_labels,\nentity_types_set,\n)\nfor metric in curr_threshold_metrics:\neval_metrics_map[metric.metric_type].append(metric.value)\neval_metrics_map[Metric.COMPLETION_RATE].append(\nlen(curr_llm_labels) / float(len(gt_labels))\n)\nif self.config.confidence():\neval_metrics_map[Metric.THRESHOLD].append(threshold)\neval_metrics.extend(\n[\nMetricResult(\nmetric_type=i,\nname=i.value,\nvalue=eval_metrics_map[i],\n)\nfor i in eval_metrics_map.keys()\n]\n)\nreturn eval_metrics\n</code></pre>"},{"location":"reference/tasks/#src.autolabel.tasks.named_entity_recognition.NamedEntityRecognitionTask.eval","title":"<code>eval(llm_labels, gt_labels)</code>","text":"<p>Evaluate the LLM generated labels by comparing them against ground truth</p> <p>Parameters:</p> Name Type Description Default <code>llm_labels</code> <code>List[LLMAnnotation]</code> <p>description</p> required <code>gt_labels</code> <code>List[str]</code> <p>description</p> required <p>Returns:</p> Type Description <code>List[MetricResult]</code> <p>List[MetricResult]: list of metrics and corresponding values</p> Source code in <code>src/autolabel/tasks/named_entity_recognition.py</code> <pre><code>def eval(\nself, llm_labels: List[LLMAnnotation], gt_labels: List[str]\n) -&gt; List[MetricResult]:\n\"\"\"Evaluate the LLM generated labels by comparing them against ground truth\n    Args:\n        llm_labels (List[LLMAnnotation]): _description_\n        gt_labels (List[str]): _description_\n    Returns:\n        List[MetricResult]: list of metrics and corresponding values\n    \"\"\"\ngt_labels = [\nself.add_text_spans(\njson.loads(gt_labels[index]), llm_labels[index].curr_sample\n)\nfor index in range(len(gt_labels))\n]\neval_metrics_map = {\nMetric.F1: [],\nMetric.SUPPORT: [],\nMetric.ACCURACY: [],\nMetric.COMPLETION_RATE: [],\n}\neval_metrics = []\nthresholds = []\nif self.config.confidence():\neval_metrics_map[Metric.THRESHOLD] = []\nall_gt_labels, all_llm_preds = self.get_labels_predictions_with_threshold(\ngt_labels, llm_labels, float(\"-inf\")\n)\nlabels, confidences = self.auroc_score_labels(all_gt_labels, all_llm_preds)\nvalue, meaningful_thresholds = ConfidenceCalculator.compute_auroc(\nlabels, confidences\n)\nthresholds.extend(meaningful_thresholds)\neval_metrics.append(\nMetricResult(\nmetric_type=Metric.AUROC,\nname=\"auroc\",\nvalue=value,\n)\n)\nelse:\nthresholds.append(float(\"-inf\"))\nfor index, threshold in enumerate(thresholds):\n(\ncurr_gt_labels,\ncurr_llm_labels,\n) = self.get_labels_predictions_with_threshold(\ngt_labels, llm_labels, threshold\n)\nentity_types_set = list(\nset(\n[\ngt_entity.get(\"label\")\nfor gt_label in curr_gt_labels\nfor gt_entity in gt_label\n]\n)\n)\ncurr_threshold_metrics = self.run_metrics(\ncurr_gt_labels,\ncurr_llm_labels,\nentity_types_set,\n)\nfor metric in curr_threshold_metrics:\neval_metrics_map[metric.metric_type].append(metric.value)\neval_metrics_map[Metric.COMPLETION_RATE].append(\nlen(curr_llm_labels) / float(len(gt_labels))\n)\nif self.config.confidence():\neval_metrics_map[Metric.THRESHOLD].append(threshold)\neval_metrics.extend(\n[\nMetricResult(\nmetric_type=i,\nname=i.value,\nvalue=eval_metrics_map[i],\n)\nfor i in eval_metrics_map.keys()\n]\n)\nreturn eval_metrics\n</code></pre>"},{"location":"reference/tasks/#src.autolabel.tasks.utils","title":"<code>utils</code>","text":""},{"location":"reference/tasks/#src.autolabel.tasks.utils.compute_f1","title":"<code>compute_f1(prediction, truth)</code>","text":"<p>Compute models prediction accuracy based on given ground truth labels</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>str</code> <p>model generated prediction</p> required <code>truth</code> <code>str</code> <p>ground truth label to compare against</p> required <p>Returns:</p> Name Type Description <code>f1_score</code> <code>float</code> <p>values range from [0,1], with 1 indicating perfect accuracy</p> Source code in <code>src/autolabel/tasks/utils.py</code> <pre><code>def compute_f1(prediction: str, truth: str) -&gt; float:\n\"\"\"\n    Compute models prediction accuracy based on given ground truth labels\n    Args:\n        prediction: model generated prediction\n        truth: ground truth label to compare against\n    Returns:\n        f1_score: values range from [0,1], with 1 indicating perfect accuracy\n    \"\"\"\npred_tokens = normalize_text(prediction).split()\ntruth_tokens = normalize_text(truth).split()\n# if either the prediction or the truth is no-answer then f1 = 1 if they agree, 0 otherwise\nif len(pred_tokens) == 0 or len(truth_tokens) == 0:\nreturn int(pred_tokens == truth_tokens)\ncommon_tokens = set(pred_tokens) &amp; set(truth_tokens)\n# if there are no common tokens then f1 = 0\nif len(common_tokens) == 0:\nreturn 0\nprec = len(common_tokens) / len(pred_tokens)\nrec = len(common_tokens) / len(truth_tokens)\nreturn 2 * (prec * rec) / (prec + rec)\n</code></pre>"},{"location":"reference/tasks/#src.autolabel.tasks.utils.normalize_text","title":"<code>normalize_text(s)</code>","text":"<p>Removing articles and punctuation, and standardizing whitespace are all typical text processing steps.</p> Source code in <code>src/autolabel/tasks/utils.py</code> <pre><code>def normalize_text(s: str) -&gt; str:\n\"\"\"Removing articles and punctuation, and standardizing whitespace are all typical text processing steps.\"\"\"\ndef remove_articles(text):\nregex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\nreturn re.sub(regex, \" \", text)\ndef white_space_fix(text):\nreturn \" \".join(text.split())\ndef remove_punc(text):\nexclude = set(string.punctuation)\nreturn \"\".join(ch for ch in text if ch not in exclude)\ndef lower(text):\nreturn text.lower()\nreturn white_space_fix(remove_articles(remove_punc(lower(s))))\n</code></pre>"}]}